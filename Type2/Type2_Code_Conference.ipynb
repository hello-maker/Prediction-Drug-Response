{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Type2_Code_Conference.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qhuPIYkKcMWS",
        "dtgH5U4VcS9g"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-rz32vHCmMz",
        "outputId": "c01aa9e9-31d5-4886-ca16-7f5b95357e0d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOLoxVqsCz9A",
        "outputId": "7cb7a6e6-0747-4f06-eb74-bae2623b7bc7"
      },
      "source": [
        "import pandas as pd\n",
        "GEB = pd.read_csv(\"/content/gdrive/My Drive/Data/TPM data list/Capstone Design Data_tpm(5FU).csv\",index_col= 0)\n",
        "GEB_IC = GEB.iloc[37262:37267,:]\n",
        "GEB_ = GEB.iloc[:37262,:]\n",
        "real_index = GEB_.index\n",
        "G_col = GEB_.columns\n",
        "import numpy as np\n",
        "log = np.log(GEB_IC.iloc[3,:].astype(\"float\"))         # Target 값인 IC50 Value를 얻어내는 과정\n",
        "log = log.T\n",
        "log = pd.DataFrame(log)\n",
        "log['target'] = log['IC50'].apply(lambda x : 1\n",
        "                                  if (x > 5)\n",
        "                                  else 0)\n",
        "log = log.astype('object')\n",
        "log = log.drop(['IC50'], axis = 1)\n",
        "log = log.T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhuPIYkKcMWS"
      },
      "source": [
        "# Data Augumentaion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEOTG0XzLmuu"
      },
      "source": [
        "GEB_ = GEB_.astype('float')\n",
        "l = []\n",
        "v = GEB_.values\n",
        "for i in range(len(v)):\n",
        "  for j in range(len(v[i])):\n",
        "    if v[i][j] != 0:\n",
        "      l.append(v[i][j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evsnBXU1NN2v"
      },
      "source": [
        "mean = np.mean(l)\n",
        "std = np.std(l)\n",
        "def n(x):\n",
        "  if x != 0:\n",
        "    x = (x-mean)/std  + 0.1\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvuMImcCJNmH"
      },
      "source": [
        "for i in GEB_.columns:\n",
        "  GEB_[i] = GEB_[i].apply(lambda x : n(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtgH5U4VcS9g"
      },
      "source": [
        "# Data Preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcjP-RmwC0Mc"
      },
      "source": [
        "import numpy as np\n",
        "dataset = np.load('/content/gdrive/My Drive/csy/dataset_KEGG.npz', allow_pickle=True)\n",
        "adj_list = dataset['adj_list']   # 각 pathway 별 인접행렬\n",
        "b_list = dataset['b_list']       # 각 pathway에 속해 있는 gene list\n",
        "adj_list = adj_list\n",
        "proteins_by_pathway = b_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRsRAoP2C0TG"
      },
      "source": [
        "unique = np.unique(sum(proteins_by_pathway, []))             \n",
        "gene_index = dict(zip(unique, np.zeros(len(unique))))\n",
        "for i in b_list:\n",
        "  for j in i:\n",
        "    gene_index[j] += 1\n",
        "import collections\n",
        "sorted_by_value = sorted(gene_index.items(), key=lambda x : x[1], reverse=True)\n",
        "sorted_dict = collections.OrderedDict(sorted_by_value)\n",
        "#print(sorted_dict)\n",
        "gene_index = list(sorted_dict.keys())\n",
        "new_gene_indexing = []\n",
        "for ind, k in enumerate(gene_index):\n",
        "  if ((ind+1)%2==1):\n",
        "    new_gene_indexing.insert(0, k)\n",
        "  else:\n",
        "    new_gene_indexing.append(k)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiSXqSt_GQUT"
      },
      "source": [
        "# GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz9a3z2dC0Zk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import scipy.sparse as sp\n",
        "def clones( module, N):\n",
        "    return nn.ModuleList(copy.deepcopy(module) for _ in range(N))\n",
        "\n",
        "class GCNLayer(nn.Module):                                                  # 가장 기본적인 GCN Layer 생성\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, bn=True, num_head=1):                          \n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.use_bn = bn                                                    # layer를 통과할 때 마다 포함된 값들을 normalization\n",
        "        self.linear = nn.Linear(in_dim, out_dim)                            # 각 노드의 feature에 weight와 bias 부여 \n",
        "        #nn.init.xavier_uniform_(self.linear.weight)\n",
        "        nn.init.kaiming_normal_(self.linear.weight)\n",
        "        #self.bn = nn.BatchNorm1d(out_dim)\n",
        "        self.attention = Attention(out_dim, out_dim, num_head)\n",
        "    \n",
        "        \n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        out = self.linear(x)\n",
        "        #out = torch.matmul(adj, out)\n",
        "        out = self.attention(out,adj)\n",
        "        #out = self.bn(out)\n",
        "      \n",
        "        #out = norm(out)                                                  # Activation function, 음수 값은 0으로 내보내는 Relu가 아닌 음수일 때 그 값을 exponenetial 값을 취해주는\n",
        "        out = F.relu(out)                                                # Elu 함수를 사용\n",
        "        #out = F.elu(out,alpha=1.0) \n",
        "        return out, adj                                                              \n",
        "\n",
        "class SkipConnection(nn.Module):                                            # ResNet처럼 GCN Block이 통과할 때마다 이전 값을 포함해주는 skip connection 기능을 수행\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(SkipConnection, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        \n",
        "    def forward(self, in_x, out_x):\n",
        "        if (self.in_dim != self.out_dim):\n",
        "            in_x = self.linear(in_x)\n",
        "        out = in_x + out_x                                                  # GCNBlock class 안에서는 in_x : residual(전 block에서 생성된 output), output : 현재 Block에서 생성된 output \n",
        "        return out\n",
        "\n",
        "class GatedSkipConnection(nn.Module):                                       # residual 학습을 할 때 z_coefficient를 추가하여 residual값과 해당 block output 값의 중요도를 계산하여 추가\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(GatedSkipConnection, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
        "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
        "        nn.init.xavier_uniform_(self.linear.weight)\n",
        "        nn.init.xavier_uniform_(self.linear_coef_in.weight)\n",
        "        nn.init.xavier_uniform_(self.linear_coef_out.weight)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, in_x, out_x):\n",
        "        if (self.in_dim != self.out_dim):\n",
        "            in_x = self.linear(in_x)\n",
        "        z = self.gate_coefficient(in_x, out_x)\n",
        "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
        "        return out\n",
        "            \n",
        "    def gate_coefficient(self, in_x, out_x):\n",
        "        x1 = self.linear_coef_in(in_x)\n",
        "        x2 = self.linear_coef_out(out_x)\n",
        "        return self.sigmoid(x1+x2)\n",
        "\n",
        "class GCNBlock(nn.Module):                                                 # 앞서 언급되었던 GCN Layer와 skip connection 기능을 모두 포함하고 있는 GCN Block 생성\n",
        "    \n",
        "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, sc, bn=True): \n",
        "        super(GCNBlock, self).__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,            # n_layer 만큼 GCN layer를 생성하여 nn.MouduleList()에 추가\n",
        "                                        out_dim if i==n_layer-1 else hidden_dim,\n",
        "                                        bn=True))\n",
        "        self.relu = nn.ReLU()\n",
        "        if sc=='gsc':\n",
        "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
        "        elif sc=='sc':\n",
        "            self.sc = SkipConnection(in_dim, out_dim)\n",
        "        elif sc=='no':\n",
        "            self.sc = None\n",
        "        else:\n",
        "            assert False, \"Wrong sc type.\"\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        residual = x                                                      \n",
        "        for i, layer in enumerate(self.layers):\n",
        "            out, adj = layer((x if i==0 else out), adj)\n",
        "        if self.sc != None:\n",
        "            out = self.sc(residual, out)\n",
        "        out = F.relu(out)\n",
        "        return out, adj\n",
        "class Attention(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, output_dim, num_head):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.num_head = num_head # multi-head attention: num_head>1\n",
        "        self.atn_dim = output_dim // num_head\n",
        "        \n",
        "        self.linears = nn.ModuleList()\n",
        "        self.corelations = nn.ParameterList()\n",
        "        for i in range(self.num_head):\n",
        "            self.linears.append(nn.Linear(in_dim, self.atn_dim))       # H*W를 만들어주는 과정 [gene X self.atn_dim]\n",
        "            corelation = torch.FloatTensor(self.atn_dim, self.atn_dim) # attention_dim*attention_dim matrix(C)\n",
        "            nn.init.xavier_uniform_(corelation)\n",
        "            self.corelations.append(nn.Parameter(corelation))\n",
        "      \n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        heads = list()\n",
        "        for i in range(self.num_head):\n",
        "            x_transformed = self.linears[i](x) # num_aton * attention_dim     \n",
        "            alpha = self.attention_matrix(x_transformed, self.corelations[i], adj)\n",
        "            x_head = torch.matmul(alpha, x_transformed)\n",
        "            heads.append(x_head)\n",
        "        output = torch.cat(heads, dim=2)\n",
        "        \n",
        "        return output\n",
        "            \n",
        "    def attention_matrix(self, x_transformed, corelation, adj):\n",
        "        x = torch.einsum('akj,ij->aki', (x_transformed, corelation))\n",
        "        alpha = torch.matmul(x, torch.transpose(x_transformed, 1, 2))\n",
        "        alpha = torch.mul(alpha, adj)\n",
        "        alpha = self.tanh(alpha)\n",
        "        return alpha\n",
        "\n",
        "class ReadOut(nn.Module):                                                # GCN Block을 거쳐 나온 matrix 값을 Nx1 크기로 Flatten (size는 해당 pathway에 속해있는 gene의 개수)\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, act=None):\n",
        "        super(ReadOut, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim= out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(self.in_dim, \n",
        "                                self.out_dim)\n",
        "        #nn.init.xavier_uniform_(self.linear.weight)\n",
        "        nn.init.kaiming_normal_(self.linear.weight)\n",
        "        self.activation = act\n",
        "        #self.norm = nn.LayerNorm(out_dim, elementwise_affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = torch.sum(out, 2)\n",
        "        if self.activation != None:\n",
        "            out = self.activation(out)\n",
        "        #norm = nn.LayerNorm(out.shape[1], elementwise_affine=False)\n",
        "        #out = norm(out)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtCeHWDEC1FP"
      },
      "source": [
        "def mean_norm(df_input):\n",
        "    return df_input.apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
        "def cell_line_dicimal_norm(G):\n",
        "   GEB1 = GEB_[[G]]\n",
        "   GEB1 = GEB1.astype('float')\n",
        "   GEB1 = mean_norm(GEB1)\n",
        "   protein_features_df = GEB1.T\n",
        "   protein_features_df = protein_features_df.astype('float64')\n",
        "   return protein_features_df\n",
        "def cell_line_dicimal(G):\n",
        "   GEB1 = GEB_[[G]]\n",
        "   GEB1 = GEB1.astype('float')\n",
        "   #GEB1 = GEB1/292291\n",
        "   GEB1 = GEB1/479\n",
        "   protein_features_df = GEB1.T\n",
        "   protein_features_df = protein_features_df.astype('float64')\n",
        "   return protein_features_df\n",
        "#################################\n",
        "def cell_line_binary(G):                                         # gene expression 값이 Float 형태였던 데이터형태를 이진수로 변환해주는 함수\n",
        "        GEB1 = GEB_[[G]]\n",
        "        GEB1['B'] = GEB1[G].astype('str').str.zfill(24)\n",
        "        non_dot_list = []\n",
        "        for i in GEB1['B'].values:\n",
        "          i = i.replace(\".\", \"0\")\n",
        "          non_dot_list.append(i)\n",
        "        GEB1['B'] = non_dot_list\n",
        "        GEB2 = GEB1[['B']]\n",
        "        dat = [list(map(int, str(x).zfill(6))) for x in GEB2.B]\n",
        "        d = pd.DataFrame(dat, GEB2.index).rename(columns=lambda x: f'2^{23 - x}')\n",
        "        GEB2.join(d)\n",
        "\n",
        "        protein_features_df = GEB2.join(d).drop(['B'], axis = 1)\n",
        "        protein_features_df = protein_features_df.transpose()\n",
        "\n",
        "        return protein_features_df\n",
        "##################################        \n",
        "def selection_(pathway_gene):                                    # 최종적으로 만들어진 GCN output의 크기가 6000x1로 너무 커서 모든 pathway에 값이 0으로 되어 있는 gene을 제거하여  \n",
        "                                                                # 400~500 x 1 형태로 변환       \n",
        "    row_list = []\n",
        "    pathway_gene_ = pathway_gene[0,:,:]\n",
        "    for row in range(pathway_gene_.shape[1]):\n",
        "      if (sum(pathway_gene_[:,row] == 0)) < 275 :\n",
        "        row_list.append(row)\n",
        "    return pathway_gene[:,:,row_list]\n",
        "\n",
        "##################################\n",
        "def normalize_adj(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)) # D\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten() # D^-0.5\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # D^-0.5\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).toarray()\n",
        "\n",
        "class Attentionisallyouneed(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attentionisallyouneed, self).__init__()     \n",
        "        \n",
        "        self.GCN_layer = nn.ModuleList()                                # GCN Block과 readout을 포함하고 있는 nn.ModuleList() 생성\n",
        "        self.GCN_layer.append(GCNBlock(1, 1, 2, 2, sc='gsc'))       # nn.Mouduleist()에 포함되어 있는 class 들은 for문을 통해 그 안에 속해있는 모든 function들을 수행\n",
        "        self.GCN_layer.append(GCNBlock(1, 2, 2, 2, sc='gsc'))       # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=nn%20modulelist#torch.nn.ModuleList.append\n",
        "        self.GCN_layer.append(GCNBlock(2, 2, 4, 4, sc='gsc')) \n",
        "        self.GCN_layer.append(ReadOut(4, 1))\n",
        "        self.GCN_layer_list = nn.ModuleList()  \n",
        "\n",
        "        for _ in list(range(308)):\n",
        "          self.GCN_layer_list.append(self.GCN_layer)\n",
        "\n",
        "        self.linear1 = nn.Linear(in_features=25116, out_features=2**7)    # gene은 총 450개\n",
        "        #self.linear2 = nn.Linear(in_features=2**14, out_features=2**10)\n",
        "        #self.linear2 = nn.Linear(in_features=2**10, out_features=2**7)\n",
        "        self.linear3 = nn.Linear(in_features=2**7, out_features=8)\n",
        "        self.linear4 = nn.Linear(in_features=8, out_features=2)\n",
        "        self.bn1 = nn.BatchNorm1d(2**7)\n",
        "        #self.bn2 = nn.BatchNorm1d(8)\n",
        "        self.bn3 = nn.BatchNorm1d(8)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        self.linear_list = nn.ModuleList()\n",
        "        self.linear_list.append(self.linear1)\n",
        "        self.linear_list.append(self.bn1)\n",
        "        self.linear_list.append(self.relu)\n",
        "        self.linear_list.append(self.dropout)\n",
        "        self.linear_list.append(self.linear3)\n",
        "        self.linear_list.append(self.bn3)\n",
        "        self.linear_list.append(self.relu)\n",
        "        self.linear_list.append(self.dropout)\n",
        "        #self.linear_list.append(self.linear3)\n",
        "        #self.linear_list.append(self.bn3)\n",
        "        #self.linear_list.append(self.relu)\n",
        "        self.linear_list.append(self.linear4)\n",
        "        #self.linear_list.append(self.relu)\n",
        "        #self.linear_list.append(self.linear5)\n",
        "        torch.nn.init.kaiming_normal_(self.linear1.weight)\n",
        "        #torch.nn.init.kaiming_normal_(self.linear2.weight)\n",
        "        torch.nn.init.kaiming_normal_(self.linear3.weight)\n",
        "        torch.nn.init.kaiming_normal_(self.linear4.weight)\n",
        "        \n",
        "        #self.theta = nn.Parameter(torch.random.normal(0,1,(334)))\n",
        "\n",
        "\n",
        "    def forward(self, bat_tensor):                      # randome_train_list : 800개의 cell line 중 미리 random 함수를 통해 700개의 cell line list 생성                                                                                   # 메모리 문제로 batch_size를 호출 받아 직접 쌓는 방식으로 구현            \n",
        "              # 특이하게 forward() 과정에서 batch를 생성 -> [H, W]의 matrix를 직접 쌓아\n",
        "        batch_size = bat_tensor.shape[0]\n",
        "        pre_input_tensor = []                                              # [batch, 1, H, W] 형태로 최종 CNN input 텐서 생성              \n",
        "        pathway_gene = []\n",
        "        protein_features_df = GEB_.T\n",
        "        uni_list_prot = np.unique(sum(proteins_by_pathway, []))                  \n",
        "        indexing = dict(zip(new_gene_indexing, np.arange(len(uni_list_prot))))\n",
        "        i_ = list(range((342)))                                                    \n",
        "        col_list = list(protein_features_df.columns)\n",
        "        pathway_gene = []\n",
        "\n",
        "        for i, adj, b in zip(i_, adj_list, b_list):                         # Pathway 별로 GCN 연산 수         \n",
        "          ##\n",
        "          col_gene_ind = []\n",
        "          for col_gene in b:\n",
        "            col_gene_ind.append(col_list.index(col_gene))\n",
        "          ##\n",
        "\n",
        "          x = bat_tensor[:,col_gene_ind,:]\n",
        "          adj = normalize_adj(adj)         \n",
        "          x, adj = torch.tensor(x).to('cuda').float(), torch.tensor(adj).to('cuda').float()    # x : feature matrix   adj : 각 pathway의 인접행렬\n",
        "          #adj = torch.tensor(adj).to('cuda').float() \n",
        "          #print(x.shape, adj.shape)  \n",
        "          \n",
        "            # GCN 학습 과정\n",
        "        \n",
        "          for k, layer in enumerate(self.GCN_layer_list[i]): \n",
        "            if k != 3:                                                    # -> GCNBlock(GCNLayer 10개) -> GCNBlock(GCNLayer 25개) -> Readout -> gene의 숫자가 size인 텐서 생성 \n",
        "              \n",
        "              out, adj = layer((x if k==0 else out), adj)             \n",
        "        \n",
        "            else:              \n",
        "              output = layer(out) \n",
        "          \n",
        "          if i == 0:\n",
        "            pathway_gene = output\n",
        "          else:\n",
        "            #pathway_gene = torch.cat([pathway_gene, output*self.theta[i]], dim=1)         # loss = crieter ~~ + torch.mean(torch.abs(model.theta))\n",
        "            pathway_gene = torch.cat([pathway_gene, output], dim=1)\n",
        "        input_data_depth10 = pathway_gene\n",
        "        x = input_data_depth10.view(batch_size, -1).to('cuda')\n",
        "        for layer in self.linear_list:\n",
        "          x = layer(x)\n",
        "        t = 0\n",
        "        \n",
        "        return x, t\n",
        "\n",
        "\n",
        "# torch.Size([1, 308, 351])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOq_bsE1cCut"
      },
      "source": [
        "def make_data(GEB, train_set):\n",
        "  random_cell_line = []       \n",
        "  bat_tensor = []  \n",
        "  for cnt, G in enumerate(GEB.columns[list(train_set)]):   \n",
        "    random_cell_line.append(G)\n",
        "    #protein_features_df = cell_line_dicimal_norm(G)\n",
        "    values = cell_line_dicimal(G).T.values\n",
        "    tensor = torch.from_numpy(values)\n",
        "    bat_tensor.append(tensor)\n",
        "  bat_tensor = torch.stack(bat_tensor, dim = 0)\n",
        "  target = torch.tensor(pd.Series(log[random_cell_line].values[0])).to('cuda')\n",
        "  return bat_tensor, target  \n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp  \n",
        "\n",
        "def eval(model, eval_list, batch_size):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  loss = 0\n",
        "  with torch.no_grad():\n",
        "    eval_set = np.random.choice(eval_list, batch_size)\n",
        "    bat_tensor, target = make_data(GEB, eval_set) \n",
        "    output, t = model(bat_tensor)  \n",
        "    loss = criterion(output, target)\n",
        "    auc_score = roc_auc_score(target.cpu().numpy(), output.cpu().detach().numpy()[:,1]) \n",
        "    _, predicted = output.max(1)\n",
        "    correct = predicted.eq(target).sum().item()\n",
        "  return 100. * correct / batch_size, loss.item(), auc_score\n",
        "\n",
        "import random\n",
        "random_seed = 1553\n",
        "torch.manual_seed(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "#np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "#torch.cuda.manual_seed(random_seed)\n",
        "#torch.cuda.manual_seed_all(random_seed) # multi-GPU\n",
        "device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbhL5rFi6RW5"
      },
      "source": [
        "------------------여기까지 실행------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf2LeVQQVYP1"
      },
      "source": [
        "EPOCH까지 연산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCQgNOwLGcvO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TebybdOBC1TC",
        "outputId": "b66130b9-7cd7-49eb-854f-58356fef9ffe"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import random\n",
        "random_seed = 1553\n",
        "torch.manual_seed(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "#np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed) # multi-GPU\n",
        "#####################################################\n",
        "\n",
        "\n",
        "#nums = set()\n",
        "#while len(nums) != 800: \n",
        "#  nums.add(random.randint(1, 7210))\n",
        "#eval_list = list(nums)\n",
        "#train_list = list(range(1, 7210))\n",
        "#for i in eval_list:\n",
        "#  train_list.remove(i)\n",
        "\n",
        "nums = set()\n",
        "while len(nums) != 100: \n",
        "  nums.add(random.randint(1, 801))\n",
        "eval_list = list(nums)\n",
        "train_list = list(range(1, 801))\n",
        "for i in eval_list:\n",
        "  train_list.remove(i)\n",
        "######################################################\n",
        " # hidden_dim, de_hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device\n",
        "device = 'cuda'\n",
        "\n",
        "model = Attentionisallyouneed().to(device)  # 메모리 때문에 308 -> 89\n",
        "#model = GraphMLP().to(device)  \n",
        "\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=5e-4) # lr = 1e-3 or 5e-4, weight_decay=0.001\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay = 0.01)#, weight_decay=0.1\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0,  verbose = False)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#model.train()  \n",
        "\n",
        "\n",
        "print(\"total parameter 수 : \", get_n_params(model))\n",
        "# 275664581\n",
        "# 423843235 -> gene 137\n",
        "# 544179655 -> gene 176\n",
        "loss_list = []\n",
        "val_loss_list = []\n",
        "import time\n",
        "start = time.time()\n",
        "batch_size = 64\n",
        "iterations = 10\n",
        "epochs = 15\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #model.train() \n",
        "  eval_acc_sum = 0\n",
        "  epoch_loss = 0\n",
        "  epoch_val_loss = 0\n",
        "  train_list = list(range(1, 801))\n",
        "  #train_list = []\n",
        "  for i in eval_list:\n",
        "    train_list.remove(i)\n",
        "  \n",
        "  for iteration in range(iterations): \n",
        "    train_set = np.random.choice(train_list, batch_size, replace = False)\n",
        "    for t in train_set:\n",
        "      train_list.remove(t) \n",
        "\n",
        "    bat_tensor, target = make_data(GEB, train_set)   \n",
        "    model.train()\n",
        "    train_loss = 0 \n",
        "    optimizer.zero_grad()\n",
        "    output, t = model(bat_tensor) # 최종 output\n",
        "   \n",
        "    loss = criterion(output, target)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()  # 역전파\n",
        "    optimizer.step()\n",
        "    #scheduler.step()\n",
        "    torch.cuda.empty_cache()\n",
        "    print()\n",
        "    print(iteration, \"번째 iteration\")\n",
        "    print(\"train loss : \", train_loss)        \n",
        "    #auc_score = roc_auc_score(target.cpu().numpy(), output.cpu().detach().numpy()[:,1]) \n",
        "    if iteration % 1 == 0:\n",
        "\n",
        "      eval_acc, eval_loss, auc = eval(model, eval_list, 64)\n",
        "      print(\"\\n eval loss : \", eval_loss,)\n",
        "      print(\"eval accuarcy : \", eval_acc, \"%\")\n",
        "      print('AUC : ', auc  )\n",
        "      print('------------------------------------\\n')\n",
        "      eval_acc_sum += eval_acc\n",
        "      val_loss_list.append(eval_loss)\n",
        "      epoch_val_loss += eval_loss\n",
        "      if auc > 0.81:\n",
        "        break\n",
        "\n",
        "    loss_list.append(train_loss)\n",
        "    epoch_loss += train_loss\n",
        "    #print(model.linear1.weight.grad)\n",
        "    #print(model.GCN_layer[3].linear.weight.grad)\n",
        "  print(\"------------------{} EPOCH 완료-----------------------\".format(epoch))\n",
        "  print(\"Epoch Loss Mean : \", epoch_loss/10) # batch 64 -> 10\n",
        "  print(\"Epoch Val Loss Mean : \", epoch_val_loss/10)\n",
        "  print(\"Epoch eval acc : \", eval_acc_sum/10)\n",
        "  print(\"---------------------------------------------------\")\n",
        "  \n",
        "print(\"time :\", (time.time() - start)/60)\n",
        "\n",
        "# weight decay : 0,1 ~  0.000001 까지 다양하게 해보기\n",
        "# adamW 사용해보기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total parameter 수 :  3216515\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.7055772542953491\n",
            "\n",
            " eval loss :  7.2107648849487305\n",
            "eval accuarcy :  34.375 %\n",
            "AUC :  0.5968614718614719\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6954653859138489\n",
            "\n",
            " eval loss :  5.377948760986328\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.5376984126984127\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.775611162185669\n",
            "\n",
            " eval loss :  4.638859748840332\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.6286286286286286\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6830260157585144\n",
            "\n",
            " eval loss :  8.118531227111816\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.6207233626588465\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.739031195640564\n",
            "\n",
            " eval loss :  10.36352252960205\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.46470588235294114\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6897194385528564\n",
            "\n",
            " eval loss :  16.696269989013672\n",
            "eval accuarcy :  40.625 %\n",
            "AUC :  0.5283400809716599\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.7254246473312378\n",
            "\n",
            " eval loss :  22.53651237487793\n",
            "eval accuarcy :  39.0625 %\n",
            "AUC :  0.5323076923076923\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6893858909606934\n",
            "\n",
            " eval loss :  23.599014282226562\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.4793103448275862\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6955451965332031\n",
            "\n",
            " eval loss :  25.50722312927246\n",
            "eval accuarcy :  50.0 %\n",
            "AUC :  0.69287109375\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.7176827192306519\n",
            "\n",
            " eval loss :  31.097888946533203\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.4465686274509804\n",
            "------------------------------------\n",
            "\n",
            "------------------0 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.7116468906402588\n",
            "Epoch Val Loss Mean :  15.514653587341309\n",
            "Epoch eval acc :  45.3125\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.7172840237617493\n",
            "\n",
            " eval loss :  32.835105895996094\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.5459433040078201\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6842374801635742\n",
            "\n",
            " eval loss :  43.16748046875\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.4538690476190476\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.7899665236473083\n",
            "\n",
            " eval loss :  48.463836669921875\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.49299299299299293\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.7004868388175964\n",
            "\n",
            " eval loss :  48.073028564453125\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.5596285434995113\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6685956120491028\n",
            "\n",
            " eval loss :  54.19329071044922\n",
            "eval accuarcy :  53.125 %\n",
            "AUC :  0.6661764705882353\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.7335382699966431\n",
            "\n",
            " eval loss :  62.59783935546875\n",
            "eval accuarcy :  53.125 %\n",
            "AUC :  0.6259803921568627\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6701510548591614\n",
            "\n",
            " eval loss :  56.67815399169922\n",
            "eval accuarcy :  59.375 %\n",
            "AUC :  0.6300607287449392\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.7013991475105286\n",
            "\n",
            " eval loss :  75.06856536865234\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.555229716520039\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6914209723472595\n",
            "\n",
            " eval loss :  70.40874481201172\n",
            "eval accuarcy :  53.125 %\n",
            "AUC :  0.45833333333333337\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6831649541854858\n",
            "\n",
            " eval loss :  70.5333023071289\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.5426470588235295\n",
            "------------------------------------\n",
            "\n",
            "------------------1 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.704024487733841\n",
            "Epoch Val Loss Mean :  56.201934814453125\n",
            "Epoch eval acc :  50.625\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6989311575889587\n",
            "\n",
            " eval loss :  60.727901458740234\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.6270935960591133\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.7241281270980835\n",
            "\n",
            " eval loss :  50.08304214477539\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.4891625615763546\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6787464618682861\n",
            "\n",
            " eval loss :  44.63252639770508\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.5855327468230694\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6657142639160156\n",
            "\n",
            " eval loss :  64.20323181152344\n",
            "eval accuarcy :  37.5 %\n",
            "AUC :  0.3421875\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6764026284217834\n",
            "\n",
            " eval loss :  59.620235443115234\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.4945812807881773\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.7319102883338928\n",
            "\n",
            " eval loss :  63.31482696533203\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.5259803921568628\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6590014100074768\n",
            "\n",
            " eval loss :  65.36669158935547\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.43939393939393945\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6990966200828552\n",
            "\n",
            " eval loss :  86.14106750488281\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.6166166166166166\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.682439386844635\n",
            "\n",
            " eval loss :  84.7119369506836\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.4059059059059059\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6712377667427063\n",
            "\n",
            " eval loss :  90.71220397949219\n",
            "eval accuarcy :  37.5 %\n",
            "AUC :  0.6552083333333333\n",
            "------------------------------------\n",
            "\n",
            "------------------2 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6887608110904694\n",
            "Epoch Val Loss Mean :  66.95136642456055\n",
            "Epoch eval acc :  44.53125\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6462247967720032\n",
            "\n",
            " eval loss :  101.09656524658203\n",
            "eval accuarcy :  37.5 %\n",
            "AUC :  0.6854166666666667\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6879858374595642\n",
            "\n",
            " eval loss :  107.71683502197266\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.625\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6785430312156677\n",
            "\n",
            " eval loss :  138.32296752929688\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.6671568627450981\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.7020160555839539\n",
            "\n",
            " eval loss :  174.79026794433594\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.34457478005865105\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6888254284858704\n",
            "\n",
            " eval loss :  238.2563934326172\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.586764705882353\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6926440000534058\n",
            "\n",
            " eval loss :  282.5597229003906\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.4711632453567937\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6749535799026489\n",
            "\n",
            " eval loss :  351.8870849609375\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.4955665024630542\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6669541597366333\n",
            "\n",
            " eval loss :  433.3039855957031\n",
            "eval accuarcy :  40.625 %\n",
            "AUC :  0.47419028340080976\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.7059188485145569\n",
            "\n",
            " eval loss :  425.0676574707031\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.619704433497537\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.749951183795929\n",
            "\n",
            " eval loss :  434.35943603515625\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.5254154447702835\n",
            "------------------------------------\n",
            "\n",
            "------------------3 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6894016921520233\n",
            "Epoch Val Loss Mean :  268.73609161376953\n",
            "Epoch eval acc :  45.78125\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6914201974868774\n",
            "\n",
            " eval loss :  715.294677734375\n",
            "eval accuarcy :  34.375 %\n",
            "AUC :  0.5395021645021645\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6518898010253906\n",
            "\n",
            " eval loss :  603.4344482421875\n",
            "eval accuarcy :  50.0 %\n",
            "AUC :  0.76416015625\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6464411020278931\n",
            "\n",
            " eval loss :  694.0359497070312\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.7137254901960784\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.647213876247406\n",
            "\n",
            " eval loss :  724.0996704101562\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.6055718475073314\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6988414525985718\n",
            "\n",
            " eval loss :  647.6600341796875\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.5430430430430431\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6817808151245117\n",
            "\n",
            " eval loss :  785.7822875976562\n",
            "eval accuarcy :  54.6875 %\n",
            "AUC :  0.5827586206896551\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6518109440803528\n",
            "\n",
            " eval loss :  1098.9232177734375\n",
            "eval accuarcy :  40.625 %\n",
            "AUC :  0.7520242914979757\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6989100575447083\n",
            "\n",
            " eval loss :  1047.861328125\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.7009852216748768\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6749347448348999\n",
            "\n",
            " eval loss :  1189.66357421875\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.5873015873015872\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6948344707489014\n",
            "\n",
            " eval loss :  1492.1302490234375\n",
            "eval accuarcy :  39.0625 %\n",
            "AUC :  0.6364102564102564\n",
            "------------------------------------\n",
            "\n",
            "------------------4 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6738077461719513\n",
            "Epoch Val Loss Mean :  899.8885437011719\n",
            "Epoch eval acc :  46.09375\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6592065691947937\n",
            "\n",
            " eval loss :  1689.4656982421875\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.6076076076076076\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.7101359963417053\n",
            "\n",
            " eval loss :  1671.8914794921875\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.6226783968719452\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.715327262878418\n",
            "\n",
            " eval loss :  2177.739990234375\n",
            "eval accuarcy :  40.625 %\n",
            "AUC :  0.597165991902834\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.7243134379386902\n",
            "\n",
            " eval loss :  2111.604736328125\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.4419419419419419\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6917458176612854\n",
            "\n",
            " eval loss :  2087.65673828125\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.5322420634920635\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6948755383491516\n",
            "\n",
            " eval loss :  2394.394287109375\n",
            "eval accuarcy :  34.375 %\n",
            "AUC :  0.4399350649350649\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.686615526676178\n",
            "\n",
            " eval loss :  1949.989013671875\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.7037037037037037\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.692098081111908\n",
            "\n",
            " eval loss :  1465.8150634765625\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.4555229716520039\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6881935596466064\n",
            "\n",
            " eval loss :  1220.02099609375\n",
            "eval accuarcy :  54.6875 %\n",
            "AUC :  0.660591133004926\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6704862713813782\n",
            "\n",
            " eval loss :  901.90087890625\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.6911045943304007\n",
            "------------------------------------\n",
            "\n",
            "------------------5 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6932998061180115\n",
            "Epoch Val Loss Mean :  1767.0478881835938\n",
            "Epoch eval acc :  44.84375\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6934351325035095\n",
            "\n",
            " eval loss :  509.08209228515625\n",
            "eval accuarcy :  40.625 %\n",
            "AUC :  0.534412955465587\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.7032979130744934\n",
            "\n",
            " eval loss :  63.751556396484375\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.5977517106549365\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6953314542770386\n",
            "\n",
            " eval loss :  141.06918334960938\n",
            "eval accuarcy :  67.1875 %\n",
            "AUC :  0.7940199335548173\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.673803985118866\n",
            "\n",
            " eval loss :  210.4995880126953\n",
            "eval accuarcy :  40.625 %\n",
            "AUC :  0.548582995951417\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6851520538330078\n",
            "\n",
            " eval loss :  202.22479248046875\n",
            "eval accuarcy :  60.9375 %\n",
            "AUC :  0.6107692307692307\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.7039780020713806\n",
            "\n",
            " eval loss :  369.20196533203125\n",
            "eval accuarcy :  54.6875 %\n",
            "AUC :  0.5960591133004925\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6795029640197754\n",
            "\n",
            " eval loss :  396.6285095214844\n",
            "eval accuarcy :  60.9375 %\n",
            "AUC :  0.5425641025641026\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6873317956924438\n",
            "\n",
            " eval loss :  747.66455078125\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.6935960591133005\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6782217621803284\n",
            "\n",
            " eval loss :  527.239501953125\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.8783783783783784\n",
            "------------------------------------\n",
            "\n",
            "------------------6 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.5521833300590515\n",
            "Epoch Val Loss Mean :  316.73617401123045\n",
            "Epoch eval acc :  47.96875\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6985258460044861\n",
            "\n",
            " eval loss :  845.8138427734375\n",
            "eval accuarcy :  54.6875 %\n",
            "AUC :  0.37438423645320196\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6719357967376709\n",
            "\n",
            " eval loss :  740.3931884765625\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.4529529529529529\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.7041627764701843\n",
            "\n",
            " eval loss :  1352.4749755859375\n",
            "eval accuarcy :  64.0625 %\n",
            "AUC :  0.5811240721102864\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6796090602874756\n",
            "\n",
            " eval loss :  2510.66015625\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.51001001001001\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6837737560272217\n",
            "\n",
            " eval loss :  3040.348388671875\n",
            "eval accuarcy :  53.125 %\n",
            "AUC :  0.39166666666666666\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6971873641014099\n",
            "\n",
            " eval loss :  3285.477294921875\n",
            "eval accuarcy :  50.0 %\n",
            "AUC :  0.59765625\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.7124864459037781\n",
            "\n",
            " eval loss :  2970.1982421875\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.5269607843137255\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.702351450920105\n",
            "\n",
            " eval loss :  2247.863037109375\n",
            "eval accuarcy :  50.0 %\n",
            "AUC :  0.55517578125\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.7076934576034546\n",
            "\n",
            " eval loss :  2205.847900390625\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.48472906403940885\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6890801191329956\n",
            "\n",
            " eval loss :  1043.4541015625\n",
            "eval accuarcy :  59.375 %\n",
            "AUC :  0.6487854251012146\n",
            "------------------------------------\n",
            "\n",
            "------------------7 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6946806073188782\n",
            "Epoch Val Loss Mean :  2024.2531127929688\n",
            "Epoch eval acc :  53.90625\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6980577111244202\n",
            "\n",
            " eval loss :  611.9326782226562\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.6989247311827956\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6783173680305481\n",
            "\n",
            " eval loss :  198.12225341796875\n",
            "eval accuarcy :  50.0 %\n",
            "AUC :  0.60498046875\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6899646520614624\n",
            "\n",
            " eval loss :  1135.8095703125\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.40690690690690695\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.704650342464447\n",
            "\n",
            " eval loss :  3236.563232421875\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.5909090909090909\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6816126108169556\n",
            "\n",
            " eval loss :  4919.7001953125\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.5933528836754642\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6812536716461182\n",
            "\n",
            " eval loss :  9017.6123046875\n",
            "eval accuarcy :  32.8125 %\n",
            "AUC :  0.33499446290143964\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6724622249603271\n",
            "\n",
            " eval loss :  7603.6630859375\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.5606060606060606\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6778257489204407\n",
            "\n",
            " eval loss :  9162.98828125\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.5235235235235234\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6909115314483643\n",
            "\n",
            " eval loss :  9327.494140625\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.45960591133004924\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6710461378097534\n",
            "\n",
            " eval loss :  13206.6669921875\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.530791788856305\n",
            "------------------------------------\n",
            "\n",
            "------------------8 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6846101999282836\n",
            "Epoch Val Loss Mean :  5842.0552734375\n",
            "Epoch eval acc :  47.96875\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6892818808555603\n",
            "\n",
            " eval loss :  20858.18359375\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.5892857142857143\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6788501143455505\n",
            "\n",
            " eval loss :  27318.822265625\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.6686507936507938\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6923146843910217\n",
            "\n",
            " eval loss :  34732.82421875\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.6536945812807883\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.7090693116188049\n",
            "\n",
            " eval loss :  44602.15234375\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.6211211211211212\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6850035786628723\n",
            "\n",
            " eval loss :  48683.25\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.6016865079365079\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6954708695411682\n",
            "\n",
            " eval loss :  39336.046875\n",
            "eval accuarcy :  56.25 %\n",
            "AUC :  0.7142857142857142\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6866756677627563\n",
            "\n",
            " eval loss :  55957.96484375\n",
            "eval accuarcy :  35.9375 %\n",
            "AUC :  0.6702014846235419\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6875091791152954\n",
            "\n",
            " eval loss :  51390.19140625\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.6101190476190477\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6907906532287598\n",
            "\n",
            " eval loss :  48203.41015625\n",
            "eval accuarcy :  39.0625 %\n",
            "AUC :  0.6317948717948718\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6815403699874878\n",
            "\n",
            " eval loss :  39609.79296875\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.5167487684729064\n",
            "------------------------------------\n",
            "\n",
            "------------------9 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6896506309509277\n",
            "Epoch Val Loss Mean :  41069.2638671875\n",
            "Epoch eval acc :  43.90625\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6780064702033997\n",
            "\n",
            " eval loss :  36714.234375\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.676930596285435\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6896583437919617\n",
            "\n",
            " eval loss :  36950.73828125\n",
            "eval accuarcy :  50.0 %\n",
            "AUC :  0.75732421875\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6877921223640442\n",
            "\n",
            " eval loss :  48566.3984375\n",
            "eval accuarcy :  37.5 %\n",
            "AUC :  0.5619791666666667\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6913695335388184\n",
            "\n",
            " eval loss :  40025.3203125\n",
            "eval accuarcy :  50.0 %\n",
            "AUC :  0.57568359375\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6790883541107178\n",
            "\n",
            " eval loss :  36735.19140625\n",
            "eval accuarcy :  53.125 %\n",
            "AUC :  0.6098039215686274\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6802921891212463\n",
            "\n",
            " eval loss :  31010.791015625\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.6701701701701702\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6929292678833008\n",
            "\n",
            " eval loss :  17873.041015625\n",
            "eval accuarcy :  56.25 %\n",
            "AUC :  0.5491071428571428\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6984525918960571\n",
            "\n",
            " eval loss :  11101.25390625\n",
            "eval accuarcy :  53.125 %\n",
            "AUC :  0.6063725490196078\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.69972163438797\n",
            "\n",
            " eval loss :  23543.607421875\n",
            "eval accuarcy :  35.9375 %\n",
            "AUC :  0.3891834570519619\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6796578168869019\n",
            "\n",
            " eval loss :  31881.9921875\n",
            "eval accuarcy :  39.0625 %\n",
            "AUC :  0.602051282051282\n",
            "------------------------------------\n",
            "\n",
            "------------------10 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6876968324184418\n",
            "Epoch Val Loss Mean :  31440.2568359375\n",
            "Epoch eval acc :  48.125\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6751859188079834\n",
            "\n",
            " eval loss :  35088.82421875\n",
            "eval accuarcy :  50.0 %\n",
            "AUC :  0.72265625\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6848455667495728\n",
            "\n",
            " eval loss :  47341.171875\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.7445436507936508\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6921942830085754\n",
            "\n",
            " eval loss :  42214.66796875\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.5348039215686274\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6777810454368591\n",
            "\n",
            " eval loss :  23849.94140625\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.5039100684261975\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6934146285057068\n",
            "\n",
            " eval loss :  14878.7158203125\n",
            "eval accuarcy :  56.25 %\n",
            "AUC :  0.726686507936508\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6579770445823669\n",
            "\n",
            " eval loss :  25764.333984375\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.4379276637341153\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6816856265068054\n",
            "\n",
            " eval loss :  33242.0078125\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.6114369501466276\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6725586652755737\n",
            "\n",
            " eval loss :  30951.3046875\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.531031031031031\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.668170690536499\n",
            "\n",
            " eval loss :  27147.107421875\n",
            "eval accuarcy :  56.25 %\n",
            "AUC :  0.5714285714285714\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.706664502620697\n",
            "\n",
            " eval loss :  24873.833984375\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.6936936936936937\n",
            "------------------------------------\n",
            "\n",
            "------------------11 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.681047797203064\n",
            "Epoch Val Loss Mean :  30535.19091796875\n",
            "Epoch eval acc :  52.34375\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.682820200920105\n",
            "\n",
            " eval loss :  26939.2109375\n",
            "eval accuarcy :  54.6875 %\n",
            "AUC :  0.46502463054187193\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.7010961174964905\n",
            "\n",
            " eval loss :  25433.634765625\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.5135135135135135\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6699240207672119\n",
            "\n",
            " eval loss :  27628.625\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.6156156156156156\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6638625860214233\n",
            "\n",
            " eval loss :  24238.203125\n",
            "eval accuarcy :  64.0625 %\n",
            "AUC :  0.32714740190880176\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6906005144119263\n",
            "\n",
            " eval loss :  27458.1484375\n",
            "eval accuarcy :  59.375 %\n",
            "AUC :  0.36082995951417\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6971169710159302\n",
            "\n",
            " eval loss :  26089.673828125\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.5025025025025025\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6909949779510498\n",
            "\n",
            " eval loss :  27366.7265625\n",
            "eval accuarcy :  54.6875 %\n",
            "AUC :  0.4916256157635468\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.7094478011131287\n",
            "\n",
            " eval loss :  26740.13671875\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.5285285285285285\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6813801527023315\n",
            "\n",
            " eval loss :  28341.06640625\n",
            "eval accuarcy :  56.25 %\n",
            "AUC :  0.5595238095238095\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6954324245452881\n",
            "\n",
            " eval loss :  21526.765625\n",
            "eval accuarcy :  62.5 %\n",
            "AUC :  0.5666666666666665\n",
            "------------------------------------\n",
            "\n",
            "------------------12 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6882675766944886\n",
            "Epoch Val Loss Mean :  26176.219140625\n",
            "Epoch eval acc :  58.28125\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6913531422615051\n",
            "\n",
            " eval loss :  21463.109375\n",
            "eval accuarcy :  46.875 %\n",
            "AUC :  0.6049019607843138\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6965590119361877\n",
            "\n",
            " eval loss :  4167.94580078125\n",
            "eval accuarcy :  40.625 %\n",
            "AUC :  0.7282388663967612\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.7008172869682312\n",
            "\n",
            " eval loss :  21479.44140625\n",
            "eval accuarcy :  39.0625 %\n",
            "AUC :  0.685128205128205\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6783226728439331\n",
            "\n",
            " eval loss :  29081.68359375\n",
            "eval accuarcy :  51.5625 %\n",
            "AUC :  0.7082111436950147\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6922818422317505\n",
            "\n",
            " eval loss :  45899.1015625\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.786206896551724\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.689846396446228\n",
            "\n",
            " eval loss :  80907.4296875\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.29279279279279274\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6976346969604492\n",
            "\n",
            " eval loss :  88172.4140625\n",
            "eval accuarcy :  42.1875 %\n",
            "AUC :  0.4054054054054054\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6868374347686768\n",
            "\n",
            " eval loss :  70505.0546875\n",
            "eval accuarcy :  48.4375 %\n",
            "AUC :  0.41788856304985333\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.67685467004776\n",
            "\n",
            " eval loss :  57269.796875\n",
            "eval accuarcy :  53.125 %\n",
            "AUC :  0.5004901960784314\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6936711668968201\n",
            "\n",
            " eval loss :  50841.18359375\n",
            "eval accuarcy :  53.125 %\n",
            "AUC :  0.49754901960784315\n",
            "------------------------------------\n",
            "\n",
            "------------------13 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6904178321361542\n",
            "Epoch Val Loss Mean :  46978.716064453125\n",
            "Epoch eval acc :  46.25\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6900631785392761\n",
            "\n",
            " eval loss :  44006.5\n",
            "eval accuarcy :  45.3125 %\n",
            "AUC :  0.5694581280788177\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6871273517608643\n",
            "\n",
            " eval loss :  29229.236328125\n",
            "eval accuarcy :  37.5 %\n",
            "AUC :  0.44843750000000004\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6773515939712524\n",
            "\n",
            " eval loss :  9258.513671875\n",
            "eval accuarcy :  50.0 %\n",
            "AUC :  0.642578125\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6787910461425781\n",
            "\n",
            " eval loss :  394.8336181640625\n",
            "eval accuarcy :  56.25 %\n",
            "AUC :  0.3883928571428571\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6828829050064087\n",
            "\n",
            " eval loss :  6557.978515625\n",
            "eval accuarcy :  60.9375 %\n",
            "AUC :  0.5548717948717949\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6862249970436096\n",
            "\n",
            " eval loss :  13579.6982421875\n",
            "eval accuarcy :  54.6875 %\n",
            "AUC :  0.3955665024630542\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6767682433128357\n",
            "\n",
            " eval loss :  19176.794921875\n",
            "eval accuarcy :  54.6875 %\n",
            "AUC :  0.4684729064039409\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6850095987319946\n",
            "\n",
            " eval loss :  28937.140625\n",
            "eval accuarcy :  43.75 %\n",
            "AUC :  0.36557539682539686\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6951398849487305\n",
            "\n",
            " eval loss :  24645.1796875\n",
            "eval accuarcy :  57.8125 %\n",
            "AUC :  0.3803803803803804\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6695930361747742\n",
            "\n",
            " eval loss :  23417.515625\n",
            "eval accuarcy :  62.5 %\n",
            "AUC :  0.41093750000000007\n",
            "------------------------------------\n",
            "\n",
            "------------------14 EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  0.6828951835632324\n",
            "Epoch Val Loss Mean :  19920.339123535156\n",
            "Epoch eval acc :  52.34375\n",
            "---------------------------------------------------\n",
            "time : 61.02858538627625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGkPVq05OdpI",
        "outputId": "15e31e9a-f109-46b3-ab67-3d123c439574"
      },
      "source": [
        "torch.sum(weight,0).size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([25116])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAW2vuWSOCQx",
        "outputId": "33adb973-e2ac-40d5-c877-99170f791b51"
      },
      "source": [
        "weight = model.linear1.weight\n",
        "weight_sum = torch.max(weight,0)\n",
        "weight_sum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([0.0317, 0.0266, 0.0190,  ..., 0.0205, 0.0212, 0.0178], device='cuda:0',\n",
              "       grad_fn=<MaxBackward0>), indices=tensor([ 77,  32, 101,  ..., 103, 120, 101], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFxchyILw9vJ",
        "outputId": "e8ca6dc1-8841-4b80-b7ff-e0811f794953"
      },
      "source": [
        "weight = model.linear1.weight\n",
        "weight_sum = torch.sum(weight,0)\n",
        "weight_sum = weight_sum.cpu().detach().numpy()\n",
        "sort_index = pd.DataFrame(weight_sum).sort_values(0, ascending=False).head(20).index\n",
        "sort_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([20865, 24045,  9754,  3187, 19928, 18297,  2771,  6727, 23035,\n",
              "            19037, 15742, 21700,  1626,  4506, 13636, 15460,  6721, 17027,\n",
              "            10522,  6520],\n",
              "           dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M4Mgoadaycs"
      },
      "source": [
        "len_list = []\n",
        "for i in range(len(b_list)):\n",
        "  len_list.append(len(b_list[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdxEvJ7ebKyC",
        "outputId": "ed8cf4ad-f042-4496-8fac-e20bd108d0a6"
      },
      "source": [
        "for number in sort_index:\n",
        "  index_sum = 0\n",
        "  f = open(\"/content/gdrive/My Drive/Data/hsa_order.txt\")\n",
        "  for i in range(len(len_list)):\n",
        "    #number = 19928\n",
        "    if index_sum <= number < (index_sum+len_list[i]):\n",
        "      #print(i)\n",
        "      print(f.readlines()[i])\n",
        "      print(b_list[i][number - index_sum])\n",
        "      print()\n",
        "      print()\n",
        "      break\n",
        "    index_sum += len_list[i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsa05164\n",
            "\n",
            "NOTCH1\n",
            "\n",
            "\n",
            "hsa05218\n",
            "\n",
            "RAD51\n",
            "\n",
            "\n",
            "hsa04371\n",
            "\n",
            "PRKAB1\n",
            "\n",
            "\n",
            "hsa05410\n",
            "\n",
            "HPRT1\n",
            "\n",
            "\n",
            "hsa05142\n",
            "\n",
            "PLEKHM2\n",
            "\n",
            "\n",
            "hsa04978\n",
            "\n",
            "PSMC1\n",
            "\n",
            "\n",
            "hsa05332\n",
            "\n",
            "AOX1\n",
            "\n",
            "\n",
            "hsa04115\n",
            "\n",
            "CCNB3\n",
            "\n",
            "\n",
            "hsa05200\n",
            "\n",
            "GSTP1\n",
            "\n",
            "\n",
            "hsa05100\n",
            "\n",
            "NCKAP1\n",
            "\n",
            "\n",
            "hsa04922\n",
            "\n",
            "PRKAG3\n",
            "\n",
            "\n",
            "hsa05160\n",
            "\n",
            "IL15RA\n",
            "\n",
            "\n",
            "hsa00532\n",
            "\n",
            "GANC\n",
            "\n",
            "\n",
            "hsa03440\n",
            "\n",
            "PPP3CA\n",
            "\n",
            "\n",
            "hsa04722\n",
            "\n",
            "CACNA1S\n",
            "\n",
            "\n",
            "hsa04914\n",
            "\n",
            "ADCY6\n",
            "\n",
            "\n",
            "hsa04115\n",
            "\n",
            "MCM4\n",
            "\n",
            "\n",
            "hsa04964\n",
            "\n",
            "CHRM3\n",
            "\n",
            "\n",
            "hsa04514\n",
            "\n",
            "ICOSLG\n",
            "\n",
            "\n",
            "hsa04141\n",
            "\n",
            "MAP2K2\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRUcl0McEyG_"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.lineplot(x=np.arange(len(val_loss_list)), y = val_loss_list)\n",
        "#sns.lineplot(x=np.arange(len(loss_list)), y = loss_list)\n",
        "#plt.legend(labels=[\"val_loss\",\"train_loss\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUYuCowVKv1p"
      },
      "source": [
        "sns.lineplot(x=np.arange(len(loss_list)), y = loss_list)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}