{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Type1_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ah0Pi_oohMwC",
        "uY4m-EwchDKS",
        "6-vSyAFrgsn_",
        "lKDj6DY6g6oa",
        "xCxMC0qmik9Z",
        "rXipmeH0iwmJ",
        "XiRXWeucjZ74"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-rz32vHCmMz",
        "outputId": "dd64dabb-7186-43df-df51-37176000f4cb"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah0Pi_oohMwC"
      },
      "source": [
        "# DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOLoxVqsCz9A",
        "outputId": "b7432d5d-c46b-4055-d655-fe57ced96f24"
      },
      "source": [
        "import pandas as pd\n",
        "GEB = pd.read_csv(\"/content/gdrive/My Drive/csy/Capstone Design Data _tpm.csv\",index_col= 0)  # col : cell line, index : gene 으로 구성된 데이터프레임\n",
        "GEB_IC = GEB.iloc[37262:37267,:]\n",
        "GEB_ = GEB.iloc[:37262,:]\n",
        "real_index = GEB_.index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsm41y8YDBl6"
      },
      "source": [
        "import numpy as np\n",
        "log = np.log(GEB_IC.iloc[3,:].astype(\"float\"))         # Target 값인 IC50 Value를 얻어내는 과정\n",
        "log = log.T\n",
        "log = pd.DataFrame(log)\n",
        "log['target'] = log['IC50'].apply(lambda x : 1\n",
        "                                  if (x > 5)\n",
        "                                  else 0)\n",
        "log = log.astype('object')\n",
        "log = log.drop(['IC50'], axis = 1)\n",
        "log = log.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcjP-RmwC0Mc"
      },
      "source": [
        "import numpy as np\n",
        "dataset = np.load('/content/gdrive/My Drive/csy/dataset_KEGG.npz', allow_pickle=True)\n",
        "adj_list = dataset['adj_list']   # 각 pathway 별 인접행렬\n",
        "b_list = dataset['b_list']       # 각 pathway에 속해 있는 gene list\n",
        "adj_list = adj_list\n",
        "proteins_by_pathway = b_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRsRAoP2C0TG"
      },
      "source": [
        "unique = np.unique(sum(proteins_by_pathway, []))             \n",
        "gene_index = dict(zip(unique, np.zeros(len(unique))))\n",
        "for i in b_list:\n",
        "  for j in i:\n",
        "    gene_index[j] += 1\n",
        "import collections\n",
        "sorted_by_value = sorted(gene_index.items(), key=lambda x : x[1], reverse=True)\n",
        "sorted_dict = collections.OrderedDict(sorted_by_value)\n",
        "#print(sorted_dict)\n",
        "gene_index = list(sorted_dict.keys())\n",
        "new_gene_indexing = []\n",
        "for ind, k in enumerate(gene_index):\n",
        "  if ((ind+1)%2==1):\n",
        "    new_gene_indexing.insert(0, k)\n",
        "  else:\n",
        "    new_gene_indexing.append(k)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY4m-EwchDKS"
      },
      "source": [
        "# Graph Model Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz9a3z2dC0Zk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import scipy.sparse as sp\n",
        "def clones( module, N):\n",
        "    return nn.ModuleList(copy.deepcopy(module) for _ in range(N))\n",
        "\n",
        "class GCNLayer(nn.Module):                                                  # 가장 기본적인 GCN Layer 생성\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, bn=True, num_head=1):                          \n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.use_bn = bn                                                    # layer를 통과할 때 마다 포함된 값들을 normalization\n",
        "        self.linear = nn.Linear(in_dim, out_dim)                            # 각 노드의 feature에 weight와 bias 부여 \n",
        "        #nn.init.xavier_uniform_(self.linear.weight)\n",
        "        nn.init.kaiming_normal_(self.linear.weight)\n",
        "        #self.bn = nn.BatchNorm1d(out_dim)\n",
        "        self.attention = Attention(out_dim, out_dim, num_head)\n",
        "    \n",
        "        \n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        out = self.linear(x)\n",
        "        \n",
        "        out = self.attention(out,adj)\n",
        "       # out = self.bn(out)\n",
        "        #norm = nn.LayerNorm(out.shape[2], elementwise_affine=False)\n",
        "        #out = norm(out)                                                  # Activation function, 음수 값은 0으로 내보내는 Relu가 아닌 음수일 때 그 값을 exponenetial 값을 취해주는\n",
        "        out = F.relu(out)                                                # Elu 함수를 사용\n",
        "        #out = F.elu(out,alpha=1.0) \n",
        "        return out, adj                                                              \n",
        "\n",
        "class SkipConnection(nn.Module):                                            # ResNet처럼 GCN Block이 통과할 때마다 이전 값을 포함해주는 skip connection 기능을 수행\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(SkipConnection, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        \n",
        "    def forward(self, in_x, out_x):\n",
        "        if (self.in_dim != self.out_dim):\n",
        "            in_x = self.linear(in_x)\n",
        "        out = in_x + out_x                                                  # GCNBlock class 안에서는 in_x : residual(전 block에서 생성된 output), output : 현재 Block에서 생성된 output \n",
        "        return out\n",
        "\n",
        "class GatedSkipConnection(nn.Module):                                       # residual 학습을 할 때 z_coefficient를 추가하여 residual값과 해당 block output 값의 중요도를 계산하여 추가\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(GatedSkipConnection, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
        "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
        "        nn.init.xavier_uniform_(self.linear.weight)\n",
        "        nn.init.xavier_uniform_(self.linear_coef_in.weight)\n",
        "        nn.init.xavier_uniform_(self.linear_coef_out.weight)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, in_x, out_x):\n",
        "        if (self.in_dim != self.out_dim):\n",
        "            in_x = self.linear(in_x)\n",
        "        z = self.gate_coefficient(in_x, out_x)\n",
        "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
        "        return out\n",
        "            \n",
        "    def gate_coefficient(self, in_x, out_x):\n",
        "        x1 = self.linear_coef_in(in_x)\n",
        "        x2 = self.linear_coef_out(out_x)\n",
        "        return self.sigmoid(x1+x2)\n",
        "\n",
        "class GCNBlock(nn.Module):                                                 # 앞서 언급되었던 GCN Layer와 skip connection 기능을 모두 포함하고 있는 GCN Block 생성\n",
        "    \n",
        "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, sc, bn=True): \n",
        "        super(GCNBlock, self).__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,            # n_layer 만큼 GCN layer를 생성하여 nn.MouduleList()에 추가\n",
        "                                        out_dim if i==n_layer-1 else hidden_dim,\n",
        "                                        bn=True))\n",
        "        self.relu = nn.ReLU()\n",
        "        if sc=='gsc':\n",
        "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
        "        elif sc=='sc':\n",
        "            self.sc = SkipConnection(in_dim, out_dim)\n",
        "        elif sc=='no':\n",
        "            self.sc = None\n",
        "        else:\n",
        "            assert False, \"Wrong sc type.\"\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        residual = x                                                      \n",
        "        for i, layer in enumerate(self.layers):\n",
        "            out, adj = layer((x if i==0 else out), adj)\n",
        "        if self.sc != None:\n",
        "            out = self.sc(residual, out)\n",
        "        out = F.relu(out)\n",
        "        return out, adj\n",
        "class Attention(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, output_dim, num_head):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.num_head = num_head # multi-head attention: num_head>1\n",
        "        self.atn_dim = output_dim // num_head\n",
        "        \n",
        "        self.linears = nn.ModuleList()\n",
        "        self.corelations = nn.ParameterList()\n",
        "        for i in range(self.num_head):\n",
        "            self.linears.append(nn.Linear(in_dim, self.atn_dim))       # H*W를 만들어주는 과정 [gene X self.atn_dim]\n",
        "            corelation = torch.FloatTensor(self.atn_dim, self.atn_dim) # attention_dim*attention_dim matrix(C)\n",
        "            nn.init.xavier_uniform_(corelation)\n",
        "            self.corelations.append(nn.Parameter(corelation))\n",
        "      \n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        heads = list()\n",
        "        for i in range(self.num_head):\n",
        "            x_transformed = self.linears[i](x) # num_aton * attention_dim     \n",
        "            alpha = self.attention_matrix(x_transformed, self.corelations[i], adj)\n",
        "            x_head = torch.matmul(alpha, x_transformed)\n",
        "            heads.append(x_head)\n",
        "        output = torch.cat(heads, dim=2)\n",
        "        \n",
        "        return output\n",
        "            \n",
        "    def attention_matrix(self, x_transformed, corelation, adj):\n",
        "        x = torch.einsum('akj,ij->aki', (x_transformed, corelation))\n",
        "        alpha = torch.matmul(x, torch.transpose(x_transformed, 1, 2))\n",
        "        alpha = torch.mul(alpha, adj)\n",
        "        alpha = self.tanh(alpha)\n",
        "        return alpha\n",
        "\n",
        "class ReadOut(nn.Module):                                                # GCN Block을 거쳐 나온 matrix 값을 Nx1 크기로 Flatten (size는 해당 pathway에 속해있는 gene의 개수)\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, act=None):\n",
        "        super(ReadOut, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim= out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(self.in_dim, \n",
        "                                self.out_dim)\n",
        "        #nn.init.xavier_uniform_(self.linear.weight)\n",
        "        nn.init.kaiming_normal_(self.linear.weight)\n",
        "        self.activation = act\n",
        "        #self.norm = nn.LayerNorm(out_dim, elementwise_affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = torch.sum(out, 2)\n",
        "        if self.activation != None:\n",
        "            out = self.activation(out)\n",
        "        #norm = nn.LayerNorm(out.shape[1], elementwise_affine=False)\n",
        "        #out = norm(out)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB6l4cejNA91"
      },
      "source": [
        "--------------Transformer Code--------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-vSyAFrgsn_"
      },
      "source": [
        "# Transformer Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hopwgEijC0wn"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim,  n_heads, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "    assert hidden_dim % n_heads == 0\n",
        "\n",
        "    self.hidden_dim = hidden_dim # 임베딩 차원 -> encoder에서는 gene개수\n",
        "    self.n_heads = n_heads # head 수 -> 아마 1도 해도 될지도..??\n",
        "    self.head_dim = hidden_dim//n_heads # 각 헤드의 임베딩 차원 \n",
        "\n",
        "    self.fc_q = nn.Linear(hidden_dim, hidden_dim, bias = False) # Weight(query)\n",
        "   \n",
        "    self.fc_k = nn.Linear(hidden_dim, hidden_dim, bias = False) # Weight(key)\n",
        "    \n",
        "    self.fc_v = nn.Linear(hidden_dim, hidden_dim, bias = False) # Weight(value)\n",
        "\n",
        "    self.fc_o = nn.Linear(hidden_dim, hidden_dim, bias = False) ## True로 바꿔주기\n",
        "    \n",
        "    nn.init.xavier_uniform_(self.fc_q.weight)\n",
        "    nn.init.xavier_uniform_(self.fc_k.weight)\n",
        "    nn.init.xavier_uniform_(self.fc_v.weight)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  def forward(self, query, key, value, mask = True):\n",
        "    batch_size = query.shape[0]\n",
        "    # query: [batch_size, query_len, hidden_dim] -> [batch_size, pathway, gene]\n",
        "    # key: [batch_size, key_len, hidden_dim]     -> [batch_size, pathway, gene]\n",
        "    # value: [batch_size, value_len, hidden_dim] -> [batch_size, pathway, gene]\n",
        "\n",
        "\n",
        "    Q = self.fc_q(query)\n",
        "    \n",
        "    K = self.fc_k(key)\n",
        "  \n",
        "    V = self.fc_v(value)\n",
        "  \n",
        "    # Q: [batch_size, query_len, hidden_dim] -> [batch_size, pathway, gene]\n",
        "    # K: [batch_size, key_len, hidden_dim]   -> [batch_size, pathway, gene]\n",
        "    # V: [batch_size, value_len, hidden_dim] -> [batch_size, pathway, gene]\n",
        "\n",
        "\n",
        "    # hidden_dim → n_heads X head_dim 형태로 변형\n",
        "    # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도   \n",
        "    Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    # Q: [batch_size, n_heads, query_len, head_dim] -> [batch_size, n_heads, Q_pathway, gene//n_heads]\n",
        "    # K: [batch_size, n_heads, key_len, head_dim]   -> [batch_size, n_heads, K_pathway, gene//n_heads]\n",
        "    # V: [batch_size, n_heads, value_len, head_dim] -> [batch_size, n_heads, V_pathway, gene//n_heads]  \n",
        "\n",
        "\n",
        "    \n",
        " \n",
        "    # Attention Energy 계산\n",
        "    energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale  # K.permute(0, 1, 3, 2) : K의 역행렬\n",
        "    # energy: [batch_size, n_heads, Q_pathway, K_pathway]\n",
        "    \n",
        "    #if mask==True:\n",
        "      \n",
        "    #  criter = torch.zeros(energy.shape).to('cuda')\n",
        "    #  boool = criter == energy\n",
        "    #  energy = energy.masked_fill(boool==True, 0)      \n",
        "      \n",
        "\n",
        "    # mask를 사용할 경우, 0일 때 -1e10으로 채우기\n",
        "\n",
        "\n",
        "    #softmax 함수 제거\n",
        "    attention = torch.relu(energy) # K_pathway에 대하여\n",
        " \n",
        "    # attention: [batch_size, n_heads, Q_pathway, K_pathway]\n",
        "\n",
        "    \n",
        "    # 여기에서 Scaled Dot-Product Attention을 계산\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "    # x: [batch_size, n_heads, Q_pathway, gene//n_heads]\n",
        "    x = torch.relu(x)\n",
        " \n",
        "    x = x.permute(0, 2, 1, 3).contiguous()\n",
        "    # x: [batch_size, Q_pathway, n_heads, gene//n_heads]\n",
        "\n",
        "\n",
        "    x = x.view(batch_size, -1, self.hidden_dim)\n",
        "    # x: [batch_size, Q_pathway, gene]\n",
        "\n",
        "\n",
        "    x = self.fc_o(x)\n",
        "    # x: [batch_size, Q_pathway, gene]\n",
        "\n",
        "\n",
        "    return x, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhk-P_XPW0aU"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        #self.fc_1 = nn.Linear(hidden_dim, pf_dim)  # hidden_dim -> gene\n",
        "        #self.fc_2 = nn.Linear(pf_dim, hidden_dim)  # pf_dim -> 아무 숫자 bias = False\n",
        "        self.fc_1 = nn.Linear(hidden_dim, pf_dim, bias = False)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hidden_dim, bias = False)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x: [batch_size, Q_pathway, gene]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "\n",
        "        # x: [batch_size, Q_pathway, pf_dim]\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "\n",
        "        # x: [batch_size, Q_pathway, gene]\n",
        "\n",
        "        return x\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    #self.self_attn_layer_norm = nn.LayerNorm(hidden_dim) # encoder단계에서 첫번째 norm\n",
        "    #self.ff_layer_norm = nn.LayerNorm(hidden_dim) # encoder단계에서 두번째 norm\n",
        "    self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "      # src: [batch_size, pathway, gene]    \n",
        "      # src_mask: [batch_size, pathway]\n",
        "\n",
        "\n",
        "    # self attention\n",
        "    # 필요한 경우 masking 사용\n",
        "    _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "    # dropout, residual connection and layer norm\n",
        "    \n",
        "    #src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "    src = src+self.dropout(_src)\n",
        " \n",
        "    # src: [batch_size pathway, gene]\n",
        "    \n",
        "\n",
        "\n",
        "    # position-wise feedforward\n",
        "    _src = self.positionwise_feedforward(src)\n",
        "    # dropout, residual and layer norm\n",
        "    #src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "    src = src+self.dropout(_src)\n",
        "    # src: [batch_size pathway, gene]\n",
        "\n",
        "    return src\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "\n",
        "  def forward(self, src, src_mask):  # final input data In GCN process\n",
        "        # src: [batch_size, pathway, gene]    \n",
        "        # src_mask: [batch_size, pathway]\n",
        "    \n",
        "    \n",
        "    for layer in self.layers:\n",
        "      src = layer(src, src_mask)\n",
        "      # src: [batch_size, pathway, gene]\n",
        "\n",
        "    return src  # src: [batch_size, pathway, gene]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c181kuV_l49y"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DE_MultiHeadAttentionLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, de_hidden_dim,  n_heads, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "    assert hidden_dim % n_heads == 0\n",
        "\n",
        "    self.hidden_dim = hidden_dim # 임베딩 차원 -> encoder에서는 gene개수\n",
        "    self.n_heads = n_heads # head 수 -> 아마 1도 해도 될지도..??\n",
        "    self.head_dim = hidden_dim//n_heads # 각 헤드의 임베딩 차원 \n",
        "    self.de_head_dim = de_hidden_dim//n_heads\n",
        "    self.fc_q = nn.Linear(de_hidden_dim, de_hidden_dim, bias = False) # Weight(query)\n",
        "   \n",
        "    self.fc_k = nn.Linear(hidden_dim, hidden_dim, bias = False) # Weight(key)\n",
        "    \n",
        "    self.fc_v = nn.Linear(hidden_dim, hidden_dim, bias = False) # Weight(value)\n",
        "\n",
        "    self.fc_o = nn.Linear(hidden_dim, hidden_dim, bias = False) \n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "    \n",
        "    nn.init.xavier_uniform_(self.fc_q.weight)\n",
        "    nn.init.xavier_uniform_(self.fc_k.weight)\n",
        "    nn.init.xavier_uniform_(self.fc_v.weight)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "    batch_size = query.shape[0]\n",
        "    # query: [batch_size, query_len, hidden_dim] -> [batch_size, gene, pathway]\n",
        "    # key: [batch_size, key_len, hidden_dim]     -> [batch_size, pathway, gene]\n",
        "    # value: [batch_size, value_len, hidden_dim] -> [batch_size, pathway, gene]\n",
        "\n",
        "    Q = self.fc_q(query)\n",
        "\n",
        "    K = self.fc_k(key)\n",
        " \n",
        "    V = self.fc_v(value)\n",
        "  \n",
        "    # Q: [batch_size, query_len, hidden_dim] -> [batch_size, gene, pathway]\n",
        "    # K: [batch_size, key_len, hidden_dim]   -> [batch_size, pathway, gene]\n",
        "    # V: [batch_size, value_len, hidden_dim] -> [batch_size, pathway, gene]\n",
        "    \n",
        "\n",
        "    # hidden_dim → n_heads X head_dim 형태로 변형\n",
        "    # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도   \n",
        "    #Q = Q.view(batch_size, -1, self.n_heads, self.de_head_dim).permute(0, 2, 1, 3)\n",
        "    #K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    #V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    # Q: [batch_size, n_heads, query_len, head_dim] -> [batch_size, n_heads, gene//n_heads, Q_pathway]\n",
        "    # K: [batch_size, n_heads, key_len, head_dim]   -> [batch_size, n_heads, K_pathway, gene//n_heads]\n",
        "    # V: [batch_size, n_heads, value_len, head_dim] -> [batch_size, n_heads, V_pathway, gene//n_heads]  \n",
        "  \n",
        " \n",
        "    #print(Q.shape, K.shape)\n",
        "    # Attention Energy 계산\n",
        "    energy = torch.matmul(Q, K) / self.scale  # K.permute(0, 1, 3, 2) : K의 역행렬\n",
        "    # energy: [batch_size, gene, gene]\n",
        "\n",
        "  \n",
        "    # mask를 사용할 경우, 0일 때 -1e10으로 채우기\n",
        "   # if mask==True:\n",
        "      \n",
        "   #   criter = torch.zeros(energy.shape).to('cuda')\n",
        "   #   boool = criter == energy\n",
        "   #   energy = energy.masked_fill(boool==True, 0)\n",
        "\n",
        "    # softmax 함수 제거\n",
        "    attention = torch.relu(energy) # K_pathway에 대하여\n",
        "    # attention: [batch_size, n_heads, gene, gene]\n",
        "    \n",
        "    # 여기에서 Scaled Dot-Product Attention을 계산\n",
        "    \n",
        "    x = torch.matmul(self.dropout(attention), V.permute(0, 2, 1))\n",
        "    # V.permute(0, 1, 3, 2) : [batch_size, n_heads, gene//n_heads, pathway]\n",
        "    # x: [batch_size, n_heads, Q_pathway, gene//n_heads]\n",
        "    x = torch.relu(x)\n",
        "    \n",
        "    #x = x.permute(0, 2, 1, 3).contiguous()\n",
        "    # x: [batch_size, gene//n_heads,  n_heads, Q_pathway]\n",
        "\n",
        "\n",
        "    #x = x.view(batch_size, -1, self.hidden_dim)\n",
        "    # x: [batch_size, Q_pathway, gene]\n",
        "\n",
        "\n",
        "    #x = self.fc_o(x)\n",
        "    # x: [batch_size, gene, pathway]\n",
        "\n",
        "\n",
        "    return x, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2rrdGmdZGbd"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_dim, de_hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    #self.self_attn_layer_norm = nn.LayerNorm(de_hidden_dim)\n",
        "    #self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "    \n",
        "    self.self_attention = MultiHeadAttentionLayer(de_hidden_dim, n_heads, dropout_ratio, device)\n",
        "    #self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.encoder_attention = DE_MultiHeadAttentionLayer(hidden_dim,de_hidden_dim, n_heads, dropout_ratio, device)\n",
        "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(de_hidden_dim, pf_dim, dropout_ratio)\n",
        "    self.dropout = nn.Dropout(dropout_ratio)\n",
        "    self.fc_in_dim = hidden_dim*de_hidden_dim\n",
        "    self.fincal_fc1 = nn.Linear(self.fc_in_dim, 10000, bias = False)  # bias = True로 변경해주어야 함\n",
        "    self.fincal_bn1 = nn.BatchNorm1d(10000)\n",
        "    self.fincal_fc2 = nn.Linear(10000, 100, bias = False)\n",
        "    self.fincal_bn2 = nn.BatchNorm1d(100)\n",
        "    self.fincal_fc3 = nn.Linear(100, 2, bias = False)\n",
        "    self.final_fc = nn.Linear(self.fc_in_dim, 2, bias = False)\n",
        "\n",
        "  def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "    # trg : [batch_size, trg_len, hidden_dim] - > [batch_size, gene, pathway]  ->  src와 반대\n",
        "    # enc_src : [batch_size, src_len, hidden_dim] - > [batch_size, pathway, gene]\n",
        "    # trg_mask: [batch_size, trg_len] - > [batch_size, gene]\n",
        "    # src_mask: [batch_size, src_len] - > [batch_size, pathway]\n",
        "\n",
        "    \n",
        "    # self attention\n",
        "    # Output Embeding Matrix에 대하여 self attention\n",
        "    _trg, _ = self.self_attention(trg, trg, trg, trg_mask) \n",
        "   \n",
        "    # dropout, residual connection and layer norm\n",
        "    #trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "    trg = trg + self.dropout(_trg)\n",
        "    # trg: [batch_size, trg_len, hidden_dim] - > [batch_size, gene, pathway]\n",
        "    \n",
        "\n",
        "    ##중요##\n",
        "    # Encoder Attention\n",
        "    # Decoder의 query를 Encoder의 key, value를 이용하여 Attention\n",
        "    _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask) ## 이 때 우리의 경우에는 Key matrix에 역행렬을 취해주면 안됌. 별도의 코드 필요!!\n",
        "   \n",
        "    \n",
        "    # dropout, residual connection and layer norm\n",
        "  \n",
        "    #trg = self.enc_attn_layer_norm(trg+self.dropout(_trg))\n",
        "    trg = trg + self.dropout(_trg)\n",
        "    # trg: [batch_size, gene, pathway]\n",
        "    \n",
        "\n",
        "    # positionwise feedforward\n",
        "    _trg = self.positionwise_feedforward(trg)\n",
        "    \n",
        "    # dropout, residual and layer norm\n",
        "    #trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "    trg = trg + self.dropout(_trg)\n",
        "\n",
        "    # trg: [batch_size, gene, pathway]\n",
        "    # attention: [batch_size, n_heads, trg_len, src_len] -> [batch_size, n_heads, gene, pathway]\n",
        "    ## attention은 concat 하기 전이기 때문에 n_heads가 남아있음\n",
        "    trg_conti = trg.contiguous()\n",
        "   \n",
        "    readout = trg_conti.view(trg.shape[0], -1)\n",
        "    #output = self.fincal_fc1(readout)\n",
        "    #output = self.fincal_bn1(output)\n",
        "    #output = self.fincal_fc2(output)\n",
        "    #output = self.fincal_bn2(output)\n",
        "    #output = self.fincal_fc3(output)\n",
        "    output = self.final_fc(readout)\n",
        "    return output, trg, attention\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, hidden_dim, de_hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "   \n",
        "    self.encoder = Encoder(hidden_dim,n_layers, n_heads ,pf_dim, dropout_ratio, device) # n_heads\n",
        "    self.decoder = DecoderLayer(hidden_dim, de_hidden_dim, n_heads, pf_dim, dropout_ratio, device)\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    enc_src = self.encoder(src, src_mask=True)\n",
        "    \n",
        "    output, t, attention = self.decoder(trg, enc_src, trg_mask=True, src_mask=True)\n",
        "\n",
        "    return output, t, attention\n",
        "\n",
        "    ## output은 size가 2인 텐서 형태로... ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #  [gene 수,    pathway 수 ... ...]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivfWS4KRSy4r"
      },
      "source": [
        "# GCN -> CNN 학습코드 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXmdb89JQ-Tk"
      },
      "source": [
        "def mean_norm(df_input):\n",
        "    return df_input.apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
        "def cell_line_dicimal_norm(G):\n",
        "   GEB1 = GEB_[[G]]\n",
        "   GEB1 = GEB1.astype('float')\n",
        "   GEB1 = mean_norm(GEB1)\n",
        "   protein_features_df = GEB1.T\n",
        "   protein_features_df = protein_features_df.astype('float64')\n",
        "   return protein_features_df\n",
        "def cell_line_dicimal(G):\n",
        "   GEB1 = GEB_[[G]]\n",
        "   GEB1 = GEB1.astype('float')\n",
        "   GEB1 = GEB1/292290725368.0\n",
        "   protein_features_df = GEB1.T\n",
        "   protein_features_df = protein_features_df.astype('float64')\n",
        "   return protein_features_df\n",
        "#################################\n",
        "def cell_line_binary(G):                                         # gene expression 값이 Float 형태였던 데이터형태를 이진수로 변환해주는 함수\n",
        "        GEB1 = GEB_[[G]]\n",
        "        GEB1['B'] = GEB1[G].astype('str').str.zfill(24)\n",
        "        non_dot_list = []\n",
        "        for i in GEB1['B'].values:\n",
        "          i = i.replace(\".\", \"0\")\n",
        "          non_dot_list.append(i)\n",
        "        GEB1['B'] = non_dot_list\n",
        "        GEB2 = GEB1[['B']]\n",
        "        dat = [list(map(int, str(x).zfill(6))) for x in GEB2.B]\n",
        "        d = pd.DataFrame(dat, GEB2.index).rename(columns=lambda x: f'2^{23 - x}')\n",
        "        GEB2.join(d)\n",
        "\n",
        "        protein_features_df = GEB2.join(d).drop(['B'], axis = 1)\n",
        "        protein_features_df = protein_features_df.transpose()\n",
        "\n",
        "        return protein_features_df\n",
        "##################################        \n",
        "def selection(pathway_gene):                                    # 최종적으로 만들어진 GCN output의 크기가 6000x1로 너무 커서 모든 pathway에 값이 0으로 되어 있는 gene을 제거하여  \n",
        "                                                                # 400~500 x 1 형태로 변환       \n",
        "    row_list = []\n",
        "    pathway_gene_ = pathway_gene[0,:,:]\n",
        "    for row in range(pathway_gene_.shape[1]):\n",
        "      if (sum(pathway_gene_[:,row] == 0)) < 295 :\n",
        "        row_list.append(row)\n",
        "    return pathway_gene[:,:,row_list]\n",
        "\n",
        "##################################\n",
        "def normalize_adj(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)) # D\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten() # D^-0.5\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # D^-0.5\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).toarray()\n",
        "\n",
        "class GraphResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GraphResNet, self).__init__()     \n",
        "        self.GCN_layer = nn.ModuleList()                                # GCN Block과 readout을 포함하고 있는 nn.ModuleList() 생성\n",
        "        self.GCN_layer.append(GCNBlock(2, 1, 2, 2, sc='gsc'))       # nn.Mouduleist()에 포함되어 있는 class 들은 for문을 통해 그 안에 속해있는 모든 function들을 수행\n",
        "        self.GCN_layer.append(GCNBlock(2, 2, 2, 2, sc='gsc'))       # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=nn%20modulelist#torch.nn.ModuleList.append\n",
        "        self.GCN_layer.append(GCNBlock(2, 2, 2, 2, sc='gsc')) \n",
        "        self.GCN_layer.append(ReadOut(2, 1))\n",
        "        self.GCN_layer_list = nn.ModuleList()\n",
        "        for _ in list(range(308)):\n",
        "          self.GCN_layer_list.append(self.GCN_layer) \n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(1,5), stride=1, bias = False)\n",
        "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=(1,5), stride=1, bias = False)\n",
        "        self.conv3 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(1,5), stride=1, bias = False)\n",
        "        self.conv4 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(1,5), stride=1, bias = False)\n",
        "        #self.conv5 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=5, stride=1)\n",
        "        #self.conv6 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=5, stride=1)\n",
        "       \n",
        "        self.fc1 = nn.Linear(44352, 500) ###### !!!!!!!!!!중요!!!!!!!!!!!!!!! gene 개수 다를 때 바꾸어주어야 할 부분\n",
        "        self.fc2 = nn.Linear(500, 2)\n",
        "       \n",
        "\n",
        "\n",
        "    def forward(self, bat_tensor):                      # randome_train_list : 800개의 cell line 중 미리 random 함수를 통해 700개의 cell line list 생성\n",
        "        \n",
        "        batch_size = bat_tensor.shape[0]                                                                   # 메모리 문제로 batch_size를 호출 받아 직접 쌓는 방식으로 구현            \n",
        "          \n",
        "        pre_input_tensor = []                                              # [batch, 1, H, W] 형태로 최종 CNN input 텐서 생성\n",
        "        random_cell_line = []        \n",
        "        uni_list_prot = np.unique(sum(proteins_by_pathway, []))                  \n",
        "        indexing = dict(zip(new_gene_indexing, np.arange(len(uni_list_prot))))\n",
        "        i_ = list(range((342)))        \n",
        "        protein_features_df = GEB_.T                                          \n",
        "        col_list = list(protein_features_df.columns)\n",
        "        pathway_gene = []\n",
        "        for i, adj, b in zip(i_, adj_list, b_list):                         # Pathway 별로 GCN 연산 수         \n",
        "          ##\n",
        "          col_gene_ind = []\n",
        "          for col_gene in b:\n",
        "            col_gene_ind.append(col_list.index(col_gene))\n",
        "     \n",
        "          x = bat_tensor[:,col_gene_ind,:]\n",
        "          adj = normalize_adj(adj)         \n",
        "          x, adj = torch.tensor(x).to('cuda').float(), torch.tensor(adj).to('cuda').float()  \n",
        "       \n",
        "          for k, layer in enumerate(self.GCN_layer_list[i]): \n",
        "            if k != 3:                                                 \n",
        "              out, adj = layer((x if k==0 else out), adj)             \n",
        "        \n",
        "            else:              \n",
        "              output = layer(out)\n",
        "         \n",
        "      \n",
        "          fix_list = torch.zeros([batch_size, len(uni_list_prot)])\n",
        "          for batch in range(batch_size):\n",
        "            for gene_index, gene in enumerate(b):\n",
        "              fix_list[batch, indexing[gene]] = output[batch, gene_index]\n",
        "          \n",
        "          \n",
        "          pathway_gene.append(fix_list)\n",
        "        pathway_gene_ = torch.stack(pathway_gene, dim = 1) \n",
        "        \n",
        "        #pathway_gene = selection(pathway_gene_)\n",
        "        pathway_gene = pathway_gene_\n",
        "        \n",
        "        input_data_depth10 = pathway_gene.to('cuda').float()\n",
        "      \n",
        "        x = input_data_depth10[:,np.newaxis,:,:]                            \n",
        "      \n",
        "        ###### !!!!!!!!!!중요!!!!!!!!!!!!!!! gene 개수 다를 때 바꾸어주어야 할 부분\n",
        "       \n",
        "        x = F.relu(self.conv1(x))                              # 아래의 과정은 기본적인 CNN 연산\n",
        "        \n",
        "        x = F.max_pool2d(x, kernel_size=(1,5), stride=(1,5))\n",
        "        \n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(1,5), stride=(1,5))\n",
        "        \n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(1,5), stride=(1,5))\n",
        "\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(1,5), stride=(1,5))\n",
        "\n",
        "       \n",
        "\n",
        "                                            \n",
        "        #print('제발 잘 학습해줘....')\n",
        "        x = x.view(batch_size, -1)   ###### !!!!!!!!!!중요!!!!!!!!!!!!!!! gene 개수 다를 때 바꾸어주어야 할 부분\n",
        "       \n",
        "        x = F.relu(self.fc1(x))\n",
        "        \n",
        "        x = self.fc2(x)        \n",
        "        T = 0  \n",
        "        return x, T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKDj6DY6g6oa"
      },
      "source": [
        "# GCN -> Transformer 학습 코드 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtCeHWDEC1FP"
      },
      "source": [
        "def mean_norm(df_input):\n",
        "    return df_input.apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
        "\n",
        "def cell_line_dicimal_norm(G):\n",
        "   GEB1 = GEB_[[G]]\n",
        "   GEB1 = GEB1.astype('float')\n",
        "   GEB1 = mean_norm(GEB1)\n",
        "   protein_features_df = GEB1.T\n",
        "   protein_features_df = protein_features_df.astype('float64')\n",
        "   return protein_features_df\n",
        "\n",
        "def cell_line_dicimal(G):\n",
        "   GEB1 = GEB_[[G]]\n",
        "   GEB1 = GEB1.astype('float')\n",
        "   GEB1 = GEB1/292290725368.0\n",
        "   protein_features_df = GEB1.T\n",
        "   protein_features_df = protein_features_df.astype('float64')\n",
        "   return protein_features_df\n",
        "   \n",
        "def cell_line_binary(G):                                         # gene expression 값이 Float 형태였던 데이터형태를 이진수로 변환해주는 함수\n",
        "        GEB1 = GEB_[[G]]\n",
        "        GEB1['B'] = GEB1[G].astype('str').str.zfill(24)\n",
        "        non_dot_list = []\n",
        "        for i in GEB1['B'].values:\n",
        "          i = i.replace(\".\", \"0\")\n",
        "          non_dot_list.append(i)\n",
        "        GEB1['B'] = non_dot_list\n",
        "        GEB2 = GEB1[['B']]\n",
        "        dat = [list(map(int, str(x).zfill(6))) for x in GEB2.B]\n",
        "        d = pd.DataFrame(dat, GEB2.index).rename(columns=lambda x: f'2^{23 - x}')\n",
        "        GEB2.join(d)\n",
        "        protein_features_df = GEB2.join(d).drop(['B'], axis = 1)\n",
        "        protein_features_df = protein_features_df.transpose()\n",
        "        return protein_features_df\n",
        "    \n",
        "def selection_(pathway_gene):                      # 최종적으로 만들어진 GCN output의 크기가 6000x1로 너무 커서 모든 pathway에 값이 0으로 되어 있는 gene을 제거하여                                                         \n",
        "    row_list = []                                  # 400~500 x 1 형태로 변환    \n",
        "    pathway_gene_ = pathway_gene[0,:,:]\n",
        "    for row in range(pathway_gene_.shape[1]):\n",
        "      if (sum(pathway_gene_[:,row] == 0)) < 275 :\n",
        "        row_list.append(row)\n",
        "    return pathway_gene[:,:,row_list]\n",
        "\n",
        "def selection(b_list, pathway_gene):                                                                                             \n",
        "    node_count = []\n",
        "    for  i in b_list:\n",
        "      node_count.append(len(i))\n",
        "    node_count_sort = np.sort(node_count)\n",
        "    path_sele = list(pd.Series(node_count).sort_values()[:300].index)\n",
        "    path_sele.sort()         \n",
        "    row_list = []\n",
        "    pathway_gene_ = pathway_gene[0,path_sele,:]\n",
        "    for row in range(pathway_gene_.shape[1]):\n",
        "      if (sum(pathway_gene_[:,row] == 0)) < 299 :\n",
        "        row_list.append(row)\n",
        "    pathway_gene = pathway_gene[:,path_sele,:]\n",
        "    pathway_gene = pathway_gene[:,:,row_list]\n",
        "    return pathway_gene\n",
        "\n",
        "def std_selection(GEB_, new_gene_indexing, pathway_gene):\n",
        "    val_GEB = GEB_.loc[new_gene_indexing,:].astype('float').std(axis=1)\n",
        "    a = val_GEB>0.25e+9\n",
        "    b = a.values.tolist()\n",
        "    c = pd.Series(a.values, index=np.arange(len(a)))\n",
        "    d = np.array(c[c==True].index)\n",
        "    return pathway_gene[:,:,d]\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)) # D\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten() # D^-0.5\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # D^-0.5\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).toarray()\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "class Attentionisallyouneed(nn.Module):\n",
        "    def __init__(self, hidden_dim, de_hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super(Attentionisallyouneed, self).__init__()     \n",
        "        self.GCN_layer = nn.ModuleList()                                # GCN Block과 readout을 포함하고 있는 nn.ModuleList() 생성\n",
        "        self.GCN_layer.append(GCNBlock(2, 1, 2, 2, sc='gsc'))       # nn.Mouduleist()에 포함되어 있는 class 들은 for문을 통해 그 안에 속해있는 모든 function들을 수행\n",
        "        self.GCN_layer.append(GCNBlock(2, 2, 4, 4, sc='gsc'))       # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=nn%20modulelist#torch.nn.ModuleList.append\n",
        "        self.GCN_layer.append(GCNBlock(2, 4, 4, 4, sc='gsc')) \n",
        "        self.GCN_layer.append(ReadOut(4, 1))\n",
        "        self.GCN_layer_list = nn.ModuleList()  \n",
        "        for _ in list(range(308)):\n",
        "          self.GCN_layer_list.append(self.GCN_layer)\n",
        "        self.transformer = Transformer(hidden_dim, de_hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device)\n",
        "        \n",
        "    def forward(self, bat_tensor):                                                                \n",
        "            \n",
        "        batch_size = bat_tensor.shape[0]\n",
        "        pre_input_tensor = []                                                      \n",
        "        pathway_gene = []\n",
        "        protein_features_df = GEB_.T\n",
        "        uni_list_prot = np.unique(sum(proteins_by_pathway, []))                  \n",
        "        indexing = dict(zip(new_gene_indexing, np.arange(len(uni_list_prot))))\n",
        "        i_ = list(range((342)))                                                    \n",
        "        col_list = list(protein_features_df.columns)\n",
        "        pathway_gene = []\n",
        "\n",
        "        for i, adj, b in zip(i_, adj_list, b_list):                         # Pathway 별로 GCN 연산 수         \n",
        "          ##\n",
        "          col_gene_ind = []\n",
        "          for col_gene in b:\n",
        "            col_gene_ind.append(col_list.index(col_gene))\n",
        "          ##\n",
        "          x = bat_tensor[:,col_gene_ind,:]\n",
        "          adj = normalize_adj(adj)         \n",
        "          x, adj = torch.tensor(x).to('cuda').float(), torch.tensor(adj).to('cuda').float()    # x : feature matrix   adj : 각 pathway의 인접행렬\n",
        "          \n",
        "          \n",
        "    # --------------------GCN 학습 과정--------------------------\n",
        "        \n",
        "          for k, layer in enumerate(self.GCN_layer_list[i]): \n",
        "            if k != 3:                                                           \n",
        "              out, adj = layer((x if k==0 else out), adj)                     \n",
        "            else:              \n",
        "              output = layer(out)\n",
        "    \n",
        "          fix_list = torch.zeros([batch_size, len(uni_list_prot)])\n",
        "          for batch in range(batch_size):\n",
        "            for gene_index, gene in enumerate(b):\n",
        "              fix_list[batch, indexing[gene]] = output[batch, gene_index]                          \n",
        "          pathway_gene.append(fix_list)\n",
        "\n",
        "        pathway_gene = torch.stack(pathway_gene, dim = 1)  \n",
        "        pathway_gene = std_selection(GEB_, new_gene_indexing, pathway_gene)       \n",
        "        input_data_depth10 = pathway_gene\n",
        "       \n",
        "        src = input_data_depth10.to('cuda').float()\n",
        "        trg = src.permute(0, 2, 1).to('cuda').float()\n",
        "        output, t, attention = self.transformer(src, trg)\n",
        "        \n",
        "        return output, t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCxMC0qmik9Z"
      },
      "source": [
        "# GCN -> MLP 학습코드 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alib595HiTXR"
      },
      "source": [
        "class GraphMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GraphMLP, self).__init__()     \n",
        "        self.GCN_layer = nn.ModuleList()                                # GCN Block과 readout을 포함하고 있는 nn.ModuleList() 생성\n",
        "        self.GCN_layer.append(GCNBlock(3, 1, 2, 2, sc='gsc'))       # nn.Mouduleist()에 포함되어 있는 class 들은 for문을 통해 그 안에 속해있는 모든 function들을 수행\n",
        "        self.GCN_layer.append(GCNBlock(3, 2, 1, 1, sc='gsc'))       # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=nn%20modulelist#torch.nn.ModuleList.append\n",
        "        self.GCN_layer.append(GCNBlock(3, 1, 1, 1, sc='gsc')) \n",
        "        self.GCN_layer.append(ReadOut(1, 1))\n",
        "        self.GCN_layer_list = nn.ModuleList()\n",
        "        for _ in list(range(308)):\n",
        "          self.GCN_layer_list.append(self.GCN_layer)        \n",
        "        self.linear1 = nn.Linear(in_features=176, out_features=300, bias=False)    # gene은 총 450개\n",
        "        self.linear2 = nn.Linear(in_features=300, out_features=300, bias=False)\n",
        "        self.linear3 = nn.Linear(in_features=300, out_features=150, bias=False)\n",
        "        self.linear4 = nn.Linear(in_features=150, out_features=1, bias=False)\n",
        "        self.linear5 = nn.Linear(in_features=176, out_features=100, bias=False)\n",
        "        self.linear6 = nn.Linear(in_features=100, out_features=1, bias=False)\n",
        "\n",
        "        self.final = nn.Linear(in_features=308, out_features=2, bias=False) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.linear_list = nn.ModuleList()\n",
        "        self.linear_layer_list = nn.ModuleList()\n",
        "        self.linear_list.append(self.linear5)\n",
        "        self.linear_list.append(self.relu)\n",
        "        self.linear_list.append(self.dropout)\n",
        "        self.linear_list.append(self.linear6)\n",
        "        self.linear_list.append(self.relu)\n",
        "        for _ in list(range(308)):\n",
        "          self.linear_layer_list.append(self.linear_list)\n",
        "    def forward(self, bat_tensor):                      # randome_train_list : 800개의 cell line 중 미리 random 함수를 통해 700개의 cell line list 생성                                                                                   # 메모리 문제로 batch_size를 호출 받아 직접 쌓는 방식으로 구현            \n",
        "              # 특이하게 forward() 과정에서 batch를 생성 -> [H, W]의 matrix를 직접 쌓아\n",
        "        batch_size = bat_tensor.shape[0]\n",
        "        pre_input_tensor = []                                              # [batch, 1, H, W] 형태로 최종 CNN input 텐서 생성              \n",
        "        pathway_gene = []\n",
        "        protein_features_df = GEB_.T\n",
        "        uni_list_prot = np.unique(sum(proteins_by_pathway, []))                  \n",
        "        indexing = dict(zip(new_gene_indexing, np.arange(len(uni_list_prot))))\n",
        "        i_ = list(range((342)))                                                    \n",
        "        col_list = list(protein_features_df.columns)\n",
        "        pathway_gene = []\n",
        "\n",
        "        for i, adj, b in zip(i_, adj_list, b_list):                         # Pathway 별로 GCN 연산 수         \n",
        "          ##\n",
        "          col_gene_ind = []\n",
        "          for col_gene in b:\n",
        "            col_gene_ind.append(col_list.index(col_gene))\n",
        "          ##\n",
        "\n",
        "          x = bat_tensor[:,col_gene_ind,:]\n",
        "          adj = normalize_adj(adj)         \n",
        "          x, adj = torch.tensor(x).to('cuda').float(), torch.tensor(adj).to('cuda').float()    # x : feature matrix   adj : 각 pathway의 인접행렬\n",
        "\n",
        "          \n",
        "            # GCN 학습 과정\n",
        "        \n",
        "          for k, layer in enumerate(self.GCN_layer_list[i]): \n",
        "            if k != 3:                                                    # -> GCNBlock(GCNLayer 10개) -> GCNBlock(GCNLayer 25개) -> Readout -> gene의 숫자가 size인 텐서 생성 \n",
        "              \n",
        "              out, adj = layer((x if k==0 else out), adj)             \n",
        "        \n",
        "            else:              \n",
        "              output = layer(out)\n",
        "   \n",
        "          fix_list = torch.zeros([batch_size, len(uni_list_prot)])\n",
        "          for batch in range(batch_size):\n",
        "            for gene_index, gene in enumerate(b):\n",
        "              fix_list[batch, indexing[gene]] = output[batch, gene_index]\n",
        "\n",
        "          \n",
        "          pathway_gene.append(fix_list)\n",
        "        pathway_gene = torch.stack(pathway_gene, dim = 1) \n",
        "\n",
        "        #pathway_gene = selection(pathway_gene)\n",
        "        pathway_gene = std_selection(GEB_, new_gene_indexing, pathway_gene)\n",
        "\n",
        "       \n",
        "        input_data_depth10 = pathway_gene\n",
        "        input_data_depth10 = input_data_depth10.float()\n",
        "        total_readout = []\n",
        "\n",
        "        for path, layer_list in enumerate(self.linear_layer_list):\n",
        "          x = input_data_depth10[:, path,:].to('cuda')\n",
        "          #x = self.linear1(x)     \n",
        "          #x = self.relu(x)\n",
        "          #x = self.dropout(x)\n",
        "\n",
        "          #x = self.linear2(x)\n",
        "          #x = self.relu(x)     \n",
        "          #x = self.dropout(x)\n",
        "\n",
        "          #x = self.linear3(x)\n",
        "          #x = self.relu(x)\n",
        "          #x = self.dropout(x)\n",
        "          \n",
        "          #x = self.linear4(x)\n",
        "          #x = self.relu(x) \n",
        "\n",
        "          for layer in layer_list: \n",
        "            x = layer(x)\n",
        "          total_readout.append(x)    \n",
        "\n",
        "        total_readout = torch.stack(total_readout, dim = 1).to('cuda')\n",
        "    \n",
        "        total_readout = total_readout.view(batch_size, -1)\n",
        "        output = self.final(total_readout)\n",
        "        t = 0\n",
        "        return output, t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXipmeH0iwmJ"
      },
      "source": [
        "# Training (1epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOq_bsE1cCut"
      },
      "source": [
        "def make_data(GEB, train_set):\n",
        "  random_cell_line = []       \n",
        "  bat_tensor = []  \n",
        "  for cnt, G in enumerate(GEB.columns[list(train_set)]):   \n",
        "    random_cell_line.append(G)\n",
        "    #protein_features_df = cell_line_dicimal_norm(G)\n",
        "    values = cell_line_dicimal(G).T.values\n",
        "    tensor = torch.from_numpy(values)\n",
        "    bat_tensor.append(tensor)\n",
        "  bat_tensor = torch.stack(bat_tensor, dim = 0)\n",
        "  target = torch.tensor(pd.Series(log[random_cell_line].values[0])).to('cuda')\n",
        "  return bat_tensor, target  \n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp  \n",
        "\n",
        "def eval(model, eval_list, batch_size):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  loss = 0\n",
        "  with torch.no_grad():\n",
        "    eval_set = np.random.choice(eval_list, batch_size)\n",
        "    bat_tensor, target = make_data(GEB, eval_set) \n",
        "    output, t = model(bat_tensor)  \n",
        "    loss = criterion(output, target)\n",
        "    _, predicted = output.max(1)\n",
        "    correct = predicted.eq(target).sum().item()\n",
        "  return 100. * correct / batch_size, loss.item()\n",
        "\n",
        "import random\n",
        "random_seed = 1553\n",
        "torch.manual_seed(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "#np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "#torch.cuda.manual_seed(random_seed)\n",
        "#torch.cuda.manual_seed_all(random_seed) # multi-GPU\n",
        "device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbhL5rFi6RW5"
      },
      "source": [
        "------------------여기까지 실행------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUMq4rH964f8",
        "outputId": "aaf963da-72e2-410a-cae0-d5577d89fcea"
      },
      "source": [
        "nums = set()\n",
        "while len(nums) != 100: \n",
        "  nums.add(random.randint(1, 801))\n",
        "eval_list = list(nums)\n",
        "\n",
        "train_list = list(range(1, 801))\n",
        "\n",
        "for i in eval_list:\n",
        "  train_list.remove(i)\n",
        "\n",
        "# hidden_dim, de_hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device\n",
        "#model = Attentionisallyouneed(176, 308, 1, 1, 100, 0, 'cuda').to(device)  # 메모리 때문에 308 -> 89\n",
        "model = GraphResNet().to(device)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=0.05 ) # lr = 1e-3 or 5e-4, weight_decay=0.00005 \n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)#, weight_decay=0.1\n",
        "#optimizer = torch.optim.SparseAdam(model.parameters(), lr=5e-4)        \n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0,  verbose = False)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.3, verbose = True)\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "print(\"total parameter 수 : \", get_n_params(model))\n",
        "# 275664581\n",
        "# 423843235 -> gene 137\n",
        "# 544179655 -> gene 176"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total parameter 수 :  22181176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT_9M5U365C2"
      },
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "batch_size = 16\n",
        "iterations = 40\n",
        "loss_list = []\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "for iteration in range(iterations):\n",
        "  train_set = np.random.choice(train_list, batch_size, replace = False)\n",
        "  for t in train_set:\n",
        "    train_list.remove(t)\n",
        "  bat_tensor, target = make_data(GEB, train_set) \n",
        "  model.train()\n",
        "  train_loss = 0 \n",
        "  optimizer.zero_grad()\n",
        "  output, t = model(bat_tensor) # 최종 output\n",
        "\n",
        "  loss = criterion(output, target)\n",
        "  loss.backward()  # 역전파\n",
        "  optimizer.step()\n",
        "  #scheduler.step()\n",
        "  train_loss += loss.item()\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  print(iteration, \"번째 iteration\")\n",
        "  print(\"train loss : \", train_loss)\n",
        "  loss_list.append(train_loss)\n",
        "  if iteration % 5 == 0:\n",
        "    eval_acc, eval_loss = eval(model, eval_list, 64)\n",
        "    print(\"\\n, eval loss : \", eval_loss,)\n",
        "    print(\"eval accuarcy : \", eval_acc, \"%\")\n",
        "    print('------------------------------------\\n')\n",
        "  # 중요 point ## 역전파가 제대로 되는지 확인하는 단계 ##  ----print(model.GCN_layer[0].layers[0].linear.weight.grad)----\n",
        "  #print(\"time :\", (time.time() - start)/60)\n",
        "print(\"time :\", (time.time() - start)/60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTaHUqUvX_Ku",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e9bca819-8360-4a5e-dc99-10882344de76"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(x=np.arange(len(val_loss_list)), y = val_loss_list)\n",
        "sns.lineplot(x=np.arange(len(loss_list)), y = loss_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f56c56ee490>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "b5QOPmkdH9pq",
        "outputId": "0b468210-92ff-45c3-fea8-a5ab3eb4366a"
      },
      "source": [
        "sns.lineplot(x=np.arange(len(loss_list)), y = loss_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f56c56dab50>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVs4tSHEjMrh"
      },
      "source": [
        "# Training( * epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JQDMKClOMZJ"
      },
      "source": [
        "def make_data(GEB, train_set):\n",
        "  random_cell_line = []       \n",
        "  bat_tensor = []  \n",
        "  for cnt, G in enumerate(GEB.columns[list(train_set)]):   \n",
        "    random_cell_line.append(G)\n",
        "    #protein_features_df = cell_line_dicimal_norm(G)\n",
        "    values = cell_line_dicimal(G).T.values\n",
        "    tensor = torch.from_numpy(values)\n",
        "    bat_tensor.append(tensor)\n",
        "  bat_tensor = torch.stack(bat_tensor, dim = 0)\n",
        "  target = torch.tensor(pd.Series(log[random_cell_line].values[0])).to('cuda')\n",
        "  return bat_tensor, target  \n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp  \n",
        "def eval(model, eval_list, batch_size):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  loss = 0\n",
        "  with torch.no_grad():\n",
        "    eval_set = np.random.choice(eval_list, batch_size)\n",
        "    bat_tensor, target = make_data(GEB, eval_set) \n",
        "    output, t = model(bat_tensor)  \n",
        "    loss = criterion(output, target)\n",
        "    _, predicted = output.max(1)\n",
        "    correct = predicted.eq(target).sum().item()\n",
        "  return 100. * correct / batch_size, loss.item()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import random\n",
        "random_seed = 1553\n",
        "torch.manual_seed(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "#np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed) # multi-GPU\n",
        "#####################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TebybdOBC1TC",
        "outputId": "071d775c-5386-4358-8b03-f6369ad97941"
      },
      "source": [
        "nums = set()\n",
        "while len(nums) != 100: \n",
        "  nums.add(random.randint(1, 801))\n",
        "eval_list = list(nums)\n",
        "\n",
        "train_list = list(range(1, 801))\n",
        "#train_list = []\n",
        "for i in eval_list:\n",
        "  train_list.remove(i)\n",
        "######################################################\n",
        " # hidden_dim, de_hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device\n",
        "device = 'cuda'\n",
        "\n",
        "#model = Attentionisallyouneed(176, 308, 1, 1, 100, 0.3, 'cuda').to(device)  # 메모리 때문에 308 -> 89\n",
        "model = GraphResNet().to(device)\n",
        "#model = GraphMLP().to(device)  \n",
        "\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=5e-1) # lr = 1e-3 or 5e-4, weight_decay=0.001\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.00001)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay = 0.0001)#, weight_decay=0.1\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0,  verbose = False)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#model.train()  \n",
        "\n",
        "\n",
        "print(\"total parameter 수 : \", get_n_params(model))\n",
        "# 275664581\n",
        "# 423843235 -> gene 137\n",
        "# 544179655 -> gene 176\n",
        "loss_list = []\n",
        "val_loss_list = []\n",
        "import time\n",
        "start = time.time()\n",
        "batch_size = 16\n",
        "iterations = 40\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #model.train() \n",
        "  eval_acc_sum = 0\n",
        "  epoch_loss = 0\n",
        "  epoch_val_loss = 0\n",
        "  train_list = list(range(1, 801))\n",
        "  #train_list = []\n",
        "  for i in eval_list:\n",
        "    train_list.remove(i)\n",
        "  \n",
        "  for iteration in range(iterations): \n",
        "    train_set = np.random.choice(train_list, batch_size, replace = False)\n",
        "    for t in train_set:\n",
        "      train_list.remove(t) \n",
        "\n",
        "    bat_tensor, target = make_data(GEB, train_set)   \n",
        "    model.train()\n",
        "    train_loss = 0 \n",
        "    optimizer.zero_grad()\n",
        "    output, t = model(bat_tensor) # 최종 output\n",
        "   \n",
        "    loss = criterion(output, target)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()  # 역전파\n",
        "    optimizer.step()\n",
        "    #scheduler.step()\n",
        "    torch.cuda.empty_cache()\n",
        "    print()\n",
        "    print(iteration, \"번째 iteration\")\n",
        "    print(\"train loss : \", train_loss)        \n",
        "    if iteration % 5 == 0:\n",
        "\n",
        "      eval_acc, eval_loss = eval(model, eval_list, 64)\n",
        "      print(\"\\n eval loss : \", eval_loss,)\n",
        "      print(\"eval accuarcy : \", eval_acc, \"%\")\n",
        "      print('------------------------------------\\n')\n",
        "      eval_acc_sum += eval_acc\n",
        "      val_loss_list.append(eval_loss)\n",
        "      epoch_val_loss += eval_loss\n",
        "\n",
        "    loss_list.append(train_loss)\n",
        "    epoch_loss += train_loss\n",
        "    #print(model.linear1.weight.grad)\n",
        "    #print(model.GCN_layer[3].linear.weight.grad)\n",
        "  print(\"------------------EPOCH 완료-----------------------\")\n",
        "  print(\"Epoch Loss Mean : \", epoch_loss/10)\n",
        "  print(\"Epoch Val Loss Mean : \", epoch_val_loss/3)\n",
        "  print(\"Epoch eval acc : \", eval_acc_sum/3)\n",
        "  print(\"---------------------------------------------------\")\n",
        "  \n",
        "print(\"time :\", (time.time() - start)/60)\n",
        "\n",
        "# weight decay : 0,1 ~  0.000001 까지 다양하게 해보기\n",
        "# adamW 사용해보기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total parameter 수 :  22178495\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6895556449890137\n",
            "\n",
            " eval loss :  0.6893316507339478\n",
            "eval accuarcy :  54.6875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.680005669593811\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6763085722923279\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.7491164803504944\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6736153364181519\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.7096790075302124\n",
            "\n",
            " eval loss :  0.6687981486320496\n",
            "eval accuarcy :  65.625 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6973009705543518\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6968393325805664\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6662819981575012\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.7161306738853455\n",
            "\n",
            "10 번째 iteration\n",
            "train loss :  0.6585794687271118\n",
            "\n",
            " eval loss :  0.6794791221618652\n",
            "eval accuarcy :  60.9375 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "11 번째 iteration\n",
            "train loss :  0.6771196722984314\n",
            "\n",
            "12 번째 iteration\n",
            "train loss :  0.6766840219497681\n",
            "\n",
            "13 번째 iteration\n",
            "train loss :  0.7067590951919556\n",
            "\n",
            "14 번째 iteration\n",
            "train loss :  0.6340322494506836\n",
            "\n",
            "15 번째 iteration\n",
            "train loss :  0.6520818471908569\n",
            "\n",
            " eval loss :  0.6669818162918091\n",
            "eval accuarcy :  65.625 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "16 번째 iteration\n",
            "train loss :  0.6483138799667358\n",
            "\n",
            "17 번째 iteration\n",
            "train loss :  0.7131615877151489\n",
            "\n",
            "18 번째 iteration\n",
            "train loss :  0.6704227924346924\n",
            "\n",
            "19 번째 iteration\n",
            "train loss :  0.6693155765533447\n",
            "\n",
            "20 번째 iteration\n",
            "train loss :  0.6682450175285339\n",
            "\n",
            " eval loss :  0.7175498604774475\n",
            "eval accuarcy :  45.3125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "21 번째 iteration\n",
            "train loss :  0.6306265592575073\n",
            "\n",
            "22 번째 iteration\n",
            "train loss :  0.6266096234321594\n",
            "\n",
            "23 번째 iteration\n",
            "train loss :  0.729173481464386\n",
            "\n",
            "24 번째 iteration\n",
            "train loss :  0.6867945194244385\n",
            "\n",
            "25 번째 iteration\n",
            "train loss :  0.6872557401657104\n",
            "\n",
            " eval loss :  0.6937951445579529\n",
            "eval accuarcy :  54.6875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "26 번째 iteration\n",
            "train loss :  0.736531674861908\n",
            "\n",
            "27 번째 iteration\n",
            "train loss :  0.6878815293312073\n",
            "\n",
            "28 번째 iteration\n",
            "train loss :  0.5880495309829712\n",
            "\n",
            "29 번째 iteration\n",
            "train loss :  0.6884413957595825\n",
            "\n",
            "30 번째 iteration\n",
            "train loss :  0.7413371801376343\n",
            "\n",
            " eval loss :  0.7152128219604492\n",
            "eval accuarcy :  50.0 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "31 번째 iteration\n",
            "train loss :  0.7152128219604492\n",
            "\n",
            "32 번째 iteration\n",
            "train loss :  0.6625613570213318\n",
            "\n",
            "33 번째 iteration\n",
            "train loss :  0.6365249156951904\n",
            "\n",
            "34 번째 iteration\n",
            "train loss :  0.6363710165023804\n",
            "\n",
            "35 번째 iteration\n",
            "train loss :  0.6889526844024658\n",
            "\n",
            " eval loss :  0.7424300909042358\n",
            "eval accuarcy :  43.75 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "36 번째 iteration\n",
            "train loss :  0.6890735626220703\n",
            "\n",
            "37 번째 iteration\n",
            "train loss :  0.7693060636520386\n",
            "\n",
            "38 번째 iteration\n",
            "train loss :  0.741204023361206\n",
            "\n",
            "39 번째 iteration\n",
            "train loss :  0.6628615856170654\n",
            "------------------EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  2.733431816101074\n",
            "Epoch Val Loss Mean :  1.8578595519065857\n",
            "Epoch eval acc :  146.875\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.7618554830551147\n",
            "\n",
            " eval loss :  0.6754652261734009\n",
            "eval accuarcy :  59.375 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6636790633201599\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6188676357269287\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6644464731216431\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.7083293199539185\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6649682521820068\n",
            "\n",
            " eval loss :  0.7019233703613281\n",
            "eval accuarcy :  51.5625 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.7701137065887451\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6456856727600098\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6466526985168457\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.627863347530365\n",
            "\n",
            "10 번째 iteration\n",
            "train loss :  0.6857370138168335\n",
            "\n",
            " eval loss :  0.6906070709228516\n",
            "eval accuarcy :  54.6875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "11 번째 iteration\n",
            "train loss :  0.6663197875022888\n",
            "\n",
            "12 번째 iteration\n",
            "train loss :  0.6662091612815857\n",
            "\n",
            "13 번째 iteration\n",
            "train loss :  0.6858409643173218\n",
            "\n",
            "14 번째 iteration\n",
            "train loss :  0.7258395552635193\n",
            "\n",
            "15 번째 iteration\n",
            "train loss :  0.7056939601898193\n",
            "\n",
            " eval loss :  0.7200114727020264\n",
            "eval accuarcy :  45.3125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "16 번째 iteration\n",
            "train loss :  0.7053409814834595\n",
            "\n",
            "17 번째 iteration\n",
            "train loss :  0.6090978384017944\n",
            "\n",
            "18 번째 iteration\n",
            "train loss :  0.7434757351875305\n",
            "\n",
            "19 번째 iteration\n",
            "train loss :  0.628786027431488\n",
            "\n",
            "20 번째 iteration\n",
            "train loss :  0.628584086894989\n",
            "\n",
            " eval loss :  0.6954656839370728\n",
            "eval accuarcy :  53.125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "21 번째 iteration\n",
            "train loss :  0.6468870043754578\n",
            "\n",
            "22 번째 iteration\n",
            "train loss :  0.6258448362350464\n",
            "\n",
            "23 번째 iteration\n",
            "train loss :  0.7279289364814758\n",
            "\n",
            "24 번째 iteration\n",
            "train loss :  0.6435657739639282\n",
            "\n",
            "25 번째 iteration\n",
            "train loss :  0.7526784539222717\n",
            "\n",
            " eval loss :  0.6921786069869995\n",
            "eval accuarcy :  54.6875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "26 번째 iteration\n",
            "train loss :  0.6644423604011536\n",
            "\n",
            "27 번째 iteration\n",
            "train loss :  0.709124743938446\n",
            "\n",
            "28 번째 iteration\n",
            "train loss :  0.6643095016479492\n",
            "\n",
            "29 번째 iteration\n",
            "train loss :  0.6417595744132996\n",
            "\n",
            "30 번째 iteration\n",
            "train loss :  0.5957437753677368\n",
            "\n",
            " eval loss :  0.6930946707725525\n",
            "eval accuarcy :  54.6875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "31 번째 iteration\n",
            "train loss :  0.6872183680534363\n",
            "\n",
            "32 번째 iteration\n",
            "train loss :  0.7598047852516174\n",
            "\n",
            "33 번째 iteration\n",
            "train loss :  0.6633936166763306\n",
            "\n",
            "34 번째 iteration\n",
            "train loss :  0.7118785381317139\n",
            "\n",
            "35 번째 iteration\n",
            "train loss :  0.6392194628715515\n",
            "\n",
            " eval loss :  0.6754741072654724\n",
            "eval accuarcy :  59.375 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "36 번째 iteration\n",
            "train loss :  0.7118850946426392\n",
            "\n",
            "37 번째 iteration\n",
            "train loss :  0.6875518560409546\n",
            "\n",
            "38 번째 iteration\n",
            "train loss :  0.7114514708518982\n",
            "\n",
            "39 번째 iteration\n",
            "train loss :  0.7345371246337891\n",
            "------------------EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  2.720261204242706\n",
            "Epoch Val Loss Mean :  1.848073403040568\n",
            "Epoch eval acc :  144.27083333333334\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.6180493235588074\n",
            "\n",
            " eval loss :  0.6925374269485474\n",
            "eval accuarcy :  54.6875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.5959639549255371\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.7099282145500183\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.7099682092666626\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.7325526475906372\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.7090588808059692\n",
            "\n",
            " eval loss :  0.6973391771316528\n",
            "eval accuarcy :  53.125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.729972243309021\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.790738582611084\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.685775637626648\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.6306725144386292\n",
            "\n",
            "10 번째 iteration\n",
            "train loss :  0.7379088401794434\n",
            "\n",
            " eval loss :  0.7017788290977478\n",
            "eval accuarcy :  50.0 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "11 번째 iteration\n",
            "train loss :  0.7346736192703247\n",
            "\n",
            "12 번째 iteration\n",
            "train loss :  0.654965877532959\n",
            "\n",
            "13 번째 iteration\n",
            "train loss :  0.7139174938201904\n",
            "\n",
            "14 번째 iteration\n",
            "train loss :  0.6589995622634888\n",
            "\n",
            "15 번째 iteration\n",
            "train loss :  0.6730518937110901\n",
            "\n",
            " eval loss :  0.6614713668823242\n",
            "eval accuarcy :  68.75 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "16 번째 iteration\n",
            "train loss :  0.6614713668823242\n",
            "\n",
            "17 번째 iteration\n",
            "train loss :  0.6977249383926392\n",
            "\n",
            "18 번째 iteration\n",
            "train loss :  0.697550356388092\n",
            "\n",
            "19 번째 iteration\n",
            "train loss :  0.6973307728767395\n",
            "\n",
            "20 번째 iteration\n",
            "train loss :  0.6859879493713379\n",
            "\n",
            " eval loss :  0.6914759278297424\n",
            "eval accuarcy :  53.125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "21 번째 iteration\n",
            "train loss :  0.7076832056045532\n",
            "\n",
            "22 번째 iteration\n",
            "train loss :  0.6965745091438293\n",
            "\n",
            "23 번째 iteration\n",
            "train loss :  0.666639506816864\n",
            "\n",
            "24 번째 iteration\n",
            "train loss :  0.6670273542404175\n",
            "\n",
            "25 번째 iteration\n",
            "train loss :  0.7060598731040955\n",
            "\n",
            " eval loss :  0.6913110613822937\n",
            "eval accuarcy :  53.125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "26 번째 iteration\n",
            "train loss :  0.6766868829727173\n",
            "\n",
            "27 번째 iteration\n",
            "train loss :  0.6765745878219604\n",
            "\n",
            "28 번째 iteration\n",
            "train loss :  0.686323881149292\n",
            "\n",
            "29 번째 iteration\n",
            "train loss :  0.7171314358711243\n",
            "\n",
            "30 번째 iteration\n",
            "train loss :  0.7169287204742432\n",
            "\n",
            " eval loss :  0.6987233757972717\n",
            "eval accuarcy :  48.4375 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "31 번째 iteration\n",
            "train loss :  0.6765297651290894\n",
            "\n",
            "32 번째 iteration\n",
            "train loss :  0.6476945281028748\n",
            "\n",
            "33 번째 iteration\n",
            "train loss :  0.6963459849357605\n",
            "\n",
            "34 번째 iteration\n",
            "train loss :  0.6453964710235596\n",
            "\n",
            "35 번째 iteration\n",
            "train loss :  0.6643236875534058\n",
            "\n",
            " eval loss :  0.6916699409484863\n",
            "eval accuarcy :  53.125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "36 번째 iteration\n",
            "train loss :  0.6507402658462524\n",
            "\n",
            "37 번째 iteration\n",
            "train loss :  0.6600154042243958\n",
            "\n",
            "38 번째 iteration\n",
            "train loss :  0.6994109749794006\n",
            "\n",
            "39 번째 iteration\n",
            "train loss :  0.6703127026557922\n",
            "------------------EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  2.745466262102127\n",
            "Epoch Val Loss Mean :  1.8421023686726887\n",
            "Epoch eval acc :  144.79166666666666\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.685317873954773\n",
            "\n",
            " eval loss :  0.7150917649269104\n",
            "eval accuarcy :  45.3125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6683793067932129\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6495875716209412\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.628585696220398\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.7469913363456726\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6862330436706543\n",
            "\n",
            " eval loss :  0.6810268759727478\n",
            "eval accuarcy :  57.8125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.642898440361023\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6416127681732178\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.663704514503479\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.5899643898010254\n",
            "\n",
            "10 번째 iteration\n",
            "train loss :  0.7141408324241638\n",
            "\n",
            " eval loss :  0.7023696899414062\n",
            "eval accuarcy :  53.125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "11 번째 iteration\n",
            "train loss :  0.6890489459037781\n",
            "\n",
            "12 번째 iteration\n",
            "train loss :  0.6347780823707581\n",
            "\n",
            "13 번째 iteration\n",
            "train loss :  0.6619661450386047\n",
            "\n",
            "14 번째 iteration\n",
            "train loss :  0.632714033126831\n",
            "\n",
            "15 번째 iteration\n",
            "train loss :  0.7516951560974121\n",
            "\n",
            " eval loss :  0.6920412182807922\n",
            "eval accuarcy :  56.25 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "16 번째 iteration\n",
            "train loss :  0.6920411586761475\n",
            "\n",
            "17 번째 iteration\n",
            "train loss :  0.7841628193855286\n",
            "\n",
            "18 번째 iteration\n",
            "train loss :  0.6919158101081848\n",
            "\n",
            "19 번째 iteration\n",
            "train loss :  0.661698043346405\n",
            "\n",
            "20 번째 iteration\n",
            "train loss :  0.6323358416557312\n",
            "\n",
            " eval loss :  0.6837030053138733\n",
            "eval accuarcy :  57.8125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "21 번째 iteration\n",
            "train loss :  0.6910097599029541\n",
            "\n",
            "22 번째 iteration\n",
            "train loss :  0.7197089195251465\n",
            "\n",
            "23 번째 iteration\n",
            "train loss :  0.8040624260902405\n",
            "\n",
            "24 번째 iteration\n",
            "train loss :  0.7442030906677246\n",
            "\n",
            "25 번째 iteration\n",
            "train loss :  0.6626190543174744\n",
            "\n",
            " eval loss :  0.6755069494247437\n",
            "eval accuarcy :  59.375 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "26 번째 iteration\n",
            "train loss :  0.6630619764328003\n",
            "\n",
            "27 번째 iteration\n",
            "train loss :  0.7592589259147644\n",
            "\n",
            "28 번째 iteration\n",
            "train loss :  0.7095226049423218\n",
            "\n",
            "29 번째 iteration\n",
            "train loss :  0.6221272945404053\n",
            "\n",
            "30 번째 iteration\n",
            "train loss :  0.6449257135391235\n",
            "\n",
            " eval loss :  0.7159041166305542\n",
            "eval accuarcy :  46.875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "31 번째 iteration\n",
            "train loss :  0.765920102596283\n",
            "\n",
            "32 번째 iteration\n",
            "train loss :  0.7047178149223328\n",
            "\n",
            "33 번째 iteration\n",
            "train loss :  0.7035639882087708\n",
            "\n",
            "34 번째 iteration\n",
            "train loss :  0.668290913105011\n",
            "\n",
            "35 번째 iteration\n",
            "train loss :  0.6690329313278198\n",
            "\n",
            " eval loss :  0.681393563747406\n",
            "eval accuarcy :  57.8125 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "36 번째 iteration\n",
            "train loss :  0.6853142380714417\n",
            "\n",
            "37 번째 iteration\n",
            "train loss :  0.6853243708610535\n",
            "\n",
            "38 번째 iteration\n",
            "train loss :  0.7000008225440979\n",
            "\n",
            "39 번째 iteration\n",
            "train loss :  0.6288924813270569\n",
            "------------------EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  2.7381329238414764\n",
            "Epoch Val Loss Mean :  1.8490123947461445\n",
            "Epoch eval acc :  144.79166666666666\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.699452817440033\n",
            "\n",
            " eval loss :  0.7027929425239563\n",
            "eval accuarcy :  48.4375 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.6715166568756104\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.685420572757721\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.6992780566215515\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.685434103012085\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.630724310874939\n",
            "\n",
            " eval loss :  0.6854056119918823\n",
            "eval accuarcy :  56.25 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6433915495872498\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6999471783638\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.6853283643722534\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.7007584571838379\n",
            "\n",
            "10 번째 iteration\n",
            "train loss :  0.669602632522583\n",
            "\n",
            " eval loss :  0.6692644357681274\n",
            "eval accuarcy :  62.5 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "11 번째 iteration\n",
            "train loss :  0.6853180527687073\n",
            "\n",
            "12 번째 iteration\n",
            "train loss :  0.6198843121528625\n",
            "\n",
            "13 번째 iteration\n",
            "train loss :  0.7023993730545044\n",
            "\n",
            "14 번째 iteration\n",
            "train loss :  0.738000214099884\n",
            "\n",
            "15 번째 iteration\n",
            "train loss :  0.6854341626167297\n",
            "\n",
            " eval loss :  0.7209808230400085\n",
            "eval accuarcy :  43.75 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "16 번째 iteration\n",
            "train loss :  0.685447633266449\n",
            "\n",
            "17 번째 iteration\n",
            "train loss :  0.6854585409164429\n",
            "\n",
            "18 번째 iteration\n",
            "train loss :  0.7571117877960205\n",
            "\n",
            "19 번째 iteration\n",
            "train loss :  0.7029668688774109\n",
            "\n",
            "20 번째 iteration\n",
            "train loss :  0.7366940975189209\n",
            "\n",
            " eval loss :  0.7099385857582092\n",
            "eval accuarcy :  46.875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "21 번째 iteration\n",
            "train loss :  0.73454749584198\n",
            "\n",
            "22 번째 iteration\n",
            "train loss :  0.6698309779167175\n",
            "\n",
            "23 번째 iteration\n",
            "train loss :  0.64106684923172\n",
            "\n",
            "24 번째 iteration\n",
            "train loss :  0.685368001461029\n",
            "\n",
            "25 번째 iteration\n",
            "train loss :  0.7135722637176514\n",
            "\n",
            " eval loss :  0.7058814764022827\n",
            "eval accuarcy :  46.875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "26 번째 iteration\n",
            "train loss :  0.6445921063423157\n",
            "\n",
            "27 번째 iteration\n",
            "train loss :  0.6719633936882019\n",
            "\n",
            "28 번째 iteration\n",
            "train loss :  0.6854668855667114\n",
            "\n",
            "29 번째 iteration\n",
            "train loss :  0.6584277153015137\n",
            "\n",
            "30 번째 iteration\n",
            "train loss :  0.6717090010643005\n",
            "\n",
            " eval loss :  0.6678740382194519\n",
            "eval accuarcy :  64.0625 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "31 번째 iteration\n",
            "train loss :  0.6573565602302551\n",
            "\n",
            "32 번째 iteration\n",
            "train loss :  0.699849009513855\n",
            "\n",
            "33 번째 iteration\n",
            "train loss :  0.7297996282577515\n",
            "\n",
            "34 번째 iteration\n",
            "train loss :  0.685338020324707\n",
            "\n",
            "35 번째 iteration\n",
            "train loss :  0.7299192547798157\n",
            "\n",
            " eval loss :  0.6890028715133667\n",
            "eval accuarcy :  54.6875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "36 번째 iteration\n",
            "train loss :  0.6707524657249451\n",
            "\n",
            "37 번째 iteration\n",
            "train loss :  0.6853626370429993\n",
            "\n",
            "38 번째 iteration\n",
            "train loss :  0.6853717565536499\n",
            "\n",
            "39 번째 iteration\n",
            "train loss :  0.5998064279556274\n",
            "------------------EPOCH 완료-----------------------\n",
            "Epoch Loss Mean :  2.738967019319534\n",
            "Epoch Val Loss Mean :  1.850380261739095\n",
            "Epoch eval acc :  141.14583333333334\n",
            "---------------------------------------------------\n",
            "\n",
            "0 번째 iteration\n",
            "train loss :  0.7148410081863403\n",
            "\n",
            " eval loss :  0.6890813112258911\n",
            "eval accuarcy :  54.6875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "1 번째 iteration\n",
            "train loss :  0.655318021774292\n",
            "\n",
            "2 번째 iteration\n",
            "train loss :  0.6390226483345032\n",
            "\n",
            "3 번째 iteration\n",
            "train loss :  0.7336277365684509\n",
            "\n",
            "4 번째 iteration\n",
            "train loss :  0.6853302121162415\n",
            "\n",
            "5 번째 iteration\n",
            "train loss :  0.6853451728820801\n",
            "\n",
            " eval loss :  0.7107810378074646\n",
            "eval accuarcy :  46.875 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "6 번째 iteration\n",
            "train loss :  0.6853624582290649\n",
            "\n",
            "7 번째 iteration\n",
            "train loss :  0.6338959336280823\n",
            "\n",
            "8 번째 iteration\n",
            "train loss :  0.7030748128890991\n",
            "\n",
            "9 번째 iteration\n",
            "train loss :  0.7034415006637573\n",
            "\n",
            "10 번째 iteration\n",
            "train loss :  0.6855010986328125\n",
            "\n",
            " eval loss :  0.7083863019943237\n",
            "eval accuarcy :  48.4375 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "11 번째 iteration\n",
            "train loss :  0.6306542158126831\n",
            "\n",
            "12 번째 iteration\n",
            "train loss :  0.6107661128044128\n",
            "\n",
            "13 번째 iteration\n",
            "train loss :  0.6857596635818481\n",
            "\n",
            "14 번째 iteration\n",
            "train loss :  0.7463588714599609\n",
            "\n",
            "15 번째 iteration\n",
            "train loss :  0.6860066652297974\n",
            "\n",
            " eval loss :  0.7066773176193237\n",
            "eval accuarcy :  50.0 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "16 번째 iteration\n",
            "train loss :  0.6860684752464294\n",
            "\n",
            "17 번째 iteration\n",
            "train loss :  0.7068778276443481\n",
            "\n",
            "18 번째 iteration\n",
            "train loss :  0.6445964574813843\n",
            "\n",
            "19 번째 iteration\n",
            "train loss :  0.6233620643615723\n",
            "\n",
            "20 번째 iteration\n",
            "train loss :  0.6649422645568848\n",
            "\n",
            " eval loss :  0.670107901096344\n",
            "eval accuarcy :  60.9375 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "21 번째 iteration\n",
            "train loss :  0.686499834060669\n",
            "\n",
            "22 번째 iteration\n",
            "train loss :  0.7088965177536011\n",
            "\n",
            "23 번째 iteration\n",
            "train loss :  0.6418594717979431\n",
            "\n",
            "24 번째 iteration\n",
            "train loss :  0.6868853569030762\n",
            "\n",
            "25 번째 iteration\n",
            "train loss :  0.6870061755180359\n",
            "\n",
            " eval loss :  0.7045295238494873\n",
            "eval accuarcy :  51.5625 %\n",
            "------------------------------------\n",
            "\n",
            "\n",
            "26 번째 iteration\n",
            "train loss :  0.6406017541885376\n",
            "\n",
            "27 번째 iteration\n",
            "train loss :  0.6400758028030396\n",
            "\n",
            "28 번째 iteration\n",
            "train loss :  0.7356688976287842\n",
            "\n",
            "29 번째 iteration\n",
            "train loss :  0.6149060130119324\n",
            "\n",
            "30 번째 iteration\n",
            "train loss :  0.6631677150726318\n",
            "\n",
            " eval loss :  0.6880617141723633\n",
            "eval accuarcy :  56.25 %\n",
            "------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b8da82298205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 역전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m#scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "r6krClpnM8Ll",
        "outputId": "e75fa5fa-0a96-494e-f040-5a63ea302d22"
      },
      "source": [
        "import seaborn as sns\n",
        "#sns.lineplot(x=np.arange(len(val_loss_list)), y = val_loss_list)\n",
        "sns.lineplot(x=np.arange(len(loss_list)), y = loss_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4f20336d90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9e9xtV1ke+ox5WWt9l/3tS/Yt5EYIIYCCXCJaqQoKivZU/B2tB0StWkvPaZEWW87RHuVYak+p+jutLWBB8YYVtNYqChLQBAmShCTcwg6EJDsk2Xsn+/7d12VexvljzHeMd4w55lxzfbe99rfn8/vll/2tNdda8/qOZzzv875DSCnRokWLFi12L4JLvQMtWrRo0WJ70Qb6Fi1atNjlaAN9ixYtWuxytIG+RYsWLXY52kDfokWLFrsc0aXeARcHDx6Uz3zmMy/1brRo0aLFZYX777//nJTykO+9qQv0z3zmM3Hfffdd6t1o0aJFi8sKQojHq95rpZsWLVq02OVoA32LFi1a7HK0gb5FixYtdjnaQN+iRYsWuxxtoG/RokWLXY420Ldo0aLFLkcb6Fu0aNFil6MN9C1aXMa45/h5PHx65VLvRospRxvoW7S4jPGv/+cDeOcdj1zq3Wgx5WgU6IUQrxFCPCSEeEQI8bOe968XQtwhhPicEOKLQojvZe/9XPG5h4QQ372VO9+ixZWOYZpjkGSXejdaTDnGtkAQQoQA3gXg1QBOALhXCPEhKeWDbLOfB/BHUspfF0I8H8BHADyz+PfrAHwdgGcA+CshxHOklO2d2aLFFiDNJEZpfql3o8WUowmjfxmAR6SUx6WUIwAfBPBaZxsJYKH4914Ap4p/vxbAB6WUQynlYwAeKb6vRYsWW4A0l0iydjnQFvVoEuivAfAk+/tE8RrHLwL4ESHECSg2/9MTfBZCiDcKIe4TQtx39uzZhrveokWLLM8xylpG36IeW5WMfT2A35FSXgvgewG8XwjR+LullO+VUt4qpbz10CFvl80WLVp4kGYSSRvoW4xBkzbFJwFcx/6+tniN4x8BeA0ASCnvEkL0ABxs+NkWLVpsEEq6aQN9i3o0Yd33ArhZCHGjEKIDlVz9kLPNEwC+EwCEEM8D0ANwttjudUKIrhDiRgA3A/jMVu18ixZXOtI8b5OxLcZiLKOXUqZCiDcBuA1ACOC3pJTHhBBvB3CflPJDAP4lgN8QQrwFKjH741JKCeCYEOKPADwIIAXwz1rHTYsWW4c2GduiCRqtMCWl/AhUkpW/9jb27wcBvLzis/8OwL/bxD62aNHCgzyXkBIto28xFm1lbIsWlymSXAX4VqNvMQ5toG8xlfjauTWsDtNLvRtTjSxXkk0b6FuMQxvoW0wlfvC/3oXfvPP4pd6NqQZp861002Ic2kDfYiqxuD7CUj+51Lsx1TCMvk3GtqhHG+hbTB3yXCLNJfK8DWB1SAuNfpTlUCa3Fi38aAN9i6kDJRnTNtDXImVMvj1XLerQBvoWUweSIvKWpdYiY8G9Tci2qEMb6FtMHSi5mLbaM86sDHB+deh9jwf3JG3PVYtqtIG+xdSBAljWMnq85Q8/j7d96Jj3Pc7oh1lbcN6iGo0qY1u02EkQo89a3RmL6wkEhPe91JJu2nPVohoto28xddCMvg30yHKp3TUuuLSVtF76FjVoA32LqQOx0zbQK9ZedR74ANAmY1vUoQ30LaYOLaM3yIqaAh/46+0qUy3q0Ab6FlOHURvoNdI8r2b0TLpp2yC0qEMb6FtMHUhvbl03QJbJSptp1iZjWzREG+hbTB1ajd6gTqNPWo2+RUO0gb7F1KHV6A3qXDdZ1mr0LZqhDfQtpg7DtO11Q6h33bT2yhbN0Ab6FlMHYvRt98pxrhsT3FtG36IObaBvMXWgQN8y+nrXTdvUrEVTtIH+MoaUErcde3rXadmtRm9Qx+gTqzK2PVctqtEG+ilClkv80l88iFOL/UbbHzu1jH/y/vtx16Pnt3nPdhaj1nWjUafRZ61006Ih2kA/RTi12Mdvfuox3P6VM422HySqY2E/2V2dC5O2qRkAlaOQEkgrgnjaSjctGqIN9FMEeljXhmmj7SkQ7raA2LYpVkjHXN+2MrZFU7SBfopAD/SkgX63rcTUavQKdPxNet2MY/SDJMM7b3+4HRCuULSBfopAybXVYTMphhjvbgv0rUavQPbJakbPNfr6c3X/4xfxqx/7Kj73xMWt28EWlw3aQD9FmJTRj5vaX65oGb0CZ/TSM5hPwujp/TZpe2WiDfRTBOpdsjpqKN3s0kW02xWmFHgg950KOj/dKBhbGUv3SJu0vTLRBvopAiXXiNGvj1K8/r1345Ezq97tSbrZbc9uWzClwAc6X78bkm5mOuHYAE5vj1q//RWJNtBPEehhXh2oQH9qsY+7jp/HAycXvdvrZOwuC4i6BcIum6lMCj7Q+WY3aS4RBQKdMBgrydDnW0Z/ZaIN9FOEVCdjVaCnB31cP/JpsCH+X3/8RfzqbQ9tyXcR66zyj18p4N0pfbObNJeIQoE4DCym/uSFdXziIbsWo5Vurmy0gX6KQIx+rdDo0zHuk2ny0X/hxCKOnVraku8yjH5Lvu6yBZdrMs9gn2YSURCgEwVWAP+9u76Gn/6Dz1nbtoz+ykYb6KcIRqNX9kpiYVWMfZp89EmWY6sWOTIa/ZUdlGyNvnxyszxHGAjEobAC+CjNS9XSdI+Ms2HuFJ68sI6PfumpS70bVwzaQD9FoIfZlW4uB0averJsTWA2bYq35OsuW4zT6JNcIi6kGx7o06IRmjVQFAF+WvrW/7d7nsCbP/j5S70bVwzaQD9FoAd7lOYYpaY9baVGL6co0NesbTopiHW2jL7edZNlsmD0gV6shX+OV8FmU6bRD9MMozTfdUaCaUUb6KcIPPm4NkzHMvZ02qSbLXpoiXXmEt5CoSsF4xl97tXo6XPD1Mg3+ZRp9EQK2gKunUEb6KcInBGv8kBfEexyPRBs737dduxpLK0ntduQXLAV4MFoGmYrlwpcCvNr9Mp10wkDqzd9HaOfFo2eZihtoN8ZNAr0QojXCCEeEkI8IoT4Wc/7/1EI8fniv68KIRbZexl770NbufO7DfxhXhul47sX7gCjXx4k+Cfvvx9/+vmTtdttKaP3sNMrCWeWB/j9ux+3Bv4qH70vGWsYvXlt2hh9MmU5g92OaNwGQogQwLsAvBrACQD3CiE+JKV8kLaRUr6Fbf/TAF7MvqIvpXzR1u3y7gXXYZV0U7hPKlhYPmYg2AoQK+QygA9Jlm9ZULYC1BUo3fzFF5/C2//iQbz7DS/Rr/nugTTLEQdB4aMvB3Wfbj8tgZVkypbR7wyaMPqXAXhESnlcSjkC8EEAr63Z/vUAPrAVO7dZnF4eXFbJHlu6ybQkU+Vm2YmmZnT+xgXxNJNbdq6vdEZPA/76yAyuvmucEaOP7MpYLYtY0o36/7QEVmL0bdvknUGTQH8NgCfZ3yeK10oQQtwA4EYAt7OXe0KI+4QQdwshvr/ic28strnv7NmzDXe9HovrI3zrf7gDH//y6S35vp0AZ/SrA8boK4JdvgNtivVgUqPtSikLjX6r7JWsmdcVGOgpFnMvvLfXTWGv7Dj2ykwz+ulNxupumm2g3xFsdTL2dQD+WErJ5/k3SClvBfDDAP6TEOIm90NSyvdKKW+VUt566NChLdmRpX6CUZbj7MpwS75vJ8AD3NqQafQVgXxc5exWYNziF/y9VqO3ceLiul7ucRLQwD0Yw+jTwl7ZCQNrcXBu09Wfp2TslDQ18+URWmwfmgT6kwCuY39fW7zmw+vgyDZSypPF/48D+ARs/X7bQEHzcuqXwh9my3Uzzke/jYzeBPrq85hm4weDScAD/eXM6P/ef/4U3n/X4xN/js45l278vW6UvTKORAWj92j0U/I80H5My/5Mit/99Ndw16PnL/VuNEaTQH8vgJuFEDcKITpQwbzknhFCPBfAfgB3sdf2CyG6xb8PAng5gAfdz24H0jGyxzSiykdfdQwk7WxnMKRBpO48JmNWQpoUozRHLw7G/u40Q0qJpX6C82ujiT9Lx7yemHUJqhi9bmrGZ0Ee/XvapBvfPl5OeOcdj+B/fPbEpd6NxhjrupFSpkKINwG4DUAI4LeklMeEEG8HcJ+UkoL+6wB8UNoVLs8D8B4hRA41qLyDu3W2E7rke0p8w01AJe1CCKw2sFeaZO327VM+ZlYBbAejl5iJQwySrbNs7jQ2w6DpnA/GMnqJmaBOo5/eytjL3Uc/SvOpOZdNMDbQA4CU8iMAPuK89jbn71/0fO7TAF6wif3bMMxydJfPxchy1Y1wphPalbGVTc0217f9gRNL+JH33YPb/+W346r5rnebdMysAjAzka103eyfjXERyeUb6DcRWOmzPBnru4+zXCIOA0ShsAumdHFUORk7LQVTo8uc0SfZ5RXod21lbJpPF6OXUo4NWkmWIwoE5rohVgfpWMeLYfQbO8avnV/DUj/B6eXqhHWTxmlJQwtmE+RFhW2vE6rfvUx99FvB6C2N3nMPJJnqXhkFAbLcrCurE52Jh9FPSWBNL3PXTZLlU5PYboJdG+inrdXtnQ+fwzf8m49hqV/dSoBK2uc6EVaH2VgPO7G8jQbDJvbMXAeP6vOYbOEar6T396Jwy77zUsC0IZh8/+mzg2S8jz4KBKJAAODup7IsQv+cFhZ6Ofe6kVIiyeTUnMsm2MWBnlw30xEoHr+wjtVhijPLg8ptkkwiCgO12DOrNK2SnyjAb1QyacLWx61ypbbZukGVGN5M5/IO9PkmAqtPuqleYSpAFBaJa+eeH3kqjKclOCWeoq7LBbp9w5ScyybYtYGepobTIt3QDb0yTCu3SQvpJgoDpHk+tmBqs/3o6WN1M4ImbRaSLfTz03eR6+ZyDfTGt75x6aY/zkefq/slDoX+m2/rbV08Jc/D5czoL0dr6K4N9JrRT4l0owP9oDrQk3QTBSq5Nq5p2WbXjKWAUjcjaJaM3TqNnh6emdhm9N/3zk/h3Z94ZNPfv1PYjEZvGP2Y7pWZkm5Ckm6cAXdafPT/+/vvx3/46Fes1y5njV5X9U7JoNkEuzbQU4C/FIz+r798Gn947xPWa3RDr9YE+qRw3cRhgDQzizJUySZpg0BdhyYe+UaMvjjXUm7eeUPnqVsE+rRIMn7xxBJ++aMPXTaBwSzdt4FATy0QRtxHX/6ehIhBaNcc0HW1WiBcQunmy08v46tPr1ivcdfNz//pA3jfpx7b8f3aKOiaTktiuwl2b6C/hJWxH/jMk3jvJ49br5HVbXVYnYwl6SYMBLJcYpyPXgfhDcbWrMFA0cQmmHqsfRuFy+hzKa1Fwj/ywOWxzuimGH0R1Mdp9NTUzCRjC6nPo9FrF9olCE7ro6w04PHGa79/9xP4t3/xIB44sTWLy283ki2Unb781DL+13f/LdZH1QRwK7BrA71x3ew8ox+m5Ru7iXRDybW48EWPq4zdLKMnO15dcG7Sx4YPpnXbfezY03jLH9avE0oPEQX6NLNtqf/tnslbClwKmEC/EdeN+v84jV7ZcQMT6Gukm0vpox+MslJPm9ST0Py///SBHd2vjYKe5a2YHX3p5BI++8RircV5K7BrA326CUa1WYzS3PIw02uAWfjbB52MDSgZW6/R0+sbTVg2SeY2aVOcsPfqtrvz4XP48BhGrhk9c93w/Tu3OnlLgUuBTfnoqakZ1+g9AVrbK0PbXultanaJngcpJdaTrCS50X7wAeDRM6s7um+3HXsa3/rLt08sByZbKN1s1lDRFLs30Gf2NHYnMUzzMqPPmjJ69eByJlup0ROD26BcQl/bxF7ZmNHXnO/lQVI5+3jtOz+F3/jkcX2eerEpmOLHd7k4HTbjujEOmfqZkrZXBuRQsqvBp6EFAs1M3fNA99XyQEmZYSB2PJ92/OwanrzQx1oN+fKBjmUrZkc7te7zrg30yRY6QSaFj9EPGyRj08wUwPA1WCs1+k366JskWg2jrymYYjd83XbL/cQ7KEkpcezUMr7y9IpmScZemVuDx7TURYzDZpqI+c6R10evZ4Ci+C2X0V/6fvSUZ+CDVs5maYvFWsQLvQijLN/RxeB9A2ITbKW9smX0m8RWFvFMijqNvla6KdrORoXrZlyL4HH96sdBF1zVfF47c2oCbNPFvJcHqdeZ008ypLnE8iApafRZbh/f5dLN0jDoyfeXn59uZLN1vk0uFRMm142bXPetMJVs4UpgTUB5Br4vCTsWCvR7Z2IAG7++D5xYwt8+cg4A8OufeBSPnVsb+xmfxNUEW1kwtROrxAG7ONDvZGXsb3zyuJVkHBVBmksajQqmcmo7K5Aw1lN1CJsvmKLPV2/TbOGRer83YaWYprsDE7WFWO4nWCvcB/PdqPh9M+CFgZiauohx2Jzrphzo3fNKf8chZ/S2AcGXjAXsQLvd0IyeO4DYDb3YdwL9Bp/XX/vrh/H2P38Qq8MU/+GjX2nkzjJr6062OMzWMnr1Hdst3TTqXnk5wpfV3y584cQiPvfEov6bZJthmmu2RQx/dVBnr5QmGdukBcIm9T1jzyx//vfvfhyrwxSH93St3/LB6pxYx+j7qd6mIOzW68uDFEsFwzs4T79rvrMbBZeRdKP+v5lkLAB0ogBClM+rGfwCnYx1B35fMlbtk0R3h558sg36+uUDwNK6Sq4vFIF+lOWYAbs5GmJ1mKCfZLo/UBOWvtFVrrSPvpgdBcVAuxFstjFhU+xaRr+TC49kubQaUNGNwG+2ZtKNSq6FhUafOb5o3+/y/0++3/T/8o1+27Gn8eEvPjVRZey4faHEmzsw0evL/QSLffXgH5jrFL+b64GoGwWXD6OXGwsigH0OQ5az4aDzEIemMjbJpF6/1/1tK6G9g156X+DlMwqazVGg3ygxWxtmGKbGxtnke3w21CZIKo5lI9gpRr9rA/1OLiWYOoGeM3pCIx99ZnqXpNn4ZKzRZMfv4x/c8wT+zz/+gv35Gukml6o7Xz5mVgE0k26SLNdtd93jIRa/Mkiw1E8QBkI/+LmUOhnbjcLLhtFvRrrh5zAKAl1AZ22TGTkr1pWxuVVc5lthaqP7tFH0R2XSw6/hWnFPbFa6WRulGKY5hsVz2OQYzYA4qXQjvf/eCJo0DdwK7OJAb6ZX2408lxiwG5luHIvRa+lmPKOnpmbjXDGTrBl79/Hz+NiDp63XZI1rh+ydzZKx4xk9P253zCBGvzJMcWEtwb6Z2CoC0ow+DnRLhGnHZgqmOLsLA4E4KEtWFCDcNsV80OUBjN8jO9lIjEs3UlYPfns3yejXhxmGSc4Y/fjzvlFGzxd02ezsaLP9qppi1wZ67aPfgal+WiRO0yxXPWqKa8YfNC3djNJK10Oa54gD09QsGXMTTCLd9JMMS317xaa6m4wKlRolY7n+WnG+l1luwt1muZi+SwmcWuxj70ys5YhcGgnLOFAun0DvFnxN8llABfIwFKUZFZ1D7qN3q4jrNPqdQt8jafrupYVebG0zKdaGKQap0eibBG/ehmESJCk/l5uLL6a6fVNfMxa7NtDrVY92gtFTJWOaWzeYT7qRElhP/FPFNJN6xSDAsIVxGn0TfW+QZJDSOF+A+qZmmZRImOOlSVFV3XaUcOW/S1hi7z15YR17Z02gV4Ooeq8bmUZn0w5+TSYNBvyhr9TomXRjkrH2GruW6+YSFZ3xNg51rQM2w+illFgbKesu5cC2U6Png9FmZ0cto98kdD/6nWD0xUM3cEq9q4J+lXxDC4/Qg0ufGafRN2GMlDcg3zJQ36Y4y5U2Ps7Lr/Z7vEbPB5gq6QYATlzsYx9n9EyOIEZ/OVTH2gx6sv3NHOnGp9HT37a90lwvIaoZ/U52AOWM3k2UcrPKZjT6YWpm0UQoGmn0xW8NK4hXFfh3b1qjz6qfwa3ELg70O8fo+dJvQ49WD6iRf0/haavqYJnlOeLQLCRB09CtWHhkUOzLxXXTK4Y+5i2vL6SjJr/Bb/yqG3bZM5PQ77HlFUdZrqQbYRg9DQzdy2gxErttwyakm5B6H7kavTopYRBYBVO03Vwncnz05rM7yuiTMqOnZ3KueB4CAcx21WxtIwyZO9nIxTMJo5/0N+1AvzWum9ZeuUGQdLMTeiS30lUx+lGa48C8sgwuVzB6km7CQrqhBO+4hUeaSDc0hV5kQbXu87m0k7F155EPplWDEpdu3MHAXUd332wHIfOGG3tlOHZfpgWbcbnYyVi/64bWoo0Du2CKtpvphFYCNJMSxdi5sxq9R7qhQWquowL9bCdCN7TlykmwPjS/QffSRD76ZNJAv3Wzo0kMFZvBrg30JN3U2QK37LcsRl++senf5A2vkm7SXCIu2hQDZkpZZRE19sjmjH6JSzc1n0+L5DJZGzdbMGUxemeb5UGCIwtd/Tdn9BmrJ6D+N5eDl34zUombjPVp9ORmme1GVsEUbTfbsRlylku94Pol0+gdJxzt40wnRKxluckD3hrr5U73WZOGYxvW6D1uuo3CbVuxXdi1gX4nWyDkOtDXJGOzHFdRoK8omkrz3ErGjsZo9Frfa3CI1PKWSzd1jhoKGiZhuzkfPZ/FuNss9VNct39W/81dN5n0JGO36Zq+5Q8/j2/5939dqjfYCDal0bPPGo3e/g66h+a7oSmYyk3NwWzBlodsVkgD5Y7aK2ukG5JrZuLQzEo2MIiv+aSbBsHbV0HcBJZ0s0lGv9kOtE2xawO9Xkqw4sY5vzrErb/0V/jSyc2vasMLL3xOG7LY1TF6KSWSTKqpeEONfpJ+9AOSbixGX/y/JtBPsji42q7CXtkvzyT4e8/YN6P/3sdcN1lWTsZul+vmM49dwKmlAf7ovhObZlib8a2X7JWBKA1ua4VcMdeNEFOb4sxUEWtGz+5Bav28o5Wxo+pk7KyWbkJd9LWRfVsblaWb7S2Y2rpkbNu9cpMYl4w9tTjAudUhHj27+cUOdLvTxNXo7cKpA3NKnvA1NqPrrHzRRaAfx+gnsVemFOhZMraBj94s4FJdqGT56CvO9zjp5sBcRwenfbMVyVgK9NvESPlybhtpXcBhMfrUf07Orgy9x+K6bqKwrNETi53rRDqfwdtm0Lkc+gL9pfLRO4GeGtfNdEJ0NiPdsOeJCIVvcH3gxBJ+6nfv07+/0TbFVjuHrfLRt4x+Y+Cd/HwBiqrbmtxYS/0Ej5+vbntqu27KrRDoxtjTo+l0mUHQ/vK2s6TRV03r6uyR7v7RcVrJ2LrK2MKTzQNM1c9M7KN3tl8ZpNg7E+uimb0zHQSBQCDUA6AZfWx05+/9tTu3fP3YtVGm5Y3BhJY7F/kYRj9IMrziV+7An3z2ZPmzJUZfdt3wLp/E6PkaBtTmmTP6S2FPXfdo9G4eYbYTwl33FlCW3Lf92Zcsnd8HK9AXs2WfHHPf4xfwV18+jYtrI2s/JpVuuP6/ZT76bb4kuzbQ8wfDN9V3g3Ad3nXHI3jDb95T+b4O9GnmTdQMi0GFAr3vN3nb2TiwffS+Hu78M+P0PR60fD56v0YPax/Udv5zZfWjl1Iz4yyX+iG1fPRsf0nGWpiJsTCjzg95qqm5W65dN+p27Y8yPPjUMr7y1HL1QU+IJFOzsauKWddgwum8C/7g+gLrMMmxNspw4uJ6+bOO6ybyuG40o+9Gprgsy0vWRSIVmTSMfic1+n6SYc6Rkeh8kOtmJo60dMOfjXu/dgG/d9fj+OKJRdSBDybLNdKNa6c0ydjNSDebZfRFDqWVbjaGcXLCcIKe0ovrI80CvL/FbFpWMtYZTHqRYi6+QE9JNLLT8e91/60/05AN9K1Az330ddJNeVpblxTuFA/q00sDvOjffBx3Hz+PP7jncbziV++AlBLLg1S7ifixkKSz0Iuwp2D0+2ZNoM9zqa8fBXrSZIdbGLAoWFAeZRyLHIdx9ko65z6rLR9PtUZfSsZm6IQBOpFxafGCqRknuOa5ScbutOtm32zH2he6njOM0fukm9VhfY7KbFdOxvoGM7rf3ZW4NrLCFFlVN22vbEjWNotdG+gTZ3rlTsUnYfRJZjctc2FcNy6jtzX6TqQeTN9vUtJYFUyVL4svyDb10VuM3pJu7P33fTdnO1UPXJrnOgg/tTTAKMtxarGPk4sDnF4eop9kWO4n+oGn3zu/OsSffV5JF0q6cRi9cBm9Cgy+VYs2C5qFXFXUOgwm9Fa7GLfOLZ1fnwPLct0Ula8+Rj9XuFaEMNWzpmDK0eg5o9/hyti9utc8Be6C0XdNoNfJWHauaLY3bmDiuRXN6D15EZNvshn9Rlw3s/HWWFXbZOwmwS/Af7/vSXzLO27394xvcKGoEKXqomofvdvrJrF/Qwf6mgc/YgtJWO9XJEz5/6tAQevAXMeameQ1MwKfx/jEhT7e8zePlnIeo0xq/ZyYcZqZFbaW+ymWBwn2F0ydvvu//s2j+NWPfRWBAK6/ahYLMzHm2ENPwUvbKwtG2k+al7k3BblYiNFvXrphRMMTdGjwWvEsRMOvtWH0vkBvVg8JA4Ekz/X3znRsmTDPccl89DrQO4tqk+tmphPqe57vG8lT4+y0a8MyGfEdo7vE4oYXHkklZotzP86vn2Y5bv/K6WojQxvoNwf+YDx6dhUX1kZ4ammgX6trsOSCtqm6IXRTsyTTCzILYbbXjD4M0AkrGH3xG7TClAu+QPYvfugY/ts9j9cmUzlogDuy0MPyIC0NEL4ZgS9R9RdfPIV//5dfwdnVob1tlmtZgNhVkpsVspb6CVaHqWb0tN/rowz7ZmN89hdejeceXcC33HQVXv38I/p7DUstkrFRoD/n7lsVhmmGr55eGbudZvQU6DeZjB3no6f3fesT8OtJTc3cQLA6TLVrBVAVsllmZC5Xpsks6WZnXTduoCcCQPs/2wm19GdLN80G9DXnXAB+AkcvGY2enuvJNXqaMY2zg37iobP4yd+5D1897Xf3tYx+k+A3B+mgTy329Wu+nvEAcP/jF/DEeTtBRjdf1cPPmQEF9/luZBhMA+mGHtAoFF5GzzXajz94Gn/7yLnG+h7t99V7ewDM9LauGIqCPx/ciGG5JeNpZmQBzujpGjy11IeU0Iyefi6XqhKYBoD/7Ruvx3963dN8I6MAACAASURBVIv194ZBgExuTrr5k8+exPf+2p21ORZ1bMTou95jnBSbCfRlRl/uR782KjN6apcNmHPF75FLIt0Ugzlg7iVdMKU1+sgr3dD9lowJguujTM/ECN48GGn0zn5sRLqhc0/7++EvPoWHPYTi/JoiRbx6l6O1V24SaSZ1kooeppMs0Fcx+jd/4PN49ycesV6jbaoCfcYGAvrehV5cGkwo0PuSiHTBicGVfoPd7MM0s5wGTaWbo0Wgv7MYJHQfFM99bhLM5ncoOebObJLcMHoKwglzgNB53+8w+iyX2i/vQxjAy1I1o28wG3tqaYA0l/gas8cOkgz/5P33WTUUW83ox7UFpve9Gj0L6lWum9VhpgMlAMTFYjVVxWV5LvW9tVPSTZ5LR6Mvnjmt0ZPrRlX3CuEEepodjgnEarYYg99KddINEbeNtkBQjN4O9D/7P76I99/9eGnbcZW6LaPfJNI815ok6aCnFo1048oqhOV+YrlUgPHSTaalG8XoA6HYCm1Pgb0TVUs3Zg3QwErGUsznLG+Q5FYjp6bJ2BdcsxdxKPDmD3wOv3Hn8co+G3kuQV/Jj5mCoRsE08z0UaGHM82lfqBPXlSBfq/W6GnqDO0w8iGqYvR6HdLxDwfNXp64YGZpJy72cdux07jn+AX92tpoOzX6Okbv1+hpUKOFR9xZ17ojV1D1rGH0ZjES+k5adrBu3eKtBN07CyXppuy6AdS973fdjE/GznUifcyAqvlwg2cpGUuz1glnb6M0N66mYua6Mky915kCfbWRoXXdbApJJtHrUKBXN/YpD6PnyRRawMCdJjeWbpIMoyxHNwotiYZr9N1x0k1gSze+/i7DNNMBtRMG4xl9EbReesN+3P8Lr0YnDHBxfaRdN+5Nxv+2+uiTdOMy+izXskBfSzc1jL74eJbn8KQjNILAbtTFffRAM0ZPD9qJi+ba03Xkbo31oe26obVONwq7BUJ1MtZnr8xyqQueqjR6NxmrGD07VzEt0mI0+kAIvOi6ffj9ux/3ss+tBg3I893IshWTRk/HSEGzEwZ+6aZBMnauG+p7kOCyeleOrGtTPEgy3PnwWXzq4XOl90aZ1KQtyfJaS2fde2ofKFk+BYFeCPEaIcRDQohHhBA/63n/PwohPl/891UhxCJ77x8KIR4u/vuHW7nzdUizXN9ImtEv1Wv0g0QtYODeIEa6qUjGco0+ydCJVEB3Zw3dOo2eXDehLd24PdipypXkizgUyCVq11GlwDgTh1joxYhDlbir6mNjs1EzuNFvusmrlCX69HSb9aghRn9AB3piMfAmnglRoAaxXAev4jeGVP04nnXTg/YkY/QU6DmzJUavC6Y2K92M1ejV/0dpXjqfOdPTq1w3bjJWMXqzznDPWXYxzyWiQOB9P34rXnL9frzz9oc3dXxNQAPpTOwQn6Lugpg8FRLGoS0rNbVXUr6CM3qgTEgyh9HXLTzyk79zL370fZ/Bj//2Z0ptKpIsR6foMjtKc12E6BuQaPW0KueQnnFts5oWjdtACBECeBeAVwM4AeBeIcSHpJQP0jZSyrew7X8awIuLfx8A8P8AuBWABHB/8dmLW3oUHiSMFTXV6LnsYH0X3RCewCKlYVEDzegDdKOwMhnrGzDoZoqCwAp+5EZwGzBRsOtEAdZGGXIJeHK4ar9ooCkCZeBUnFZNcdXveZKxzgOUZrmxV7LpduIwekrK0e/muUSNcoNAuIzelm6atq8AbOmmrxk9G8SKY6M1A7a0MrZGugFUQOvOh9Z7FOh9Pno188y0Dx1QBMHP6G3pZrYT4QXX7sVDT493Im0WdJ93Y9tWnGY5olDg65+xF7/8Ay/E3332oeIYXOmmub1ythPp+4PgDhBuoK/T6Mmhl+YSi/0EB+dNG+0kKxYIimxG77vO45qs1RUtbiWaMPqXAXhESnlcSjkC8EEAr63Z/vUAPlD8+7sBfFxKeaEI7h8H8JrN7HBTpFmupRt6oE8t9jXz9QV6ClJVjN6n5fEYOUjVSvQ66ZraEkO9Ru933XQcZkb7YBj9+FWXiLFwlsj72JSkm6pAPyLXjZvDMBr9+sgEYTpvTy+rh2afy+iLBGEViNG7uvMkrhvN6C9yRq8+5zL6ThRo29xWFUy5CUYCz6tw542UErk0x+rrdTNMVV0Hl26iCo3eMHo1wAOKYbt5KMKTF9Ynms08vTRAnkv8zVfP4u//l0/Z7TCK344dW3FazC6CQOCHvvE6fY+XpBtm1a2D0uhDPas0C6yMYfQeCzFhlJoiwAuOYytJc51LU4F+5P09oL4lA9+HaZBurgHwJPv7RPFaCUKIGwDcCOD2ST4rhHijEOI+IcR9Z8+ebbLftchz9bDMxPbhDZIcF4tpli8ZW+XbrXPdWEExyTHUjL4s3VDJer1GbzN694Gl79QaffF+XUKW9puCMQUOzqyrj4mzXr/rJi2WQAwDwdi20ehp1/Y5BVOZVLpxFdyZR9dJ+E4S6E8tDvSsSTN6FugpWAgh1LVrEOz+y18/jLsePe99LyvWFojDwKvR83PMAz29TLq1z3VjetHzQO9o9E6nz0wah9NsJ0Say9L5S7Mc3/Nrd+L9dz2OExfX8ZY//Ly+d86uDPGuOx6x7pWVQYJv/5U78GdfOIl7H7uAB04uWesd8EZ9HfY8KEZcDj2udKM1+pqke54rGXO2axj9PDlinM/R4DvSckm1ySLNcxxZUC6186t2oB9lErEmbVJLNz4d3gR6/zFcri0QXgfgj6WUE817pZTvlVLeKqW89dChQ5veCWIAMyw5Q3ogJWSNXlh2lbhTRa3ljZmCG0ZfkYyNAnSi0HtD0D67jN71Q9Msge6LTgNG30+yIuio742KHjJV1q4qRr9a4aNXi5oLi51zHz2gSvLdfc1y6a0ZIESBKLpX2hp9PylfO0Bdv6+dW7N+d6mvKnKzXOrpuNHozW1K039AzXyasNpf/5tH8aEvnPK+l+WqhYPLUvX70g6Y5nOksTsavScAksUPUEFSdRylfJAj3eRSM3qdOHeO8dzqCKvDFBfXR7jn+AX8z8+d1LbUD3/xFH7ltocsS+rKIMUwzXH87BrOrii/eN9j+41DYT0Paea/7pFzrrR0U8Po6f6ciQ2jn6fmgZmT+3AYvGb0LLdBSDOpVz0rMfpCo+8U0o3R6GtcN1WMfooWBz8J4Dr297XFaz68Dka2mfSzWwbXvgUANx2aBwC855PH8di5NS+jp4ScW6AxqmP0ju2Ra/RDZzCpk24yzeiFbjtLnwHMze5KClraqWX0OXpRAFEwOtMVsvjtumQsu0G1XTR1pZscURBYnviUVcYCwJ6evWoU/U6dj14z+pJ042f0b/3jL+IVv/oJ3PLzf4lv/5U78LePnMMozfH11+wFYBKydB3XXEbPVjwaJ91IKTFIMsu5w5FrO6Pft24x42FqfQ4wNQM+1w0FQLdgyspnsCQ+/ZZh9JF1HghnVtRAmGTGj08J0dNFID+9bKqi6fyfXh7oamm7HQExenXfa0afVzF6o9GnWa6vQZ27io6BnjmAd4n139euRu/7jVGW43DB6C+slwO96kmlru1iBWuXUo7V6KfJR38vgJuFEDcKITpQwfxD7kZCiOcC2A/gLvbybQC+SwixXwixH8B3Fa9tK0yBjQn033DdXnzvC47io196Cr/80a+UVrsBeG+NCaSbjAf6zLhu4oAFRlu6qZoqAuVeNyWN3gmy9MDUMYJBklnnQi1kkVcmgjiDkrLsdXeDIBWncbdQwnrdAMDCjGmnq/voS8MyfYiKJfTcZGxVwdSZ5QGedXAOb3rls/HkhXX8ecG2n/+MBQDGYumzV66NOKMPNNv98y+cwht/777SviWZGijXKjzplH+Iqxh9hXRDx2q5bkLbdUPHb0k3xe+UfPS5WQ6SYutMxy48I5wpgrjKr6jP0KByupgNUb5FbaeO68zKUDN6fk7pOYwDJYfxNWN9gb7DBkW+alRdMpaepV4c6mOmLqgljd6tjM1N19WywUDiUJGAveBKN45GX6XDr48ya+EeH6bGRy+lTAG8CSpAfxnAH0kpjwkh3i6E+D626esAfFAyn5+U8gKAfws1WNwL4O3Fa9sKkkF4cNs7E+Pdb3gpXnjtPiwPklKiFKhuopQ4AZuDB0VqgdCNiMGYpGEnDLT+67MF8n70PLBWafQEdyDwoe8EeuoKWcUm3Jly2bbm2itzRGGgVzoC1GDJb+6FDTB6Yql5LiEEtPRUlYztJxluPDiHn/muW3B0oYcvnVLLRF5/QK1HSy2RySPPk7HrrBskl27uf/wiPvFQOW/U1/JPdaAPhBqIfYVd46SbOh89/eYsd92UkrFUf2GCv0nGRsV5cBm9CtZpnutBmhg6BfjTy+WiwzPLJtBbAZpmEgFJN6bGwlf9zQdFPoDWrSim809xoO/xed1wrD4Zm+VSn0P3XkqyHLOdEAu9CBfWzCwmL2ZNFOiHaa5bf7vfscQ6xVYz+lx/73ZirL0SAKSUHwHwEee1tzl//2LFZ38LwG9tcP82BDqptkavDnWuG2G5n5jVdtIyU3Kz/CTl+Kbz9MB2wkC3QOg6jH6U5sZZUMXoM/NQcLZT0uidfaDgV8cIhkmup/L0G9zN4iZyXU20GwUW++P7b611yxl9Lq3vWZiJdeKVL3hS57pRA5Ji9KEwA2BVU7P+KNNOq2sPzOJzTygX7zP2zlifI+skP6bVYYr9c2pA6MahtqQOU2WZlVJq6QswSWqXFRNIuqnqVspPsZWMJUbf4Rp9oFdKE0LoIOgy+vVRZgJ9zBi9I92QpEldQAkUxFPG6Om3KNA/zRsDZka6oaDGE9zGSaZmsn3myIo8jD4KhX4e+QBa1yGSriVn9KTRu3ZH/QwV1zPLJWbjEItILPJClukoDHBgroMLbLEeig28YGqxgtHz5TPbythtgNboWaCn0X6uE2J9lLJpJNfoy4xeBbJCuvEwcbp5ZrthsfBIhm4Uah+9lBKjLLMsZBQ4OOg34jDQfT+Acs8SVz7qFANBnQNtkGTWuSCNnnahTqNXv1FdiJKxh9lOxubWedzTYyshMUtZrb2yWCs1KyQeIZQuyp099nGaIrnr9s/qYHVgrlPUG6jrSwHHYvQjsxJSLwr0eXblN0ITRq81+poGW+530OvaIcXWEKbLsubR6KPADIoAEAeBrkMw0o1x3ajz4EhfK0y6KW4oyh+QrMMZPR3X+bWR/l1bciE5Uuj7HjAuLRdxGOjfXW3I6In4ELkCoNc1cAdYvfBIavJTs3olLrOtOYdCBXrG6OmeMslYWVkwtcQGiCqHWF2r8K3Ergz0mtGzZCwFgNlOhLVhpm8QX0EQv7EyFhB9Pnq9dFsnKhY4yXVlLKBuNpJuABU0pSyP8Jp1FQ8jJWTHSjcNGP0gLWv0OQsApUDvfJdbiMIHG7ui19xOKQsWgCPdMBZTF+gDUQT6TOpgZw0mLNEIqOBL1/na/TP69b0zMea7ESv4MslYGnDXR6l+6HtxqBm7m1A356AsMXDkhXW0SqN3bYru6/OFpNCNzABK30NuoflOtY8+LNpd83NEMyo6R24i+ewKFQmZQXptmGK1+A+wA71vpmJp9OzeKLluPNe9UyXdjMk/AcToKRlLGn11MpZmmzTo+Rb8jqMAB+a6lr2SBjeejK1KuHLppso5ZFxR2xvpd2WgdxdIBkwCaq5bw+ipYIrdWPwm8zF6Ygk0jV4eJNpHD6hA4Uo3gEcTZA8FYAJax2H0rj6uffR1Gj1b9Fp9tx0ASpWxzgNSYvSJ56EIbEaf5KrrJM1MrGSsVRk7JhkrpeUBj52WCa49lgb36wpdHlCBfrYT6joAYvS5ZHUJQ8PoueumaiUy49yp6H+U1Us3/Jzzfjc0yF69bwa/9roX4Xu+/qgepHWxnE+jp8pY5t4iW6ZLIox049foVQ2ECbgk18zEoTcZy2G5blhtCK8Ur/LRc+lmzZJuajR6nYw1jF5r9CXpxuy3mwsZWoHenMMDc7G3NiCOgiL/0izQj/XRt4y+Gc6uDPETv/0Z3PGVMxWMnhY5iLA2yrzJWOOjN6/x932uG72ifde0W6DKWEDdbKMst6Qbep3/7onC+keBjAK+y+jdPEGTylhlr7QTd3xK796D7nfVJWN5H30r0Kc5kizHvqJz4Z5erIM1nVKqkKxCULDUPJc60ev6r+n65Ll0pBvD6BeI0evum+Ycrg7VQiz9xHbdDJz7w5f4BZTc5+szlI1h9HTuo0BYGj3X01/7omuwpxfrAUhXJqd5KZdjqojVeqZB0RyPu254ZSxQTsYSW08yqXXx1WGqX//6axZwdmVomoF55Ii+xegL6cZh9NUFU1y6Ufs22wkbJWNJLgWMvbJcGWuupV5ykaSbpCw5xaFi9BfWRqainr0XF8c0LhlbVR3N26e0/egbYk8vwie+ehZfOLHo1egp6M91FLPQ7Q4s6YZcAeak8/d9SVQz1TbTaC+jDx1Gzy78m/7gc3jPJ4+jEwZ6P2Nn+yp7pS5CGifddFyN3rCaUmVsSbqp1ujd3AKBet3sL9r+LvRi3amS99gZZ6+kgqlQ1wA4jN7R0F1Gv6cbFT1eQn19OZNdH2b6b+66oSDoSjgECjBSlpkxUDQRC6lgqnxt6JzvnYmxakk36v/8vFAwMt0czf1EoD7zmTSDJw3o9J28MtY9D1kuca6QKFKH0VOgf+G1+5BL4FzhmXeTpGEgvLbIyJnZpLm/YMon3eyf7dT2NBoyRt9zGX3JXgn9OtmiTbvhMqOPwwBXzanfX9Hnnmn0RRfYXBZLOTr7uTxIIQSwbyauaIPB9m2bXTe7JtD34hDX7JvB8bNrmklwXVpr9MVNsOhpH6pL6z0XHfB3udOMngVSl9EnbBEUn3RzdmWIF123Dx//mW+z/NMAr3D0JwXjBtLN0GH0YaF9Uzyvyhfw4+HgMxuSnKp89LSQx8JMpAONrgaUDQummJbvJvHoPFLQout8ZKGHOBS6B/6cxehtt42WQjyVsVWM3v0OF5lU57mqYIoG05mOXSltiozMtjQAURAdpuVkJiWuuZOJJDrXR++uBgao6k+67ryVwuow1XLNC69VhWck5fBz0o2UQ4Vr9DxRbxVMZdLbtTQOg5LrZm9FkCT4Gb3fR28WHmGMvmPLYvxzUSg0UaEVyjixiUOhW6qoAcFx3fQTzHcjdCK1Qtji+ggf/dLT+Nixpws2z3KB28zoG9krLxfceHAOx8+t6uDsS8bShTWJGWNbM50XGaO3pJtqrfXQHlVc0YsDvOi6/foGzArmTHYyzvQJozTHMw/O4oar5vRrFDSrmpoRfIz+wtoIs53QKnXnGn0UCgxSWc3oS9KNnYy1XDfaFuph9LnEi67bh2+7+RBeccthv49+HKMvdGfariTdFPvCW+Kq/RF4xr4Zze7mOpFmpoM0L1wqEuujFGuj4t6gBGgcGHtlpUbPpLdhBuyx9z0vZiuk47rQrQ7i0Ho/l+Z8EqjVgcXoncE3DNTMIWNBNArsdtSUD+lGypHDByuqiqXvN4nfFKeXBtjTi/Csg6q63Eg8ahsh1P0fh4Gl0eu2HlQwpZOxftdNxAumhimiQGCuG9YXTLFkbM913VTYKxOWtKbB3ee66YSBXh3r/NoIN1w1p7+T2joQDu3p4szKUF93QEk3fHWtX/rwl/HH958AANzxr16hWywA09EC4bLBTYfm8djZNVMZyy6EXs2GSSymy53NIhTbNaM/wdemmG6YVzznMP7kn34LPvcL34VXP/+IZSVM81wzV59GrzR8O5i6A4PW6CuSsTw4/+Cvfxr/+a9Nv/H1UVqyV9oa/WSM3n4ozI3PGX1aMPpuFOKnv/NmzHcjCCEQCFhJ4FoffTEd5u4clwm6Vctcovo7z7pKs9DZrpFuBmyN0VWWbKRWtL0icZjnkuVy7PPOg+QjZ1bxo++7R2u1+tiErU1zUECfcQI93W58puOTblyNOw5NFTGd0jAQSJhER4OkEAIzcagZ/R/e+wTe+Hv36/3hnUfXhhlOLw9xZKGHI3vV+aFAT/t9ZE8Ph/Z0VcJ7lOKdtz+M99/9uPndQKAbq5Ygt/z8X+LhM6ule0odQ2ANMHPdyHrtkTMreMWv3GENSrwVsm5qVgT6e792Ad/2y3fg5e+4HXcfP+913dDg/rY/+xJe/o7b8cO/cbc+Ls7oqTqWa/TUVmXvTIznX62qr7nTjAJ9HBKjNxLd6iC1COV2Sze7itE/69Ac1kaZblwWF8wll7ZGT5jvRGoJsIIh8Wlnkkl0Ilt38/aRJ+kiCvCS6/fr13Wgz5RG6rpo+HSda/iEKJyM0XN31tmVIR5nfV0GSa5vWIBrt35G70o5dclY7ujgQXuUqkVcXAYeFk4aYLy9kjRbPiC4yVsadMgTPssGtHf8wAv1v+c6kVkKMc1w1bxiYGvDTC8x+aziwaV7ZVgk0oHyeef69h0PncGdD5/DAyeX8K03H9LHFgTC6nnEQZd/Jg6tplnmfJptSRbkhWJuoIyKBcT57DEO7QGdO5xmOpE+htuOncbaKMWPfPP1ePTMGtbZKmtrhXRzdKGHPV3FTtecgrW3fvct2Dcb4z1/cxxrwwz/47Mncd2BWXz7cw7pffuBl1yj7hupBpr/5YVXl84J+dIBU/sRh4H+vc8+voivnV/HY2fXcHiP6kND92IvCvEdzz2Mf/Gqm/GcI2p69amHz2lH05dOLpmlA1Mz+N1w1Rx+8uU34uL6CI+cWcWnHz2P1WGi95vO/UBX9Rr9/idefiN+7O88E2Eg8N5PPgpAxQ3ikhToaYbEn5thmlntU7Y7Gbu7An0xtXyoWI09DgJExdRZSzeM0c/3VKBP0hzouuXbOTowbCIKhNd1QxfIDUCmyEUx+m7h+vFp9EPvg+tq9P5kbOyRbkaZcQIQi6ApJMBaC1Qwejfwc+lGTfnL09zICfR0rlzmGQjBBhjU2iupOyBvleBWVFIg1hp9x54ZEea6kUnGjjLdFmFtlOKxc6voRgGuLppY0UxQ9S4qAn2Fjx6A7vDI/daqGKxw8FQkawE1Axl6pBt+Xkh+WmUJQfe88oVHjEZvN4Xj12emYypVz68O8cJr9+GXvv8F+KnfvRdL/UTnXlaGKfpJhptuOqjlFrfz69974dXoxSHef/fjuLg2wtmVIY4u9LTLJQwFbrhqDj/3Pc8rnQfrGALBZmhqwfk4NJ07KVfAcyKDJC9aTajipn/xqufo2TglQ6luxWb06t/dKMDb/v7zAQDv+9RjeODkknb8dCJRmoHrIjDHBk3XgzuElvoJbj48j8X1BEkmMUyMZDhMc+u5a5OxE+BZh5TG/dUi0EehQOxUA/LWrm52fm2Y6mkvMQu68eZ7UW3rAjdgcekmY35xCpqWdJNmJdZMEkXZdZNbsxL3fVWJm+Pimgrw5AGm9Vrpu/mNX2rLXCPdLMzEFYze1uj7OtB7GL0l3aASpG/XMXo3GeuuG0qYK5KeozTHIMl0knhtmOKxc2u48eBcqY3voGh/wH+HwIP3186p2RO5UQBop1Alo9fSjd37yPW8A0ZuXGdmgVKgL5xUtGSgei1AlrFkLLtHZ+NIB/pzqyMcLFbWUmvPGtfNyiDBmZUhjix0EYVqhuxbCxlQzxi1OuYrjNVZaDlo3Vsppa4wjwJDtqjNtB3o1Xa8PQVVUAOm/QUnNtxH7+srRcn5KAhKxIybD9x9B+yZ+jJJNwVhGaaZXih9mGZWcG+wWNqmsKsC/dGFHmbiUC+TFoeipHXzIpM5VlhBCxjQhaAbnW7W+W5Uu/CIT6Kg97nlzUg35ru4z55AN5KvBQJn57QdtyxKiRKj3z/rY/SwPmuOyQ5MfBBa6MWWjMEZPdfPKfC6mjo1VKPPunZJDrLkWYG+OF4zIJN0YydjXcyxYDlIclxV6PHrowzHi0BP4Aud+yqoATvQ01rE55kEQ505Kxk91+h5JbbjeQeMHEVM0y/dVDH6clMzQM0k1pMMUkqcXR3qTo1RoSdzZp3lEkf3qtkOzbIAak8t9PfOdiI8VZyLEUt4Ng30dExJphhvL1ZedSIipz2MnrZzQYH3Gft6el81o0/9+0a/TzP7KDS1CmadWZrh27/ZCc2+E7RGX1yHYZrrRPEwsdt4t8nYCRAEAjcenNMVflGgpn4zsRnxOaPfw3piUGCiAh/TXjQvto1r+9FXMfrMcY14k7Fejd5m9HwB8gUW6N1kLQUNsn1RwN/HGL0bAMq9buxj5Pr73pnY6XWTl7YBjLRRYvShMJWxsp7RkwedBy96MOec6sfx0o0paBtluV4kfamf4Inz606gJ+nG6Ko+Rk+HS+Mkb2dLclM3CouKVb/Vr+S6IabJ7qegqANYt3z0rr1StdYYZaYzZBySdIPSd87GIQajTOWo0hxXEaMPBEaZ3XkUgF5tidsk3QFnrhNq8pCkZlZQl4fhoHslyXLN1Gl/AMboBzaj983iaL+OLPRMzx/2jPCZKIGeJb0ec2gYPbeGAmViF0dCHzftFz2rZBsdJJzR55ZG30o3E+LbbzErVMWRWpaPP/yc0ZN0k2S59lgTW+asBVCFN/6kmtHwOWjEpxvMTcbSd6WZSlqW7XJ+Rq8YTKi3d/vRkw+5n2QYJJkO+PvnHEafSSvgcrh9OSIn0Fu9btj03MfcXE2dPPx0bup89B2mlbuuG7fMXSdjKwI92eiIdffiAHPdCF95egVpLq1ATwtrr41SHbh8lbEH5rrWa+dZ8yu65jRouPcOt1fmsrwAhRsceR2Az3VD2/Nz5bqrbI0+xHqS6rzCQc3oVTWyew/oQG/1lbf3gzvaqMFaFAhLVqkDXVslc6iOq7Q/gJ/RD4rnwQXt1+E9vZJUyStjLUZffIa+PwrtnlV0XPz73d+jeEE96hdmYmUb1YzeSDc76aPfdYH+h269Tv87DlQZuNWuOC4H+gurI3zq4XMA5EABlAAAIABJREFUwKQbYvSFdFNo9G65u7bDBS6jV/+nghU3cLsJrWrphnz/5P7IrMpbd4WpIZOEFteTCo1eOV8owI8rmAoCk+tYmLHPg891w6UeN/gHAWP0eX1lLD14fR7oQ4fRO8nYKo2ervX5Qkef6YSY60Q4dlL1rKf8DmDkn+U+7z7o2itzzHdD61jPrdrSjQr05V4qgNFkdWVm8b6WWZzgOMcqe0eeZCzdL9QeAbCdOIAt3cx0VPUv5RUo0JNG7zL6o4zR83vXZfQEmonVLRXpImbSzSBR1lzan0GSaXdSWaMvhzG6d1Ruoej5Y2n0eemc0LFQLiQKjHTDG7LRe9a+Oxo9tT/YOxNr99gwzTWR5M4foJVuJgZnZlGxOABn9HyUJr/tuz/xKH7mj74AwAREGm2NdFMurABs6YIj1Iy+KJiqsFe6CS29n04yVmv0aY5ubIqh3F43/AG9uD7C4voI3SiwAqDbj35cm2LeC54YCS9nB+xeN7wdRIl5MkY/rtcNHXt/lBnXTZV0M0ajJ6ZPDLYXhZjvRprh31g4tgAzWPCmVD6NvheH1rH6GH2XzUo46MGm/dWBXprzae+/sYf6NHq63xSjD4rXigStJxk7U7R5OLdSDvTcRw+ofAgla7lGP0qldd/OMll0lOZFl8rmIabDpJthmqEbm8ZhZ9gShly6GRbPgwsa+A4vdEvuI75wPb//9OLz2nUTaAKTuPf7GI2eB3qalSjpxq/Rt4x+A3jvj74UL7vxABZ6EaJAlB5+ChJ7iv8/cWEd+2ZjvPW7b8ErCumn5LrpmgvE4ZsC8r+zXLEAYg6uRs8XDuegGzUOVW96rdEnGXoeRm96bZv9u7g+wsX1xGLztG+0FB4wvjI2DExSmzMSdfw00BnXjdUn3ZOk5ost1DJ6n3QTknRj9yhxF0B3QfukpZtOiLd+9y14/cuux5u/82YrWU2DwsWafuK0ahdJgfPdyLJXZlKxcu3gcQJ9Ju1ATzOxKkY/342YvbKc0+GMPmKzHy5Z8Ng02wnRTzKcWyPpRt0jqt2xCoR0zx+c71o5oypGz2UzJd3kEzF6S7pJ1AI+lGTmXTNdRt/zMfqISzf2erq8Mjb0MPq1oWH0gD2Lce2VhJJ0MzCBPi568bvSzU4y+l3loyd819cdxXd93VEAKjC4CbrZTogLa4bRP708wLMPzeOfvfLZ+PiDpwFwjd5IN4Cy3O2FCQq+aTHAk7EVjN5pxlWl0dNScnrFeofR6xYIRRziDo7F9QSL6wn2sSCmvjOwe+67chSrDaBEqGb0RaAfJBkWerFXo+cPvMt8ggBWoVadRh97pBsKaJRU5xo9T7q7mHOkm14U4FXPP4JXPf9I5bYXmYvGbZA1LHzetB/PPboH9z1+UfW170TI8txi9JUavSPd5B49HVC5pQus34rPtqr2yyQn3cI4l9GvF4xeCOhK4SgMkOSK0e+dibE6TLXjBjCWV7XPmc3o2QCvpZuGiViASzeK0ffiULtuKNDzAY+Od69DZGg/AWhbaMJaQVRq9Np1k1rfYXXerHDZ6URyWpZu4jDAsHAvzXUjBMKWboSo77m/FdiVjJ5jTzey2BpgggSf/lOy0mT+bUZPI3GJmVUwestHL42N0A30FEBKPvriJosKpmzaFFdo9OwmJlxYU9JNidGHwgpc7poH9F30GyEL4to1kNiashoM7EQpUHbdREGg9emxlbFaM+UJRvs3nl4e4J23P2z1oveB9GNi9JNsC/gZ/QyTbp5XlMATq8/yekavXTdMl6bPASgNgHMswPmkG2pvPWKMPiw0+tTDXmeKQq0zK0Psn+1Y1bRUUETPBCViATcZK7XbhJ839V5u9d1pgg579ojRk+vm6cK2+ezD82V7ZUU7BQA4pBm90ehHrAWCxehDYvR2sZ/VeZNaIDjHRYOU1ujXeaAXenGZXtGqYcgGm04YtC0QNotf/QffUBp959h0m0DB0K1wc6Ubtw2C7yHifxvXjXpduRDGa/QxY/Tce06+YVejz9lNTFhcH+Hi+gi3HLU7bgXCtHYgHdc6puK9bhxirQiydA4XnFyF7aP3STcOoy9kqLzw+49rgQDY0k3s/Maffe4Unl4e4JYjeyr1eb49JR+rkrZ8W76EXJVGT88nBfoLayNcd2BWV8ZWMnrWvRLwJGOd2DXHFk5p6rpx1x2wkrHF8Z+4uK6LxwB1P0mppIWrCybPm29ZyVjHFsw1+iTLkeR57fV1UXLdFMlYADh5sY+5Toir9/bwyJlVPLXUx8ogrbZXFrm4hWIJSyVhmfeJqPCBqJSMpTYkoVngna+/wBEH9oC91FffsdBT/XpocOpGoVpPOjHSTScK2hYIm8X1V82WXtMafc8cvp66MiYOmAtnkrF+Zube0BEL9GrVe3UjCCFKDwvgk24Mm+aMnh4Asu3pxcG1vZJr9CTdlDV6/u9hKq3Fr+mm04xeCF0laFwktqYcBkIvDjLHLKyxZwC0LH810k2XsV3TAqGQh4rfOF00uHrMKXpyQYGNGHfdoBAX/mmqLlbH6wT6NCukIjX1vuWoSuZSQpZmK90xjN6tlK6SbubYUoij1FMZ63PdFLY+n3RD8tqJi33tqOHf0x/l2D/bwUwcWonqTmSClluhy6876eBVORMfYjYojjJFaIgoPL08wME9XX0e/t+PfAXHTi3pVgkuOlGAIwu9okpWzWx4MCWXlq8ydrWG0SdV9kry0TPXzVwn1IYQIgQ0G1eM3pC81ke/DaCbfL5rJB1i9HRjuT56mr6+55PHvetZug8mJdNUP/Vy0sddi7QqGRsViVAT6Em68TN6y3WzNsJiPylJV77pKr/PeB8Q2j4MhJUEppmNbuoWmkWseVFa7BxXIIrlAStyG/Y5MJ91pZtuqHqc07M7ynKrc6WLIBCY6xid2xccOJQjxzD6knQzUrmSA3MdHF3o6SZbZLFUjD6oZfRhYFrdjpxkbEm66UR6NatRlpekvtgz+9GM3nOP0oD95IV1HNxjGHvM3DuznRAf/Rffih/55uv1+yVGbyVjaRW3EFmh80/C6Ome5+yXXru4rnq7z3dVf6onL6zj5MU+BkWrBBcvum4f/u7NB/VxE+Ei6MrtsBzo6fmOLUZvNzWrslfyQE/GBT7YdZl0YzP6xqdpQ9j1jN4HCkRcuiFGTxdF++iLm/obn3kAP/Pq5+A//dVX8eWnlvHrb3gpbjm6x2tdA8wNlOdFm2KHOYy1V7KmSWbqKYvueIbRlytjDXN88uI6slxi30w1o+9EATC0WwZnDtskjZ4ngQ2jN1onDW6WdFPB6KuawXHwIOImY4l1c6lqZkzwnmXBu066UccQWq4bdyY3LHr8/x+vuAlv+KYbdGWp1uilRChQ7brJ1T1D133oMHp3AJzthnqNW19TM52M9Wj0vkZpFJTTXOr2B4C579ZHKeIwsNZIAJzEpOP+ObLQxdGFHm4+Mo87Hz6H/ijzLhlYBbrfSN/mC6Mvro+wb7aDPT3F6J9a6mOY5pUtEP7Vd99ijknbTM3+U58fn+vG+OjLydiqal/XTbc8SHQ+i8uXaoGUoCiYMoG+ZfTbgFmfRq+lG/Kt21n2OBR483fejN//qW/Ccj/Fmz/wObUdWwCZw21q5rLocdINXzxCec9NOX4vLjN6k4xV/79qroPHzqmuiiXXDWMYvjVndaAvHiBaf9RdIpGOj/aTAkydj961ujVJxvLt6LzEUVA6Z1wj9mGuE+oZz9hA34msRaGrkrGH9/TwvKsXMNuJMBOH2tWT5dSm2M/oVS8cu78LUM0Y+eLzShLxzwB5/yDD6NU2nL2S7v7Ca/fix/7ODfr1iM3wvMv9OSSFX4M9vRh3/+vvxLcWTJq7pZpAF7XpWVeo9+HieoI93Qhz3Qi5BE4zX/24a0k+9iyXWrKjgdfnull1GL1KUBfEr5CjXHeXYfTGR28YvTlHtIj5MDEtENpk7DZBM3qu0c/ajJ67bviF/ZabDuJ7vv4oPvzAUwDYVNvjLqH3ub0SsBlClb2Stg8Yo6cEkiqAKh5mp6kZPYSHF3r48lPLAOD10et/k8bP9EvuBqDtSYagAcbrunGqVvn3E6gy1l31yAc+5XUrY+NCuuGo0935fglR3SqBMN+NtCw06y73V7hS3ABzYK6jZwFkHdUzII9bizN6t2DKPS80iFGTOu52AeyeLbaPPvd+50tv2I9PvvWVuO7AjBW0Oh4SwNEJAz3LrVvkGwD6SV5KxteBNH5KgncZ011cH2GukG5c+CpjOfiSir04wFLfr9Gb5H9utW5wGb1/GURbo1/uJ7oVNs9T0ZKHluuG5T22C1dkoL/5yB5cf2DWSh6RlYxuTM3oPYkvKh8HUJlUpL+TopeNO0Vsbq9UbFqtdlRsG4dFC1fB5Bbo/QWAb7h2rw70NzgJ6UCUb26b0eeW00YlY4W1XNuwpFkGjNFX++ipMraqpwtH18vozcPnBrtxzO71L7sen33iIl7zdUfHsn/uCV/oxRajp2UG3YFl32ysZwFUf1DX6ybgGj1JNxXnhc4pefurXFqAIR1UB+H7TiGE16gQeQYM63dqGL3+HAXMUWaRqXHQLUkYoycpMskk5nuRZaAgjGX0gaknoGtGvZFCi/QYKzMnKJ0o1L74JPO3deA1AIDD6Nk56ka0fm5mmR5aRr8N+MGXXosffOm1Vt8a13VjM3r7Zu7FqqpQyuqARQ8bPRR8IOgUGh3ANXr7ZiWGwDV6vVxeHODQni72zcb6e3mvbQD456+6GT/7Pc9FlkvdkpfAH2C3KRpg9GN6j/ahGwXaRVJi9KHx0fMg6rouAtd1U2uvtBc05/veCQ0bvvnwPB4+s4qZTj2z+5FvvgE/8s031G5D4IPVnqLP0X+/70ncfGQPrtmnepy72vCBuY4OUtSmmGZAJddNVTK2qmCKGH0RcKoK7ABzjkLqR1+R4PUhasDouZHAv03h3EmykmxYB2OBVeew69gO93QjK9FP8LVA4KAFTfgsrJ8oBu3T2vu5nVvgs5g0rzpmuo6G0WuN3mX0cYD1tdRi9K29chuhrY5Z7vHR25ochy5bT/0LGADm4lIg59IO/SZ/v1qjL8rAM8booxA/+fIb8f0vvkYn7VzpJg6Dkq2SEPKb2GmKBhhGz6tzf+jW65DnUjeFo+rBSX30oRCVCz+48Gr0urAn0OvsvvSG/SrQj3ngJ4G1QE0vwijN8W//4kG84pbDeGuR6HMDzP7ZDp4slnAkaSYOha6EBIB/8+fHcGZliH0zsV5TFigz+lJTsy5JNyN9/Bz8PHMrappzH/344+bf67NG8kW+R2nZ/cO/Y32UTdbUrEiw02DZjQOrxmOuG1kzBJqx+AqmOKJQ6Bl4VzN6/3oJnShAP3ECfSRYwZS/2ldr9IVVmXI49J0E5boJithhntUsl/i9u76GYZLjH3/bs2qPZyO4IpOxHHGoen2bAiRKatVINwWT649M5ty99vSg0gPujupGoy9uQOdmPbzQ1VV11EiKbzvTCXHNvhn9ULuVsb4pNcGr0TNGT6XrPE/wgy+9Fj/0jdfpqfNynxZSN1PgVz//CN70ymfbyVh3AAxte2V9C4RxGr3690tuUGv1zoyRYyYBH6z29GKsj1IsD9T6qXQd3IFl/2ysgxQl4EXRk54Y/bFTy3j0zKpm/IYJStx27GmtwZd99Hb/nbr8BEkyuqlZg0GVYN8bHubKm5plY6SbJJuoMhaAY4ENrWdvnmn03SjQkuQ4Rh8GhljpZ9ej0dPxAU6SNuROo3IinL4nEEWhWNFHSufR2DnocY0+s6Wbjx07jb/80lO1x7JRXNGMHlAXlksNdJOOahJOZvqXIcvtxI3+HofRc4Y21w1xajGx3ncfmH/w0mvxmq87WnTbDDFIM4vRE4jRuwuPuIGAg9/cPtcNtQ/mzh99XGGA+W6kmzalLGA/7+oFPO/qBdx9/Ly1PQetGUtErUlTM77PXKOn928tAj2XWzYLPljt6UZ6MZvTywOt77ra8P65DpYHqV5jgK65WmXKTOn5Orh0DE8v9fELf/olXfTl89EDPBlrn9fnXb0H737DS/Dsw/O4+bAqcIoDm9E3kW7491Yx11yqpKQKeh5nDpNuJul1A6gBlqqXu1Fg3T/zXaPRX723h6N7e3j07NpYRh8HQkuNWqNPygRM7buZMerXeDK2plFbHAZI8lwvJO6SR4Ax+lJlLNXIbN09zHHFB/o4DLQ+r/42NjWgQrrp8EDvD1ZBULQ68DB63pipUroJA235VDdGbmn0BAqApnulKWCqgm+xBR+jJ7nJlREWepFOTmW5RCDsc8DPl697ZSal1TitCnX2Slr9pxsFuPHgHH7zx27FS4uAvxUgRh8FAjOdECtFa9ynlwb6QXYZPd1Hi/3ESugpBqc+s9RPCt+0OiY6/2eLgYSSuaUWCI50464wJYTA977gauu1MFAFZXpd4wZBl/dw8bF13Tq6CFRubgmwycMk0g2gno0TF1VfG7XClHGjzPcifR6O7jVFauOSsWEgNEkyy0T6NfquU3EO2LOYukZtSstn7jinTYk6psJe6el1M0pzzM1tT0i+4qWbTmQCKsB89Fm1dMOLYIjR+6DaCxRTRM5MepFVRi5EfcAjTc/YK8tJSu26yWx93YfQE5R5Mohkhchh0YSFmVgvyqFufH9+ASg3fwqEQJZXd/3k4J+l47QKpkJT5v6q5x+xruNmQVIJnzkASoqjxef5ql2AWa7x4tpItWD2MPqlfqKLmLiP/kLRboHOazkZS9LNSO/XOFCQ1XmiCZOxPtml1PgrKn8nZ+ETSzdsJkVrxvL3aKZ19d4ZHC5qAZr46EduoPf46AFmKbZyFePtlQD0IuCakEXl2QG3V3JGn0mVg6ubiW8GVzyjv3b/DJ5z2PTy8Pvo/XroIFEafdUDFAjhfcj29GKsDBJVzl5c3Lrl1nqxYoTGXmn2h+45nowd11/EJ93wNqmUcKIb2g06CzOxZvSpp8zddm64jF5JQ000+iAQuliFZhfk7IlDgX9w63VWh8mtBEklZIfj+PQjSpp6FusBA5hajAtrI93UTH2Hun5JlmN9lGG+a6QbGpTJO06XocQ0I5WUJ42+ScUpfQfJeY0Yfc1sDCgXFfkC07jvqAMP9GQhJsx3I3SjAPtnYzzr4JzedpyPPmIafc9JxvrOszoGe2Yz5MnYSulGWLk03S7aabNQVRlbtYjKVuCKD/S//4++yQqyQgidxALg1SG1dDNSiS63WIoQBSZb796wtNL9sMKLzNGNFCMkpmD5yz3J2HGswLJXRj57pdRtD4BygFjoxTi5qKbXvqmsr0cN/5svTD4uXqll2DLLSQKoh8OVKrYSJoiE1sAKAJ9+9Byu2TdTanVMDP/iemIRAGL0fBbEF13phIHVbgEoD4BCKNNAlevGB7ouRBCaJWPrXTeG0ReB3ifv1Fz/ceB5lm4UlJKxQgh8+M3figNzHdx9/DyEgG4/UYUoELpgjS/8Tslya98jQyT0fhSMXkqJJJeVRWC0wMiAFTYC5nx0wkBbbvkqXp0w1NLNuEFro7jipRsqkrBeC8yCxHWMvl8w+irZJWRJIP4blFBaHabeBlUuiBG6OiP/Xp6MHTdw8CpKrdFze2Xh8aagWpZuIh20Ms9AR+fLVyoeBirxZLo01u+r64Lg0s12giSCThSg6/zWxfUENx2eL31mP2P0gBkgu8WMzBTd5NaiK50o0K0TCL4AOd+NJmL0JtAX7LVJMtaRLFy4qzD5SIUle0zsuuHSjeO6KZ6bZ+ybQS8O8e3POYRPvvWVuHZ/ufCLI2SEiz+7vnPsthYBzDGnuWqM5jrJ9HZhUKx362f09JwTcVgf2fJXv2L9263AFR/ofaB1M4Eq141JSOVMi3XBb7DQYfSAWvuyCQOn5I3PiilEsdQgW0pwIkZfYa+0GL1zfHtLGn15oFT/99jQhGkLAYxn9HTuKWjOxPaiMdsF0ui7TKPn5/WmQ+WWyBToyTVCgZVmZEbusvsfdaIAy2wdVNX6uHxPzXYjq+HXOFBeSDu/Gjzt8TiNvvjdlaG9ClPVd1TNdqtgSzeBJXvMO/ZZIQSuO1Af5AEVaOl5nmPPno+g+eyVfIHweulGFVYRo3fXjKAAT9dunbp0Fu/3R1mj3MtG0AZ6D6hHCOCXbnQydpRVFlAAiq36ps060A/TyjJy9/eGSe61VwKmrYDa37xkvSvvV/km5muPZMUx6eZYzvEv9GKsDFMVsLPyKlH0nb4HgipjTf1BfSCgc0NB8zueexi/9eO31vae3wpov3ZsAv3B+Y5eW/WmQ2VGP9MJMROHOtDT4KRyLLkO5rRgNx27OzBXMe+5bmQVxI1DSbpplIzdWummiv1WQSfBC5mDf9fcBu2zfMA6MNdBINTM18foq+yVgAr0SUVlLKCYuZWMdRw89NzqRchHZhFygBj99mj0jQK9EOI1QoiHhBCPCCF+tmKbHxJCPCiEOCaE+AP2eiaE+Hzx34e2ase3E4rR5/jol57CICm3WtXJ2GKB36okV8RsXXww2FMsS7g8SBoF+m4UYJBmXo0eKIKnlYydhNHTtNREenKM0A3qY/QAsDJI/K6bGnklFEVTswYtEAAW6Jk2/x3PLa/zutUg5tcJTSO3fbMdvS6BL9ADqmiKWhXTsZFvmvdLcRk9R9X9xJfqa7KgR6k6ewsKpohErNYweqtKd4OuG7rH6V7ii5BMCpfYkA3WG+g9Gr1pU5HXEjut0Zd89BWMfpQiEPY52i7pZuz8VwgRAngXgFcDOAHgXiHEh6SUD7JtbgbwcwBeLqW8KIQ4zL6iL6V80Rbv97YiDgTuPn4BH/jMkwCA5zhL8ZlkbIZM1mv0Ztrs0egHaSNNvRspP/TqINVMx/qdoggJUG2Kx0k3vh7cubSTsVFot0DgoB4ey/1Ut0vgiD2FVvy307xZUzPAMKwmbHQrYVw3oT5HB+Y66EYBjp1axk2H/TOK/XMdnC0YPR0/MXoK9IAK9jwZy1F1rLywbxJ75TDNK+UgF+NaINC+msVB6qWbSVaYAuyZFGDYOF8kaFK4K6pdNdfFudVRvXTDGT2TbpKsuiMnEUQt3US0SDsF+NA6trWhqhzmX3cppZuXAXhESnlcSjkC8EEAr3W2+ccA3iWlvAgAUsozW7ubO4soDLSrBChPP+kCUjK2KlipQF/265akm3EaffF7y4PE+2Cpjnvq36OJpRvS6M371NO80kdfDFRL/cSv0dcx+kDoNWOB8QFcM/oJJYDNwvLRF8exbzbGjQfncHC+ay3WwXFgrmOkG0ujz3ReA1AJUoqBdQ3KOLgjpYnfmpjiKG1eoWoH6QbJ2C123fABln/XZqqeeWAOAoGDe6oZfXcco/cUUOrtSsnYIrcTOcnY4tjWRyn4gj18m61Gk2+9BsCT7O8TxWsczwHwHCHE3woh7hZCvIa91xNC3Fe8/v2+HxBCvLHY5r6zZ89OdADbAV5gAQCPF42qCNRetp9khV96vHRjafS9STV69f5yPy1Z/YBiwW2WjHVdIi6atEAIhXlAqqQbWgSjykfv0+ipMrbJwiMAY/Q7HOhV64nASsYemOvgn7/qZvzPf/otlex432wH54oq15Ax+nKgNzOhpoGet06eRKMfpfnYXIj+jFXsVB0IVwfNpJtJ5ZY5l9EX99Ak7Y5L+8OOIxQCB4tBui7Z7HavBCgZW10wFWkfvb8y1gT6YrAcZaXixmn30UcAbgbwCgDXAvikEOIFUspFADdIKU8KIZ4F4HYhxANSykf5h6WU7wXwXgC49dZbt7dfZwO4lrDVQXlRgJk4VMnY3Kzo4yIMBIbDanvlyiDFMMuxt1M/LdVLrPUTb7KGemgD0Isq18H2SpcDfZrnxZTSH7CNdFMweudhrpNuAmG3KR5XxONzQewU5rtR0ftf7cP+2Q729GKdY/Fh30ysk67aXllUNnPpZpjkpruhK91UMvrqtXh90IF+grVbxzF6eo0W0PaRFNu5s0HpJrKtib72xE3Bjz0MoAN9nUbPnxGzClhe2Y8eMBW0LqOn54OCON1P/YLRWw0PL2Fl7EkA17G/ry1e4zgB4B4pZQLgMSHEV6EC/71SypMAIKU8LoT4BIAXA3gUUwy6kAu9CP/fD70IN3qsdDNFT/qMVUC64Bq9NWpHITph0Fy6KW6YpX7iZfRhIPC182u45/h5JFnuXZzB3Z7AffRnVgZ4352P6VlGlb2SAv0S9XRxHpggUJ38mkg34wKBa6/cSTznyB7ceHBOP+juIus+8P7rpmAqRJpLq4p3mGYIg6I1tpuMrdToJ5NutEaf5I1zHD4SwOFKNz4ZI7YY/cZcN9qxUuzPuHu6DnwfAyF0gZV3xsJqQAjcdZPmeWVtQKfQ6IdJBiG4g6e4DyI74CuNXlj3tu/53go0OXv3ArhZCHEjVIB/HYAfdrb5UwCvB/DbQoiDUFLOcSHEfgDrUsph8frLAfzylu39NoFG4Kvmu3jV8/0Oj5lOiH5SlLLXMHrT1MzeZr4XYWWQYJSOL5Kg95cHiZdNBkLgzofP4fNPLOKa/TPjffSeZFmeS/z1l8/gPZ88jvluhBddt0+fB5f5cOmmKkcRhUGtdJNNaK+8FIz+A2/8ZgCqEhZAo146dG4AUyNA1486YAJKuqm0V1ZcPnst3vHnQ2v0Wd54oGzaAoHWI6jrRw9Mft1c1w0tpbmZuglrmcVQNGL0vsKx8T565dcfFBWuJO/RwOAyelqAPZwGjV5KmQJ4E4DbAHwZwB9JKY8JId4uhPi+YrPbAJwXQjwI4A4Ab5VSngfwPAD3CSG+ULz+Du7WmVZQ8rWOwZHu6mO0BKupmXOm57tRc9dNcYMs9xOvLPMdzz2M6w7MYGWYYmWQTpSMpWCe5VJXdK4O7Smle3xznRBhIApG72/q1gkDLyPU0k1TjZ4e+B123XAcXehBCDQQDsPLAAAWq0lEQVTy7vNAHzBGDwBnlwf6vWGaW9IOoNohA+NdN76KYx+4Rt9UuhHCXHcfc6VBaaVGo6fe7GofNqjRM4kyCoR3rdim4PdnIIROpNdVxkYeRj/M8sp1cgHuusms6vXY0ea5Ru8y+u1y3TQ6e1LKjwD4iPPa29i/JYCfKf7j23wawAs2v5s7C7rIB+b8zgpALWBAgb7qwQwDwZpUOYy+aFXczHVTPFzD1Dviv+MHXogPfOYJ/NyfPICzK8OJKmO5dHOBSQu8BYLLBoUQWOhFWO6nSD0FU4A6h75AQet3NllKEDBT6Z1OxnI869A8Pvvzr27E6PmqXiYZaxg95VOGSdl1c3ihi5WzabWPvuvX9KtA13m5n0y0+hatTOVl9A0KpgAV9IZpde/2KlD1Kyc0r3reEXzTs66a6Hs4+D6EgdCDcd2MxZeMTYqOk1XErheHWB+pepeeM1ABvAWCem+U5moJTovRX8KCqSsNdJEPzNUz+v4o87pOCBZzdrbZ04uwMmjmuqGLL2X1jUCzj1E2fuCwFgePDKO/6AT6l1y/H6+85ZCuBuWgDpZVPcejwC/dBMXg15TRx1MQ6IFmsg3gSjdqn4mJp7nUxTqW66Y4xqN7e9bnXFBCskkiFjCB7PzaSOdVmoAG6DqNntolz8Z+rqjb/U7M6O3qUQB41xtegu/7hmdM9D0cVa4bn+TqbWrG7JV1Tc32zcZYHiToJ7Yhgs5jz5Fu1L7ZvbbaXjc7CBPo6xh9WPjoq6fFoTNl5NjTixrbK/nFr7oR9jMm6esRzuFbADrLJS6sm0AfBWrFqN/+iZd5B5eFXsw0er/zwvdAEHuh3iPT6qPfKKxkbLHPr3zuYbz4+n0AgKuKQJ/mrAVCcYxHioU0KqWbbrnhVh3ougzTHHtnmksfpg6imvGeWx1hthNioeJ7qxrijf9tZWkd5xybBPz+DIP6ZKxvgLI1+uo24HtnYkgJnF0ZlBoP9uLAWgaREAjsiHTTBnoP6AaoZfSdwnUjq4NQ5CSBOOa7BaNvoNHzm6ZqkQW+SpZv1R+OqoVHuHQzLnk30wmLNXP9Gr2SbnzJWPX/pmX5lzIZuxHsmym7bua7Ef7gp74Z//QVN+H7X2xKUOjYKZAcXqhn9POsLUMT8HO2UGMJdWF6Ffn1d9q/q/f2KnMFdf2OxuHG/7+9M42xpLru+P/U8l736+31Mj30MjM9S4+HAWaAtIFhcYAEGMDJBMVKSCJnFJEQx4OUxXICIkqI8yVfssgSiUTkiZ0VKZszkVBiYkWylM3gCLBnHMwYbAECxhgIE0xPL3PzoerWu6+69v1VnZ/01K/r1au6dd+tU+eec+45MyOhGSnj4DbdtA0dY0OG5wM1KAXC6rpVItLv95GzuXPvXtgSD//n912L49cvAXD7H9zOWDbdFEZUjX51LbjClCos3YNjdMiqu7q+GZ6yIIpG342j0avaimK6UQV9mGCVseF+NnozwHQDwMnFHfZAccIrS3TGxkE1kajXNtzS8atHD+Dg3PiWz6UguWTc30kI9ExAUbU+tf9jmW704IerFILz3eHQYySZiX3+xA04ccu+2N/zo88Za7/fNtoOWRmr2OiVKBn3ZypS0L/x7uqWOrYrS1OOMtYyNHzyjg9g+3gbe7aN9AVqlJbrpon0nLH+N4djutkMTmrm9R6wEpvJQs/hUTea53sV1WQQK9eNk9TMZaMPEaxtQ8ebG2u+zqkfOjyPhcmtgsBwCfqwB0ov98hgCPohU3cKjXj1YZ926Tbd2Bq930NNavRR88ck1eiDUlgA1phZXb+IOdun4EWaugFhpQHj0rdgyu7ba3ZP9eUOknjmunEWOHmXH5RIZeu9tc3Qazhxyz58/Oa9ICJ84fTrW86VNSzoPZAab6BG39LtqVxwUjOJ++YddeXdDkKdzg35TO1MXbMiYezEZ0F4Za9cXd900qa62+7ZJtMqhwafqewv37bf83uyH+Qy8TBN3YmnHhCNHgC6wy28vr7qPdPxyAMjr3F6tG1nM8zWRg/0O4nDMOzwSL92tAwdwEZuGn3WePX57/zoIc99e7UHtipD8v7wc8aqfRxFM5dmL3bGloTURqY6/pEWQ3aumyCNXtXo3BrptGJTd5ek23KuCBo90IsMSZLU7M3z/RWOwjRoK/XuxcAKW0Hnlhp91Fw3caM3ykTe8F7jwh3TDaiJuwyMtA3f8dSRKROimm5UjT6GM9bUg1MCS2E0PxEu6ONmr8yD/hQI0RQLr+yVPdNNsI0eiDcr6V8ZW+1cN7XCMd0E1KIcaskMdP6ZAXXdf4Ddc/UCup0W3npvDXcfCq59qmroQc6ayU4L3/7u92KZbuQNKVPrSrNDuKZtpd5tG5rvyuCgczuCPuQ8Tk78wZHzmLDNaF7X1q9dym3WfmNDBkbbBvxko6Fb0SjRNfrkztiggiHyQTPXDTfdxM1HnwdGDEEvAxncdn1DI7x3QZpuws2ncaKG1HES1dEeFxb0HrR0Kwf5SICmLRegyFWkXrjjd1Xaho6jl18SqT1E5Dg/g6Z2U1E1evIQ9Oct+/zS9Aj+5/XzEZ2xm9C1eBEx8gHiRN2EaHwt+8FWdD76NHQdjX7rZ+4IEAC4auckjuyZxux4Gx171bEfIy0jetRNQmesoVPgGJJCfC6KRl8B001fmuIwBcapCtV//Va22g37eN7HGDItubG2cTGWRi9/b6L8ZkAs6D34yWt34PCOicBl5n2CPmBlrCSt6UEK+qABJDWKVshgkUnHLoqediY1+j3bLEEfponJOrZtQ4tVF3RL1aOouW4qYAKIirPy0nNl8Fahc3hH18mrM9o2AoXRSNuIYbrp7TceIymYqWmB41Wefz5Qo6+OjT6ORr97ZgTHj+zC9fv6V+KauuZo9EHCuDts4tz5C/FMN0rdgiipLZLAgt6DfbNj2Dc7FriPalf3E0KqEItbJNnNkKnj3VXvFAgS6VOIVH1I0+yyg/02+qVpK59LmNI4ZOh2xZ14Nnppj1zblGmKg/evQq6buMgHrlcfmgHmPAD4yMqOwIff0swIFgKcoCrq8SciZN6UGDoFCrOWrqHbMT2jVtRjWH/LN90ErVB3Y+oafuvY5Vu2twzNsdEHPQQnpKCP4VR1r5DOAxb0CVGf2H5CyCuULilOvckoztgIA0bXCNjst9ETAbumO/bn0UI+37eLJ0SltzI2okY/yM5Yz/DKYDPCR6/bFXjszxxfQdTeTh5e6b0GQtI29ECzDaD+buU/oL3i6OPSiqrR2w/UOE5VJ/oqJ0cswII+Mfu39zT+KOGVaaew0gnrF14J9NIgRMpV7lqVubZxEd2O6YSUhh1Ctmdt03/BmBdxo26u3zeNn//+PTgwFzzDqhITHf9SdWbKMREnLj2pjb6lU+B5Pn7LXmxsBtcHSrMyNmv6ipUnVLhG2wbesVOEhGn0QLyoG3eYbR6woE/I7pkRHN7RxbMvv+Ofj14Nr0wt6MM1ernAK0rSK3VVpqGRk3CrZ3aIvlo3TmSFptjoNQovWD0+ZOKhOy+NfPwqIBPMedroPWK680KeX9coMLDAzWSnhcnOuu/nNy1vCz+3k+um/JlYv0af7BjdjomX3nzPOl7Aw0s+UOMIbZ1Y0Feae66cx7Mvv4Oz3/k/z89VAZi2QpK7sLAXO6cs+7pf4WoVNdf8n/zMB/HUt97GoYUJxZEYLd44yr4quhJ1UwVHXR7cemAWv373pThwydZZiCok8vY7yO4dHzJiOfkevvvSvprJSSir1q8XXitj49LtmFizZ6FBs53usDWbixdHb/1t5ZTnBmBBn4oPH57HI/94BsM+WnbSDH5eeKU4dXNwfhz//uCtgSsWJXLw6xrhpuVtjpZ2zi6OEb4yNtwZHXTetc3oBasHjU7LwM/etMfzMzVHf95+SllEJI7ZBujPm5SUKi2Y6q9hm6zT1eywQfdzz3QT3xnLGn1FmRlt49QDN2CHT6Y9VZimRQ6CME0hipAHeoPVLWy7nRbmJoawcyo4e2BijV6x0VfBUVc0WTroo54vjiM2y/MCVdHo1Zl1smOoD7+gSCJp+gzypblh080AcGix6/uZ/AGzEfThGn0cZLinu2ktQ8N/PPQDEdqTzEavpikuo+B32WQRARLvfFqs9AdZ0dPoq2WjT3ovqqteg2YpSZyx7iymeVD+r1BjMtXozXAbfRwMTYvkDPVtj0eptChoTnilf2WuOqPWZC1Co1dL5xVJlQrGZOEXUetHB5l/Zu1U090Y6xZ6Gn1+NnoW9DniV1w7CXIqmFXlHbWARBLU6J9YcfRK1M0gpTXIEr9avHkwZGqZ2NzjElRgvGjiLJjyY2JYNd34H+PInmn8zceO4PKFidjtC4qoSwubbnJEq7BGrxOlWm7dZ6NP4Ixd36xv1E0YpqZhFcVc/x/8+FVY9KgLkDdOCoQKOGMNLZlSotKv0fsfg4iwsjQV69hSTrR5ZexgYmQp6CPE0cdB1yiVRq0+cJKsjL1Q4/DKMBxHZQEzmiN7p8N3yoFecrDyf2PZ35TCVKkWh8/a7+CYblijH0x0x3ST/gccaRswNMosH4ahpzTdZBB1UwVHXRnIqI06O6M/cvUiFic7udqdo5KFT0StBZz1al8pHvLsKxb0OeKEMGYgz37q2l34vl2TmQkH3c5gmZR+G32cZfnWvqvr4eXW6orpCJ6SG5Ijs+ND+OHD82U3A0BPuUhz70xEdMYmwV1SMg9Y0OdIlhr9trE2to2FLz2PiqFRqoGfNOpmzE6Xe/7CBmYirOCtI0aFVo02AekQThMU0TZ0dFo6vre2mbk5qogFU82cOxeErlX3hk5vo0/m4JKhfkLU23QRRJFRN4zVz0TpfSJydWzWqZc1FvSDTZHx0nExNC2lRp/MRq+u0qxivxSB1DCbev1lkHYGC/Ri47Ne0T1s6pgda2NpZiTT46qw6SZHsgyvzBotpUZPRE7ZtDjX1zI0DJs63l/fbKxGyxp98Rialvo+lII+6yACU9fw5Yd/MNNjumGNPkecBVMV9LoZKZ2xQE+rj3t9UTNk1hXHRs8afWEYKRcIAla+G6JqKm5hsKDPkSxTIGSNnsFUVkbNxIm6AZQKTBXslyIwKzwu6oqup5vBAlaIZRVW+iaBTTc5Um0bfXoNx9HoYx5HCvoKTnQKgU03xZOF6ebmD8zi/bXNjFpULCzoc6TKNvpOy8Bwyjj2dsLEVTKbYhX7pQhMNt0UjuWMTXeM2w5ux20Ht2fToIJhQZ8jVbbRf+L2/Ti/upHqGDKWPq5GLwthNFXQ91JjlNyQBpE2nHjQYUGfI86KvAoOsKgFSoKQq2PjCuyJpgt6mQKhguOirpgpU34MOqxT5EhvZWw9B1jPRp/QGdtQQWdWqPpSU0iblnvQYY0+R3pRN/V8njqmm5imKbloqqk3nnwwsjO2OAxNQ0P1CgAs6HPFcFIglNyQnEgbdVPXmU4YRaYpZix0Ow1CU4kkgojoKBE9T0RniehBn31+jIjOENFpIvpLZftxInrBfh3PquGDQJZJzapI24mjTyboG2u6qXAOpLpi6tRYxQKIoNETkQ7gUQC3AXgFwFNEdEoIcUbZZxnAQwBuEEK8TUSz9vYpAL8JYAWAAPAV+7tvZ38p1aPKC6ayILGNvtNw041eXSd9XdE1gii7ESUS5Q69BsBZIcSLQog1AI8DOOba5+cAPCoFuBDinL39DgBPCiHesj97EsDRbJpefbKsMFVFnDh6ttHHwuQ0xYVjaFqjTWVRbPQLAF5W/n8FwLWuffYDABH9GwAdwCNCiH/y+e6C+wREdD+A+wFg586dUdteeeqv0SeLo298eCXH0RfOh/bPYONic3X6rJyxBoBlADcDWATwJSK6IuqXhRCPAXgMAFZWVmrza9Q+vDJtHH1DNSyOoy+eB25dLrsJpRJFp3gVwA7l/0V7m8orAE4JIdaFEC8B+AYswR/lu7UlixJmVSZp1M2QqaGlp8uHP8i0OI6eKZgogv4pAMtEtJuIWgDuBXDKtc/nYWnzIKIZWKacFwH8M4DbiWiSiCYB3G5vawRGzTX6oYRRN0SE6dFW6lw7gwpr9EzRhJpuhBAbRPQALAGtAzgphDhNRJ8C8LQQ4hR6Av0MgE0AnxRCfBcAiOi3YT0sAOBTQoi38riQKlJ3G/2HD83B1DWMKVWjovLYR1cwM9bKoVXVx2CNnimYSDZ6IcQTAJ5wbfsN5b0A8Cv2y/3dkwBOpmvmYOII+ppqbouTHdx34+5E371icSLj1gwOHEfPFA37/XPEEfQVzF7JlAfH0TNFw4I+R+RCorra6JlkGBxHzxQMC/ockSabuppumGSYNTfpMdWDBX2O6I7TjbuZ6XHF4gQ+uDSJ2fF22U1hGgJnr8yRKleYYsrjsvkJ/PXHri+7GUyDYEGfI0Omjl87egB3XDaYdSYZhqkHLOhz5hdu3lt2ExiGaThsPGYYhqk5LOgZhmFqDgt6hmGYmsOCnmEYpuawoGcYhqk5LOgZhmFqDgt6hmGYmsOCnmEYpuaQlUq+OhDRdwB8O8UhZgC8mVFzBhnuBwvuBwvuB4s698MuIcQ2rw8qJ+jTQkRPCyFWym5H2XA/WHA/WHA/WDS1H9h0wzAMU3NY0DMMw9ScOgr6x8puQEXgfrDgfrDgfrBoZD/UzkbPMAzD9FNHjZ5hGIZRYEHPMAxTc2oj6InoKBE9T0RniejBsttTJET0LSL6KhE9Q0RP29umiOhJInrB/jtZdjvzgIhOEtE5Ivqass3z2sni0/YYeY6Iri6v5dni0w+PENGr9rh4hojuUj57yO6H54nojnJanT1EtIOI/pWIzhDRaSL6RXt748aESi0EPRHpAB4FcCeAgwB+gogOltuqwrlFCHGlEiP8IIAvCiGWAXzR/r+OfBbAUdc2v2u/E8Cy/bofwB8V1MYi+Cy29gMA/L49Lq4UQjwBAPa9cS+Ay+zv/KF9D9WBDQCfEEIcBHAdgBP29TZxTDjUQtADuAbAWSHEi0KINQCPAzhWcpvK5hiAz9nvPwfgR0psS24IIb4E4C3XZr9rPwbgT4XFfwLoEtFcMS3NF59+8OMYgMeFEBeEEC8BOAvrHhp4hBCvCSH+235/HsDXASyggWNCpS6CfgHAy8r/r9jbmoIA8AUi+goR3W9v2y6EeM1+/zqAJlUo97v2Jo6TB2yTxEnFfNeIfiCiJQBXAfgvNHxM1EXQN50bhRBXw5qGniCiD6kfCiuGtpFxtE2+dlhmiL0ArgTwGoDfLbc5xUFEowD+FsAvCSHeVT9r4pioi6B/FcAO5f9Fe1sjEEK8av89B+DvYU3D35BTUPvvufJaWDh+196ocSKEeEMIsSmEuAjgj9Ezz9S6H4jIhCXk/0II8Xf25kaPiboI+qcALBPRbiJqwXI0nSq5TYVARCNENCbfA7gdwNdgXf9xe7fjAP6hnBaWgt+1nwLw03akxXUA/leZztcOl635HljjArD64V4iahPRbliOyC8X3b48ICIC8BkAXxdC/J7yUbPHhBCiFi8AdwH4BoBvAni47PYUeN17ADxrv07LawcwDSu64AUA/wJgquy25nT9fwXLLLEOy756n9+1AyBY0VnfBPBVACtltz/nfvgz+zqfgyXQ5pT9H7b74XkAd5bd/gz74UZYZpnnADxjv+5q4phQX5wCgWEYpubUxXTDMAzD+MCCnmEYpuawoGcYhqk5LOgZhmFqDgt6hmGYmsOCnmEYpuawoGcYhqk5/w+px+TRMVdRYwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiRXWeucjZ74"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtr8rpKissv3"
      },
      "source": [
        "random_seed = 1553\n",
        "torch.manual_seed(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed) # multi-GPU\n",
        "#####################################################\n",
        "\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "loss = 0\n",
        "cnt = 1\n",
        "#random_test = random.sample(list(range(801)), 5) # 다시 100으로 바꿔주어야 함\n",
        "with torch.no_grad():\n",
        "  for interval in range(1,11):\n",
        "    \n",
        "    eval_list_ = eval_list[10*(interval-1) : interval*10]\n",
        "    \n",
        "    output, sample, t = model(eval_list_, 10) # 최종 output\n",
        "    target = torch.tensor(pd.Series(log[sample].values[0])).to(device) # target 값\n",
        "    loss = criterion(output, target)\n",
        "        #loss.backward()  # 역전파\n",
        "        #optimizer.step()\n",
        "    _, predicted = output.max(1)\n",
        "    correct += predicted.eq(target).sum().item()\n",
        "    print(cnt, \"번째 interval 완료\")\n",
        "    cnt += 1\n",
        "\n",
        "  print(\"최종 정확도 : \", 100. * correct / 100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}